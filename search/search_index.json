{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":"<p>This is the documentation for the Smart Smoker system. This a full stack solution for tracking meat smoking receipts prep, and temperatures. This app has 4 main components as follows, Backend, cloud-frontend, device-service, smoker-frontend. Database for this project uses mongoBD</p> <p>agile/ jira board link figma link (This is very out dated)</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>This project uses npm workspaces for all its apps, therefore you can install all apps at once. </p> <p>pre-requisite to install</p> <ul> <li><code>node v20</code></li> <li><code>npm v10</code></li> </ul> <p>run this cmd from the base folder of the repo  <code>npm run bootstrap</code> </p> <p>This should install all apps in the project. To run each one please see appropriate tab above running each app locally should all connect and run together without extra config</p> <p>or to run all apps locally run run <code>npm run start</code> in the root folder. This will app all app in one terminal note you must have a local mongo DS instance running for the backend to boot</p> <p>You will also need a local mongo db running  install link  Run <code>sudo systemctl start mongod</code> to start on linux </p>"},{"location":"#documentation-development","title":"Documentation Development","text":"<p>This project uses MkDocs with Material theme for documentation. To work with the documentation:</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Make sure you have <code>mise</code> installed and the project tools set up: <pre><code>mise install  # Installs Python 3.11, Node 20, and npm 10\n</code></pre></p>"},{"location":"#required-dependencies","title":"Required Dependencies","text":"<ul> <li>MkDocs: <code>1.6.1</code></li> <li>MkDocs Material Theme: <code>9.6.15</code></li> </ul>"},{"location":"#documentation-commands","title":"Documentation Commands","text":"<pre><code># Install documentation dependencies\nmise run docs-install\n\n# Serve documentation locally (auto-reload on changes)\nmise run docs-serve\n\n# Build static documentation site\nmise run docs-build\n\n# Deploy to GitHub Pages (requires permissions)\nmise run docs-deploy\n</code></pre> <p>The documentation will be available at: http://127.0.0.1:8001</p>"},{"location":"#adding-new-documentation","title":"Adding New Documentation","text":"<ul> <li>Add new markdown files to the <code>docs/</code> folder</li> <li>Update <code>mkdocs.yml</code> navigation if needed</li> <li>Follow the existing structure and Material theme guidelines</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml          # The configuration file.\ndocs/\n    index.md        # The documentation homepage.\n    CI-CD/          # CI/CD and deployment documentation\n    ...             # Other markdown pages, images and other files.\napps/\n    backend/        # umm this is the backend\n    device-service/ # handles pi devices example. serial port, wifi\n    frontend/       # frontend for cloud app\n    smoker/         # frontend for pi on smoker\npackages/           # shared components and utilities\n    TemperatureChart/ # D3.js temperature visualization component\nMicroController/    # arduino file\n.github/\n    workflows/      # github action workflows\n</code></pre>"},{"location":"#devgit-requirements","title":"Dev/Git Requirements","text":""},{"location":"#branch-naming-convention","title":"Branch Naming Convention","text":"<ul> <li>Names start with either <code>feature/</code>, <code>bug/</code>, or <code>hotfix/</code></li> <li>Then has Jira number example <code>SS2-14</code></li> <li>Then has name of card</li> <li>Example: <code>feature/SS2-14-login</code></li> </ul>"},{"location":"#pull-request-workflow","title":"Pull Request Workflow","text":"<p>All changes must come from a separate branch and merged to master via a PR. DO NOT COMMIT STRAIGHT TO MASTER</p>"},{"location":"#pr-requirements","title":"PR Requirements","text":"<ul> <li>Must be rebased off of latest master</li> <li>All PR's must have Ben's approval before being merged</li> <li>All automated tests must pass (enforced by GitHub Actions)</li> </ul>"},{"location":"#automated-testing-cicd","title":"Automated Testing (CI/CD)","text":"<p>Every PR automatically runs: - \u2705 Jest Tests: All 4 apps (backend, device-service, frontend, smoker) - \u2705 Package Tests: TemperatureChart and future packages - \u2705 TypeScript Compilation: Ensures code compiles without errors - \u2705 Build Verification: Frontend and Smoker apps must build successfully - \u2705 Code Quality: Linting and formatting checks</p> <p>Branch Protection: The <code>master</code> branch is protected and requires all status checks to pass before merging.</p> <p>For detailed CI/CD documentation, see the CI/CD section.</p>"},{"location":"#setting-up-branch-protection","title":"Setting Up Branch Protection","text":"<p>See <code>.github/BRANCH_PROTECTION_SETUP.md</code> for detailed instructions on configuring required status checks in GitHub.</p>"},{"location":"Documentation/","title":"Documentation","text":""},{"location":"Documentation/#how-to-contribute-to-docs","title":"How to contribute to Docs","text":"<p>You can add to docs and create a PR to be review and merged into master just like code to add a page create a new <code>.md</code> file and add it to the <code>nav</code> section in <code>mkdocs.yml</code></p>"},{"location":"Documentation/#mkdos-material","title":"mkDos Material","text":"<p>These docs use mkDocs material  mkdocs-materal link</p>"},{"location":"Documentation/#run-local","title":"Run local","text":"<ol> <li><code>pip install mkdocs-material</code></li> <li><code>mkdocs serve</code></li> <li>visit <code>http://127.0.0.1:8000/</code></li> </ol>"},{"location":"piSoftware/","title":"Pi Software","text":""},{"location":"piSoftware/#smoker-pi-software","title":"Smoker Pi Software","text":"<ul> <li>tailscale</li> <li>network manager</li> <li>https://pimylifeup.com/raspberry-pi-network-manager/</li> <li>nvm/node/npm</li> <li><code>curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.31.3/install.sh | bash</code></li> <li>docker</li> <li><code>curl -fsSL https://get.docker.com -o get-docker.sh</code></li> <li><code>sudo sh get-docker.sh</code></li> <li>power button stuff</li> <li>github action runner if needed</li> </ul>"},{"location":"piSoftware/#kiosk-mode","title":"Kiosk mode","text":"<ul> <li>used this video to set boot screen settings</li> <li>removed mouse by setting <code>xserver-command = X -nocursor</code> in <code>/etc/lightdm/lightdm.conf</code></li> <li>auto hide the task bar and set desktop to boot splash screen</li> <li>lots of more setting in <code>.config/lxpanel/LXDE-pi/panels/pane</code></li> </ul>"},{"location":"Backend/","title":"Getting Started","text":"<p>after installing (on the welcome page)</p> <p>before you start you will need to run a local mongodb server  you may need to do some googling to set that up but here are the mongoDb Docs</p> <p>make sure it is running on this <code>http://127.0.0.1:27017</code> </p> <p>You will need to create a .env.local for this app. The values you need for this are as follows: * DB_URL=mongodb://127.0.0.1:27017/SmokerDB * VAPID_PUBLIC_KEY= * VAPID_PRIVATE_KEY= <p>once that is set up just run  <code>npm run start</code>  and you should be good to go</p>"},{"location":"Backend/#api","title":"API","text":"<p>once you get this running you can go to <code>http://localhost:3001/api/</code> to see the swagger of all api endpoint and test them to your live local env</p>"},{"location":"Backend/#websocket","title":"Websocket","text":"<p>This is used for live temps from the pi and to the cloud frontend It is also use for live updates like start and stop smoking button</p>"},{"location":"Backend/backend-architecture/","title":"Smart Smoker Backend Architecture","text":"<p>This document illustrates the architecture of the Smart Smoker backend system, showing how different modules interact with each other. The system consists of two main services:</p> <ol> <li>Backend Service: Handles the business logic, database operations, and web socket communication with clients</li> <li>Device Service: Manages communication with the physical smoker device through serial connection</li> </ol>"},{"location":"Backend/backend-architecture/#backend-service-module-interaction-diagram","title":"Backend Service Module Interaction Diagram","text":"<pre><code>graph TD\n    AppModule[Backend App Module] --&gt; StateModule[State Module]\n    AppModule --&gt; PreSmokeModule[PreSmoke Module]\n    AppModule --&gt; SmokeModule[Smoke Module]\n    AppModule --&gt; EventsModule[Events WebSocket Module]\n    AppModule --&gt; TempModule[Temperature Module]\n    AppModule --&gt; SmokeProfileModule[Smoke Profile Module]\n    AppModule --&gt; PostSmokeModule[Post Smoke Module]\n    AppModule --&gt; RatingsModel[Ratings Module]\n    AppModule --&gt; HistoryModule[History Module]\n    AppModule --&gt; NotificationsModule[Notifications Module]\n    AppModule --&gt; SettingsModule[Settings Module]\n\n    %% WebSocket gateway dependencies\n    EventsModule --&gt; StateModule\n    EventsModule --&gt; TempModule\n    EventsModule --&gt; NotificationsModule\n\n    %% Smoke Module dependencies\n    SmokeModule --&gt; StateModule\n\n    %% Temp Module dependencies\n    TempModule --&gt; StateModule\n    TempModule --&gt; SmokeModule\n\n    %% PreSmoke Module dependencies\n    PreSmokeModule --&gt; StateModule\n    PreSmokeModule --&gt; SmokeModule\n\n    %% PostSmoke Module dependencies\n    PostSmokeModule --&gt; StateModule\n    PostSmokeModule --&gt; SmokeModule\n\n    %% SmokeProfile Module dependencies\n    SmokeProfileModule --&gt; StateModule\n    SmokeProfileModule --&gt; SmokeModule\n    SmokeProfileModule --&gt; RatingsModel\n\n    %% Database connection\n    AppModule --&gt; MongoDB[(MongoDB)]\n\n    classDef module fill:#b3e6ff,stroke:#3399ff,stroke-width:2px;\n    classDef database fill:#ffcc99,stroke:#ff9933,stroke-width:2px;\n    class AppModule,StateModule,PreSmokeModule,SmokeModule,EventsModule,TempModule,SmokeProfileModule,PostSmokeModule,RatingsModel,HistoryModule,NotificationsModule,SettingsModule module;\n    class MongoDB database;</code></pre>"},{"location":"Backend/backend-architecture/#device-service-module-interaction-diagram","title":"Device Service Module Interaction Diagram","text":"<pre><code>graph TD\n    DeviceAppModule[Device App Module] --&gt; SerialModule[Serial Module]\n    DeviceAppModule --&gt; DeviceEventsModule[Device WebSocket Module]\n    DeviceAppModule --&gt; WifiManagerModule[WiFi Manager Module]\n\n    DeviceEventsModule --&gt; SerialModule\n\n    classDef module fill:#b3e6ff,stroke:#3399ff,stroke-width:2px;\n    class DeviceAppModule,SerialModule,DeviceEventsModule,WifiManagerModule module;</code></pre>"},{"location":"Backend/backend-architecture/#system-data-flow","title":"System Data Flow","text":"<pre><code>graph LR\n    Device[Smoker Device] --&gt;|Serial Connection| SerialService[Serial Service]\n    SerialService --&gt;|Temperature Data| DeviceWebSocket[Device WebSocket Gateway]\n    DeviceWebSocket --&gt;|Forward Data| BackendWebSocket[Backend WebSocket Gateway]\n\n    BackendWebSocket --&gt;|Process Data| TempService[Temperature Service]\n    BackendWebSocket --&gt;|Update State| StateService[State Service]\n    BackendWebSocket --&gt;|Check Thresholds| NotificationService[Notification Service]\n\n    TempService --&gt;|Store Temperatures| Database[(MongoDB)]\n    StateService --&gt;|Update Smoking Status| Database\n\n    Client[Web/Mobile Client] --&gt;|HTTP Requests| API[REST API Controllers]\n    Client --&gt;|Real-time Updates| BackendWebSocket\n\n    API --&gt;|Manage Smoke Sessions| SmokeService[Smoke Service]\n    API --&gt;|Configure Profiles| ProfileService[Profile Service]\n    API --&gt;|Setup Pre-Smoke| PreSmokeService[PreSmoke Service]\n    API --&gt;|Record Post-Smoke| PostSmokeService[PostSmoke Service]\n\n    SmokeService --&gt; Database\n    ProfileService --&gt; Database\n    PreSmokeService --&gt; Database\n    PostSmokeService --&gt; Database\n\n    classDef external fill:#ccffcc,stroke:#66cc66,stroke-width:2px;\n    classDef service fill:#ffccff,stroke:#cc66cc,stroke-width:2px;\n    classDef data fill:#ffcc99,stroke:#ff9933,stroke-width:2px;\n    classDef gateway fill:#ffffcc,stroke:#cccc66,stroke-width:2px;\n\n    class Device,Client external;\n    class SerialService,TempService,StateService,NotificationService,SmokeService,ProfileService,PreSmokeService,PostSmokeService service;\n    class Database data;\n    class API service;\n    class DeviceWebSocket,BackendWebSocket gateway;</code></pre>"},{"location":"Backend/backend-architecture/#core-components","title":"Core Components","text":""},{"location":"Backend/backend-architecture/#backend-service-components","title":"Backend Service Components","text":""},{"location":"Backend/backend-architecture/#state-module","title":"State Module","text":"<ul> <li>Central module for maintaining system state</li> <li>Tracks smoking status and current session information</li> <li>Many other modules depend on this for state information</li> </ul>"},{"location":"Backend/backend-architecture/#events-module-backend-websocket","title":"Events Module (Backend WebSocket)","text":"<ul> <li>Handles real-time communication with clients</li> <li>Receives temperature data from the device service</li> <li>Processes data and broadcasts updates to connected clients</li> <li>Relies on Temperature, State, and Notifications modules</li> </ul>"},{"location":"Backend/backend-architecture/#temperature-module","title":"Temperature Module","text":"<ul> <li>Manages temperature readings</li> <li>Stores historical temperature data</li> <li>Depends on State and Smoke modules</li> </ul>"},{"location":"Backend/backend-architecture/#smoke-module","title":"Smoke Module","text":"<ul> <li>Core module for smoke session management</li> <li>Depends on State module for tracking smoking status</li> </ul>"},{"location":"Backend/backend-architecture/#presmoke-module","title":"PreSmoke Module","text":"<ul> <li>Handles preparation phase before smoking begins</li> <li>Depends on State and Smoke modules</li> </ul>"},{"location":"Backend/backend-architecture/#postsmoke-module","title":"PostSmoke Module","text":"<ul> <li>Manages completion phase after smoking ends</li> <li>Depends on State and Smoke modules</li> </ul>"},{"location":"Backend/backend-architecture/#smoke-profile-module","title":"Smoke Profile Module","text":"<ul> <li>Manages smoking profiles and configurations</li> <li>Depends on State, Smoke, and Ratings modules</li> </ul>"},{"location":"Backend/backend-architecture/#settings-module","title":"Settings Module","text":"<ul> <li>Handles system configuration settings</li> <li>Operates independently with its own database schema</li> </ul>"},{"location":"Backend/backend-architecture/#notifications-module","title":"Notifications Module","text":"<ul> <li>Manages user notifications based on temperature thresholds</li> <li>Used by Events module to trigger notifications</li> </ul>"},{"location":"Backend/backend-architecture/#ratings-module","title":"Ratings Module","text":"<ul> <li>Handles rating system for smoke profiles</li> <li>Used by Smoke Profile module</li> </ul>"},{"location":"Backend/backend-architecture/#history-module","title":"History Module","text":"<ul> <li>Tracks historical smoking sessions</li> <li>Provides analytics and reporting capabilities</li> </ul>"},{"location":"Backend/backend-architecture/#device-service-components","title":"Device Service Components","text":""},{"location":"Backend/backend-architecture/#serial-module","title":"Serial Module","text":"<ul> <li>Manages serial communication with the smoker device hardware</li> <li>Reads temperature data from temperature probes and sensors</li> <li>Exports a service that other modules can use to interact with the hardware</li> </ul>"},{"location":"Backend/backend-architecture/#device-events-module-device-websocket","title":"Device Events Module (Device WebSocket)","text":"<ul> <li>Forwards temperature and status data from the serial module to the backend service</li> <li>Provides a WebSocket gateway for real-time communication</li> </ul>"},{"location":"Backend/backend-architecture/#wifi-manager-module","title":"WiFi Manager Module","text":"<ul> <li>Handles WiFi connectivity for the device</li> <li>Provides API for configuring WiFi settings</li> </ul> <p>Note</p> <p>I paid $2.50 for AI to create this soooo hopefully this is right</p>"},{"location":"CI-CD/","title":"CI/CD &amp; Deployment","text":"<p>This section covers Continuous Integration, Continuous Deployment, and infrastructure management for the Smart Smoker V2 project.</p>"},{"location":"CI-CD/#overview","title":"Overview","text":"<p>The Smart Smoker V2 project uses a comprehensive CI/CD pipeline that includes:</p> <ul> <li>Automated Testing: GitHub Actions run tests on every PR</li> <li>Container Deployment: Docker containers deployed via watchtower and GitHub Actions</li> <li>Network Management: Tailscale for secure private networking</li> <li>Monitoring: Portainer for container management and monitoring</li> </ul>"},{"location":"CI-CD/#documentation-structure","title":"Documentation Structure","text":""},{"location":"CI-CD/#github-actions-cicd","title":"GitHub Actions CI/CD","text":"<p>Comprehensive guide to the automated testing and deployment workflows: - Testing Pipeline: Jest tests for all 4 applications and packages - Branch Protection: Required status checks for PR merging - Parallel Execution: Fast, efficient testing across the monorepo - Build Verification: Frontend and Electron app build validation</p>"},{"location":"CI-CD/#test-coverage-reports","title":"Test Coverage Reports","text":"<p>Complete guide to generating, viewing, and interpreting test coverage: - Coverage Generation: Commands for all applications and packages - HTML Reports: Interactive browser-based coverage dashboards - CI Integration: Coverage reports in GitHub Actions - Best Practices: Improving coverage quality and identifying gaps</p>"},{"location":"CI-CD/#code-quality-linting","title":"Code Quality &amp; Linting","text":"<p>Comprehensive guide to linting, formatting, and code quality tools: - ESLint &amp; Prettier Setup: Workspace-wide configuration for consistent code style - App-Specific Linting: TypeScript, React, and NestJS best practices - VS Code Integration: Auto-formatting and error detection - CI/CD Integration: Automated code quality checks in GitHub Actions</p>"},{"location":"CI-CD/#testing-library-migration-guide","title":"Testing Library Migration Guide","text":"<p>Systematic approach to fixing Testing Library rule violations: - 240+ Violations Analysis: Categories and priority ranking for fixes - Migration Strategy: Phase-by-phase approach to modernize testing patterns - Best Practices: Modern Testing Library patterns and anti-patterns to avoid - Implementation Timeline: Structured 6-week plan for systematic improvements</p>"},{"location":"CI-CD/#dependency-management","title":"Dependency Management","text":"<p>Comprehensive guide to managing dependencies across the monorepo: - Clean Installation: npm run clean and bootstrap processes - Workspace Management: Using npm workspaces effectively - CI/CD Integration: Package-lock.json management for reliable builds - Troubleshooting: Common dependency issues and solutions</p>"},{"location":"CI-CD/#deployment-infrastructure","title":"Deployment &amp; Infrastructure","text":"<p>Production deployment processes and infrastructure management: - Version Deployments: Release process with GitHub tags - Container Orchestration: Docker with watchtower auto-deployment - Network Configuration: Tailscale setup and SSL management - Monitoring Setup: Portainer installation and configuration</p>"},{"location":"CI-CD/#manual-version-deployment","title":"Manual Version Deployment","text":"<p>Runbook for deploying specific container versions to the cloud using GitHub Actions or local Docker Compose, including rollback and verification steps.</p>"},{"location":"CI-CD/#quick-reference","title":"Quick Reference","text":""},{"location":"CI-CD/#ci-pipeline-status-checks","title":"CI Pipeline Status Checks","text":"<p>Every PR must pass these automated checks: - <code>Run Jest Tests (backend)</code> - <code>Run Jest Tests (device-service)</code> - <code>Run Jest Tests (frontend)</code> - <code>Run Jest Tests (smoker)</code> - <code>Test Packages</code> - <code>Code Quality Check</code> (linting &amp; formatting) - <code>Build Check (frontend)</code> - <code>Build Check (smoker)</code> - <code>All Tests Status</code></p>"},{"location":"CI-CD/#deployment-environments","title":"Deployment Environments","text":"<p>Cloud Environment: - Frontend: https://smokecloud.tail74646.ts.net - Backend: https://smokecloud.tail74646.ts.net:8443 - Deployed via GitHub Actions</p> <p>Smoker Environment: - Local Electron app + device service - Deployed via Docker + watchtower - Auto-updates from Docker Hub</p>"},{"location":"CI-CD/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create Feature Branch: <code>feature/SS2-XX-description</code></li> <li>Develop &amp; Test Locally: Run tests and code quality checks before pushing</li> <li>Code Quality: Use <code>npm run check</code> for linting and formatting</li> <li>Create Pull Request: CI automatically runs all tests and quality checks</li> <li>Code Review: Requires approval + passing tests + code quality</li> <li>Merge to Master: Triggers deployment workflows</li> <li>Production Release: Tag version for container updates</li> </ol>"},{"location":"CI-CD/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Developer     \u2502    \u2502   GitHub Actions \u2502    \u2502   Production    \u2502\n\u2502                 \u2502    \u2502                  \u2502    \u2502                 \u2502\n\u2502 \u2022 Local Dev     \u2502\u2500\u2500\u2500\u25b6\u2502 \u2022 Jest Tests     \u2502\u2500\u2500\u2500\u25b6\u2502 \u2022 Cloud Apps    \u2502\n\u2502 \u2022 Feature Branch\u2502    \u2502 \u2022 Build Checks   \u2502    \u2502 \u2022 Smoker Device \u2502\n\u2502 \u2022 Pull Request  \u2502    \u2502 \u2022 Deploy         \u2502    \u2502 \u2022 Monitoring    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n         \u2502              \u2502  Branch Protect \u2502               \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 \u2022 Required Tests\u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502 \u2022 Status Checks \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"CI-CD/#best-practices","title":"Best Practices","text":""},{"location":"CI-CD/#for-developers","title":"For Developers","text":"<ul> <li>Test Locally: Run <code>npm test</code> before pushing</li> <li>Code Quality: Run <code>npm run check</code> for linting and formatting</li> <li>Check CI Status: Monitor Actions tab for test results</li> <li>Fix Issues: Address linting, formatting, and test failures before requesting review</li> <li>Keep PRs Small: Easier to review and test</li> </ul>"},{"location":"CI-CD/#for-devops","title":"For DevOps","text":"<ul> <li>Monitor Deployments: Check container health after releases</li> <li>Update Dependencies: Keep GitHub Actions and packages current</li> <li>Backup Configs: Maintain infrastructure as code</li> <li>Security Updates: Regular security scanning and updates</li> </ul>"},{"location":"CI-CD/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CI-CD/#cicd-issues","title":"CI/CD Issues","text":"<ul> <li>Failed Tests: Check logs in GitHub Actions tab</li> <li>Deployment Failures: Review workflow logs and container status</li> <li>Network Issues: Verify Tailscale configuration</li> <li>Container Problems: Use Portainer for monitoring and debugging</li> </ul>"},{"location":"CI-CD/#common-solutions","title":"Common Solutions","text":"<ul> <li>Test Failures: Run tests locally to reproduce issues</li> <li>Build Errors: Check TypeScript compilation and dependencies</li> <li>Deployment Stuck: Restart watchtower or redeploy manually</li> <li>Network Access: Verify Tailscale funnel configuration</li> </ul> <p>For detailed troubleshooting guides, see the individual documentation pages.</p>"},{"location":"CI-CD/code-quality/","title":"Smart Smoker V2 - Linting &amp; Formatting Implementation Guide","text":""},{"location":"CI-CD/code-quality/#what-was-implemented","title":"\ud83c\udfaf What Was Implemented","text":"<p>This implementation provides comprehensive linting and formatting for the Smart Smoker V2 monorepo while respecting its existing structure and best practices.</p>"},{"location":"CI-CD/code-quality/#completed-setup","title":"\u2705 Completed Setup","text":"<ol> <li>Root-Level Configuration</li> <li><code>.eslintrc.js</code> - Workspace-wide ESLint rules</li> <li><code>.prettierrc</code> - Unified formatting configuration  </li> <li><code>.prettierignore</code> - Proper ignore patterns</li> <li> <p><code>.vscode/</code> - Workspace settings and recommended extensions</p> </li> <li> <p>Enhanced Package Configurations</p> </li> <li>Added linting to <code>packages/TemperatureChart</code></li> <li>Updated all React apps with proper ESLint + Prettier</li> <li>Maintained existing NestJS app configurations</li> <li> <p>Added consistent script names across all packages</p> </li> <li> <p>Comprehensive Scripts</p> </li> <li>Root-level formatting and linting commands</li> <li>App-specific safe linting with error handling</li> <li>Pre-commit hooks support</li> <li>CI/CD friendly commands</li> </ol>"},{"location":"CI-CD/code-quality/#usage-guide","title":"\ud83d\ude80 Usage Guide","text":""},{"location":"CI-CD/code-quality/#recommended-daily-development-workflow","title":"Recommended Daily Development Workflow","text":"<pre><code># 1. Format entire workspace (always works)\nnpm run format\n\n# 2. Check formatting without changes\nnpm run format:check  \n\n# 3. Run all app linting (safe with error handling)\nnpm run lint:apps\n\n# 4. Fix linting issues across all apps\nnpm run lint:apps:fix\n\n# 5. Quality check before commits\nnpm run check\n</code></pre>"},{"location":"CI-CD/code-quality/#app-specific-development","title":"App-Specific Development","text":"<pre><code># Backend (NestJS) - Full TypeScript linting\ncd apps/backend\nnpm run lint        # ESLint with auto-fix\nnpm run format      # Prettier formatting\n\n# Device Service (NestJS) - Full TypeScript linting  \ncd apps/device-service\nnpm run lint        # ESLint with auto-fix\nnpm run format      # Prettier formatting\n\n# Frontend (React) - React + TypeScript linting\ncd apps/frontend\nnpm run lint        # ESLint with React rules\nnpm run format      # Prettier formatting\n\n# Smoker App (Electron + React) - React + TypeScript linting\ncd apps/smoker\nnpm run lint        # ESLint with React rules  \nnpm run format      # Prettier formatting\n\n# TemperatureChart Package - React + TypeScript linting\ncd packages/TemperatureChart\nnpm run lint        # ESLint with React rules\nnpm run format      # Prettier formatting\n</code></pre>"},{"location":"CI-CD/code-quality/#cicd-integration-commands","title":"CI/CD Integration Commands","text":"<pre><code># For GitHub Actions - fail on issues\nnpm run check                    # Format check + app linting\nnpm run format:check            # Just formatting check\nnpm run lint:apps               # All apps with safe error handling\n\n# Pre-commit - auto-fix and check\nnpm run fix                     # Auto-fix formatting + linting\n</code></pre>"},{"location":"CI-CD/code-quality/#code-quality-rules-implemented","title":"\ud83d\udccb Code Quality Rules Implemented","text":""},{"location":"CI-CD/code-quality/#typescript-rules-backenddevice-service","title":"TypeScript Rules (Backend/Device Service)","text":"<ul> <li>\u2705 Explicit function return types (NestJS only)</li> <li>\u2705 Explicit module boundary types (NestJS only)  </li> <li>\u2705 No unused variables/imports</li> <li>\u2705 Prefer const over let</li> <li>\u2705 Object shorthand syntax</li> <li>\u2705 Template literals over string concatenation</li> </ul>"},{"location":"CI-CD/code-quality/#react-rules-frontendsmokerpackages","title":"React Rules (Frontend/Smoker/Packages)","text":"<ul> <li>\u2705 Functional components only (per best practices)</li> <li>\u2705 Arrow function component definitions</li> <li>\u2705 React Hooks rules enforcement</li> <li>\u2705 JSX accessibility best practices</li> <li>\u2705 No React.Fragment imports (React 17+)</li> <li>\u2705 Testing Library best practices</li> </ul>"},{"location":"CI-CD/code-quality/#formatting-rules-all-files","title":"Formatting Rules (All Files)","text":"<ul> <li>\u2705 Single quotes for strings</li> <li>\u2705 Trailing commas (ES5 style)</li> <li>\u2705 100 character line length</li> <li>\u2705 2-space indentation</li> <li>\u2705 Semicolons required</li> <li>\u2705 Unix line endings (LF)</li> </ul>"},{"location":"CI-CD/code-quality/#configuration-details","title":"\ud83d\udd27 Configuration Details","text":""},{"location":"CI-CD/code-quality/#eslint-configuration-eslintrcjs","title":"ESLint Configuration (<code>.eslintrc.js</code>)","text":"<ul> <li>Backend/Device Service: TypeScript + NestJS rules</li> <li>Frontend/Smoker: React + TypeScript + Accessibility rules</li> <li>Packages: React + TypeScript (relaxed rules)</li> <li>JavaScript Files: Basic ES2022 rules</li> </ul>"},{"location":"CI-CD/code-quality/#prettier-configuration-prettierrc","title":"Prettier Configuration (<code>.prettierrc</code>)","text":"<ul> <li>Consistent across all file types</li> <li>Special rules for Markdown (80 chars) and JSON (120 chars)</li> <li>Ignores build artifacts, dependencies, and generated files</li> </ul>"},{"location":"CI-CD/code-quality/#vs-code-integration","title":"VS Code Integration","text":"<ul> <li>Format on save enabled</li> <li>Auto-fix ESLint issues on save  </li> <li>Recommended extensions for the team</li> <li>Proper working directories for monorepo</li> </ul>"},{"location":"CI-CD/code-quality/#current-code-quality-status","title":"\u26a1 Current Code Quality Status","text":"<p>After implementation, the linting found these issues in your codebase:</p>"},{"location":"CI-CD/code-quality/#backend-23-warnings","title":"Backend (23 warnings)","text":"<ul> <li>Mostly unused imports and variables in service specs</li> <li>Some controller parameters not being used</li> <li>Clean code with good structure \u2705</li> </ul>"},{"location":"CI-CD/code-quality/#frontend-148-issues","title":"Frontend (148 issues)","text":"<ul> <li>Testing Library rule violations (using act unnecessarily)</li> <li>Multiple assertions in waitFor callbacks  </li> <li>Some unused imports</li> <li>Direct DOM access in tests</li> </ul>"},{"location":"CI-CD/code-quality/#smoker-app-93-issues","title":"Smoker App (93 issues)","text":"<ul> <li>Similar Testing Library issues</li> <li>React Hooks dependency warnings</li> <li>Some unused variables</li> <li>WiFi component test improvements needed</li> </ul>"},{"location":"CI-CD/code-quality/#device-service-12-issues","title":"Device Service (12 issues)","text":"<ul> <li>Some TypeScript strict mode violations</li> <li>Require statements instead of imports</li> <li>Unused variables in tests</li> </ul>"},{"location":"CI-CD/code-quality/#next-steps-recommendations","title":"\ud83c\udfaf Next Steps &amp; Recommendations","text":""},{"location":"CI-CD/code-quality/#1-immediate-actions","title":"1. Immediate Actions","text":"<pre><code># Fix all formatting issues\nnpm run format\n\n# Review and fix critical linting issues\nnpm run lint:apps:fix\n</code></pre>"},{"location":"CI-CD/code-quality/#2-team-adoption","title":"2. Team Adoption","text":"<ul> <li>Install recommended VS Code extensions</li> <li>Use <code>npm run check</code> before commits</li> <li>Run app-specific linting during development</li> <li>Address linting warnings incrementally</li> </ul>"},{"location":"CI-CD/code-quality/#3-cicd-integration","title":"3. CI/CD Integration","text":"<p>Add to your GitHub Actions workflow: <pre><code>- name: Check Code Quality\n  run: |\n    npm run bootstrap\n    npm run check\n</code></pre></p>"},{"location":"CI-CD/code-quality/#4-incremental-improvement","title":"4. Incremental Improvement","text":"<ul> <li>Week 1: Fix unused imports/variables (easy wins)</li> <li>Week 2: Improve Testing Library usage in React apps  </li> <li>Week 3: Address TypeScript strict mode issues</li> <li>Week 4: Refactor complex test cases</li> </ul>"},{"location":"CI-CD/code-quality/#important-notes","title":"\ud83d\udea8 Important Notes","text":"<ol> <li> <p>Workspace-Level Linting Limitation: Due to TypeScript project reference conflicts in the monorepo, workspace-level TypeScript linting has been disabled. Use app-specific linting for best results.</p> </li> <li> <p>Backward Compatibility: All existing npm scripts continue to work. New scripts are additive.</p> </li> <li> <p>Build Process: Linting issues are warnings and don't break builds. Use CI/CD enforcement for strict quality gates.</p> </li> <li> <p>Performance: App-specific linting is faster and more accurate than workspace-level for this monorepo structure.</p> </li> </ol>"},{"location":"CI-CD/code-quality/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>ESLint Rules: https://eslint.org/docs/rules/</li> <li>Prettier Configuration: https://prettier.io/docs/en/configuration.html</li> <li>React Testing Library: https://testing-library.com/docs/react-testing-library/intro/</li> <li>TypeScript ESLint: https://typescript-eslint.io/rules/</li> </ul> <p>This implementation provides a solid foundation for maintaining code quality across your Smart Smoker V2 project while being practical for daily development workflows.</p>"},{"location":"CI-CD/dependency-management/","title":"Dependency Management Guide","text":"<p>This guide covers how to properly manage dependencies across the Smart Smoker V2 monorepo.</p>"},{"location":"CI-CD/dependency-management/#clean-installation-process","title":"Clean Installation Process","text":"<p>When you encounter dependency conflicts or need to ensure a fresh installation:</p>"},{"location":"CI-CD/dependency-management/#1-clean-all-dependencies","title":"1. Clean All Dependencies","text":"<pre><code>npm run clean\n</code></pre> <p>This command removes: - All <code>node_modules</code> folders from root, apps, and packages - All <code>package-lock.json</code> files from root, apps, and packages</p>"},{"location":"CI-CD/dependency-management/#2-fresh-bootstrap-installation","title":"2. Fresh Bootstrap Installation","text":"<pre><code>npm run bootstrap\n</code></pre> <p>This installs all dependencies using workspace configuration with <code>--legacy-peer-deps</code> flag.</p>"},{"location":"CI-CD/dependency-management/#3-one-step-clean-install","title":"3. One-Step Clean &amp; Install","text":"<pre><code>npm run clean:install\n</code></pre> <p>Combines both steps above for convenience.</p>"},{"location":"CI-CD/dependency-management/#when-to-use-clean-installation","title":"When to Use Clean Installation","text":""},{"location":"CI-CD/dependency-management/#required-scenarios","title":"Required Scenarios","text":"<ul> <li>After major dependency updates</li> <li>When CI/CD builds fail due to dependency conflicts</li> <li>When switching between Node.js versions</li> <li>Before creating PRs with dependency changes</li> </ul>"},{"location":"CI-CD/dependency-management/#recommended-scenarios","title":"Recommended Scenarios","text":"<ul> <li>Weekly maintenance (clean slate)</li> <li>After pulling major changes from other developers</li> <li>When experiencing unusual build errors</li> </ul>"},{"location":"CI-CD/dependency-management/#workspace-commands","title":"Workspace Commands","text":"<p>The project uses npm workspaces for monorepo management:</p>"},{"location":"CI-CD/dependency-management/#individual-app-installation","title":"Individual App Installation","text":"<pre><code># Install only backend dependencies\nnpm install --workspace=backend --legacy-peer-deps\n\n# Install only frontend dependencies  \nnpm install --workspace=frontend --legacy-peer-deps\n\n# Install only smoker dependencies\nnpm install --workspace=smoker --legacy-peer-deps\n\n# Install only device-service dependencies\nnpm install --workspace=device-service --legacy-peer-deps\n</code></pre>"},{"location":"CI-CD/dependency-management/#individual-package-installation","title":"Individual Package Installation","text":"<pre><code># Install only TemperatureChart dependencies\nnpm install --workspace=temperaturechart --legacy-peer-deps\n</code></pre>"},{"location":"CI-CD/dependency-management/#cicd-considerations","title":"CI/CD Considerations","text":""},{"location":"CI-CD/dependency-management/#github-actions-requirements","title":"GitHub Actions Requirements","text":"<p>Our CI pipeline expects: - \u2705 Committed package-lock.json files in each app/package - \u2705 Compatible dependency versions across workspaces - \u2705 Jest 28.0.3 alignment for consistent testing</p>"},{"location":"CI-CD/dependency-management/#before-pushing-changes","title":"Before Pushing Changes","text":"<p>Always run after dependency modifications: <pre><code>npm run clean:install\ngit add apps/*/package-lock.json packages/*/package-lock.json package-lock.json\ngit commit -m \"chore: update package-lock.json files after dependency changes\"\n</code></pre></p>"},{"location":"CI-CD/dependency-management/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"CI-CD/dependency-management/#issue-cannot-find-module-errors","title":"Issue: \"Cannot find module\" errors","text":"<p>Solution:  <pre><code>npm run clean:install\n</code></pre></p>"},{"location":"CI-CD/dependency-management/#issue-peer-dependency-warnings","title":"Issue: \"Peer dependency warnings\"","text":"<p>Cause: Version mismatches between workspaces Solution:  1. Check if versions align in package.json files 2. Run <code>npm run clean:install</code> 3. Use <code>--legacy-peer-deps</code> flag as configured</p>"},{"location":"CI-CD/dependency-management/#issue-ci-fails-but-local-works","title":"Issue: CI fails but local works","text":"<p>Cause: Missing or outdated package-lock.json files Solution: <pre><code>npm run clean:install\ngit add . &amp;&amp; git commit -m \"fix: update package-lock.json files for CI\"\n</code></pre></p>"},{"location":"CI-CD/dependency-management/#issue-eresolve-unable-to-resolve-dependency-tree","title":"Issue: \"ERESOLVE unable to resolve dependency tree\"","text":"<p>Cause: Conflicting dependency versions Solution: 1. Review package.json files for version conflicts 2. Align major versions (especially React, TypeScript, Jest) 3. Run <code>npm run clean:install</code></p>"},{"location":"CI-CD/dependency-management/#dependency-version-strategy","title":"Dependency Version Strategy","text":""},{"location":"CI-CD/dependency-management/#core-dependencies-must-match","title":"Core Dependencies (Must Match)","text":"<ul> <li>Jest: 28.0.3 across all apps</li> <li>TypeScript: ^4.7.4 for consistency</li> <li>React: ^18.2.0 for frontend apps</li> </ul>"},{"location":"CI-CD/dependency-management/#build-dependencies","title":"Build Dependencies","text":"<ul> <li>Webpack: 5.x for modern builds</li> <li>Babel: 7.x for transpilation</li> <li>CSS Loaders: Compatible versions</li> </ul>"},{"location":"CI-CD/dependency-management/#testing-dependencies","title":"Testing Dependencies","text":"<ul> <li>@testing-library/react: ^13.4.0 for React 18</li> <li>@testing-library/jest-dom: ^5.16.4</li> <li>identity-obj-proxy: ^3.0.0 for CSS mocking</li> </ul>"},{"location":"CI-CD/dependency-management/#best-practices","title":"Best Practices","text":""},{"location":"CI-CD/dependency-management/#1-regular-maintenance","title":"1. Regular Maintenance","text":"<pre><code># Weekly cleanup\nnpm run clean:install\n</code></pre>"},{"location":"CI-CD/dependency-management/#2-before-major-changes","title":"2. Before Major Changes","text":"<pre><code># Clean before dependency updates\nnpm run clean\n# Make changes to package.json files\nnpm run bootstrap\n</code></pre>"},{"location":"CI-CD/dependency-management/#3-after-pulling-changes","title":"3. After Pulling Changes","text":"<pre><code># Ensure fresh state\nnpm run clean:install\n</code></pre>"},{"location":"CI-CD/dependency-management/#4-before-committing","title":"4. Before Committing","text":"<pre><code># Test all apps work\nnpm run clean:install\ncd apps/backend &amp;&amp; npm test\ncd ../device-service &amp;&amp; npm test  \ncd ../frontend &amp;&amp; npm test\ncd ../smoker &amp;&amp; npm test\ncd ../../packages/TemperatureChart &amp;&amp; npm test\n</code></pre>"},{"location":"CI-CD/dependency-management/#scripts-reference","title":"Scripts Reference","text":"Script Purpose Usage <code>npm run clean</code> Remove all node_modules and package-lock.json Cleanup only <code>npm run bootstrap</code> Install all dependencies with workspaces Fresh install <code>npm run clean:install</code> Clean + Bootstrap in one command Full reset <code>npm start</code> Start all services in parallel Development <code>npm run front:start</code> Start only frontend Frontend dev <code>npm run back:start</code> Start only backend Backend dev <code>npm run smoker:start</code> Start only smoker app Smoker dev <code>npm run devices:start</code> Start only device service Device dev"},{"location":"CI-CD/dependency-management/#emergency-recovery","title":"Emergency Recovery","text":"<p>If you encounter severe dependency issues:</p> <pre><code># Nuclear option - complete reset\nrm -rf node_modules\nrm -rf apps/*/node_modules  \nrm -rf packages/*/node_modules\nrm package-lock.json\nrm apps/*/package-lock.json\nrm packages/*/package-lock.json\n\n# Fresh start\nnpm run bootstrap\n\n# Verify all apps\nnpm test --workspace=backend\nnpm test --workspace=device-service  \nnpm test --workspace=frontend\nnpm test --workspace=smoker\nnpm test --workspace=temperaturechart\n</code></pre> <p>Following this guide ensures consistent, reliable dependency management across the entire Smart Smoker V2 project.</p>"},{"location":"CI-CD/deployment-infrastructure/","title":"Deployment &amp; Infrastructure","text":"<p>This document covers production deployment processes, container orchestration, network management, and monitoring for the Smart Smoker V2 project.</p>"},{"location":"CI-CD/deployment-infrastructure/#version-deployments","title":"Version Deployments","text":"<p>To create a new production deployment:</p> <ol> <li>Update version in <code>package.json</code></li> <li>Create a version tag in GitHub for that commit</li> <li>Create a GitHub Release with tag <code>vX.Y.Z</code> (preferred) or run the <code>Release Smart Smoker v2</code> action manually</li> </ol>"},{"location":"CI-CD/deployment-infrastructure/#container-deployment","title":"Container Deployment","text":""},{"location":"CI-CD/deployment-infrastructure/#smoker-environment","title":"Smoker Environment","text":"<p>Containers for the smoker are handled by watchtower. When a new container is pushed to Docker Hub, watchtower automatically pulls it down and replaces the running container on the smoker. </p> <p>The deployment workflow is set for manual trigger and must be used when there are updates to: - <code>smoker.docker-compose.yml</code>  - <code>smoker-deploy.yml</code> </p> <p>Watchtower settings can be seen in the <code>smoker.docker-compose.yml</code> file.</p>"},{"location":"CI-CD/deployment-infrastructure/#cloud-environment","title":"Cloud Environment","text":"<p>Containers for the cloud are deployed via GitHub Action workflows.</p>"},{"location":"CI-CD/deployment-infrastructure/#network-management","title":"Network Management","text":""},{"location":"CI-CD/deployment-infrastructure/#tailscale-configuration","title":"Tailscale Configuration","text":"<p>Using Tailscale to manage the network, providing a private internal network for all devices.</p> <p>Tailscale creates the SSL cert and key and also serves the sites. The Tailscale funnel feature is used to expose the frontend and backend to the public web for the cloud app:</p> <ul> <li>Frontend: https://smokecloud.tail74646.ts.net</li> <li>Backend: https://smokecloud.tail74646.ts.net:8443</li> </ul>"},{"location":"CI-CD/deployment-infrastructure/#verifying-tailscale-setup","title":"Verifying Tailscale Setup","text":"<p>Use the command <code>tailscale funnel status</code> - it should result in this output if correctly set up:</p> <pre><code>ubuntu@ubuntu:/etc/nginx/sites-available$ sudo tailscale funnel status\n\n# Funnel on:\n#     - https://smokecloud.tail74646.ts.net\n#     - https://smokecloud.tail74646.ts.net:8443\n\nhttps://smokecloud.tail74646.ts.net (Funnel on)\n|-- / proxy http://127.0.0.1:80\n\nhttps://smokecloud.tail74646.ts.net:8443 (Funnel on)\n|-- / proxy http://127.0.0.1:3001\n</code></pre>"},{"location":"CI-CD/deployment-infrastructure/#setting-up-tailscale-funnel","title":"Setting Up Tailscale Funnel","text":"<p>To configure services for external access:</p> <ol> <li>Set up serve: <code>tailscale serve http:&lt;port&gt; / &lt;local_port&gt;</code></li> <li>Enable funnel: <code>tailscale funnel &lt;port&gt; on</code></li> </ol> <p>Repeat for each service you want accessible outside the network.</p>"},{"location":"CI-CD/deployment-infrastructure/#deployment-workflow-notes","title":"Deployment Workflow Notes","text":"<p>For the deploy workflow, the process requires: 1. Stop the Tailscale service 2. Run <code>docker compose up</code>  3. Start Tailscale service again</p> <p>This is necessary because Tailscale holds onto the ports needed, so it must be stopped first to allow containers to bind to the ports, then restarted.</p>"},{"location":"CI-CD/deployment-infrastructure/#tailscale-documentation-links","title":"Tailscale Documentation Links","text":"<ul> <li>General Setup</li> <li>Tailscale Serve</li> <li>HTTPS Configuration</li> </ul>"},{"location":"CI-CD/deployment-infrastructure/#container-monitoring","title":"Container Monitoring","text":""},{"location":"CI-CD/deployment-infrastructure/#portainer-setup","title":"Portainer Setup","text":"<p>Using Portainer to host the container monitoring dashboard.</p>"},{"location":"CI-CD/deployment-infrastructure/#cloud-pi-installation","title":"Cloud Pi Installation","text":"<p>To install Portainer on the cloud pi, run the following Docker command:</p> <pre><code>docker run -d -p 10000:9000 --name portainer --restart always \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  portainer/portainer-ce\n</code></pre> <p>Once installed, connect to it via <code>smokerCloudIp:10000</code> (using port 10000 because it was the last available funnel port in Tailscale).</p>"},{"location":"CI-CD/deployment-infrastructure/#smoker-pi-setup","title":"Smoker Pi Setup","text":"<p>To set up the smoker pi, follow the Portainer Agent Environment instructions to configure a Portainer agent environment.</p> <p>Note: Portainer is not included in the deployment process as it operates as a separate entity from the smoker app. Additionally, resetting the container clears all settings.</p>"},{"location":"CI-CD/deployment-infrastructure/#docker-commands-reference","title":"Docker Commands Reference","text":""},{"location":"CI-CD/deployment-infrastructure/#smoker-app-commands","title":"Smoker App Commands","text":""},{"location":"CI-CD/deployment-infrastructure/#build-and-push-test-smoker-image","title":"Build and Push Test Smoker Image","text":"<p>Prerequisites: Run <code>npm run build</code> for smoker first</p> <pre><code># Build for ARM/v7 platform\ndocker build -f apps/smoker/Dockerfile --platform linux/arm/v7 \\\n  -t benjr70/smart-smoker-smoker:smokerTest .\n\n# Push to Docker Hub\ndocker push benjr70/smart-smoker-smoker:smokerTest\n</code></pre>"},{"location":"CI-CD/deployment-infrastructure/#pull-and-run-smoker-image-on-pi","title":"Pull and Run Smoker Image on Pi","text":"<pre><code># Pull latest image\ndocker pull benjr70/smart-smoker-smoker:smokerTest\n\n# Run container\ndocker run -p 8080:8080 benjr70/smart-smoker-smoker:smokerTest\n</code></pre>"},{"location":"CI-CD/deployment-infrastructure/#device-service-commands","title":"Device Service Commands","text":""},{"location":"CI-CD/deployment-infrastructure/#build-and-push-test-device-service-image","title":"Build and Push Test Device Service Image","text":"<pre><code># Build for ARM/v7 platform\ndocker build -f apps/device-service/Dockerfile --platform linux/arm/v7 \\\n  -t benjr70/smart-smoker-device-service:device-serviceTest .\n\n# Push to Docker Hub\ndocker push benjr70/smart-smoker-device-service:device-serviceTest\n</code></pre>"},{"location":"CI-CD/deployment-infrastructure/#pull-and-run-device-service-on-pi","title":"Pull and Run Device Service on Pi","text":"<pre><code># Pull latest image\ndocker pull benjr70/smart-smoker-device-service:device-serviceTest\n\n# Run container with USB device access\ndocker run --privileged --device=/dev/ttyUSB0 -p 3000:3000 \\\n  benjr70/smart-smoker-device-service:device-serviceTest\n</code></pre>"},{"location":"CI-CD/deployment-infrastructure/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Development   \u2502    \u2502   Docker Hub     \u2502    \u2502   Production    \u2502\n\u2502                 \u2502    \u2502                  \u2502    \u2502                 \u2502\n\u2502 \u2022 Build Images  \u2502\u2500\u2500\u2500\u25b6\u2502 \u2022 Store Images   \u2502\u2500\u2500\u2500\u25b6\u2502 \u2022 Watchtower    \u2502\n\u2502 \u2022 Push Updates  \u2502    \u2502 \u2022 Version Tags   \u2502    \u2502 \u2022 Auto Deploy  \u2502\n\u2502 \u2022 Test Locally  \u2502    \u2502 \u2022 Multi-arch     \u2502    \u2502 \u2022 Health Check  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n         \u2502              \u2502   Tailscale    \u2502               \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 \u2022 Private Net  \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502 \u2022 SSL/HTTPS    \u2502\n                        \u2502 \u2022 Public Funnel\u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"CI-CD/github-actions/","title":"GitHub Actions CI/CD","text":"<p>This directory contains GitHub Actions workflows for the Smart Smoker V2 project.</p>"},{"location":"CI-CD/github-actions/#workflows","title":"Workflows","text":""},{"location":"CI-CD/github-actions/#ci-testsyml-continuous-integration-testing","title":"<code>ci-tests.yml</code> - Continuous Integration Testing","text":"<p>Triggers: Pull Requests to <code>master</code> branch Purpose: Runs comprehensive testing suite on all PRs</p> <p>What it tests: - \u2705 Jest unit tests for all 4 applications (backend, device-service, frontend, smoker) - \u2705 Package tests (TemperatureChart and future packages) - \u2705 TypeScript compilation verification - \u2705 Build verification for frontend applications - \u2705 Code quality and linting</p> <p>Jobs: - <code>test</code>: Parallel testing of all applications using matrix strategy - <code>test-packages</code>: Tests shared packages - <code>lint-check</code>: TypeScript compilation verification - <code>build-check</code>: Build verification for React/Electron apps - <code>coverage-report</code>: Aggregates test results and generates summary - <code>all-tests-passed</code>: Final status check (required for merge)</p>"},{"location":"CI-CD/github-actions/#other-workflows","title":"Other Workflows","text":"<ul> <li><code>install.yml</code>: Installation and setup workflow</li> <li><code>build.yml</code>: Application build validation (reusable)</li> <li><code>publish.yml</code>: Docker Hub publishing (reusable)</li> <li><code>cloud-deploy.yml</code>: Cloud environment deployment (reusable)</li> <li><code>smoker-deploy.yml</code>: Smoker environment deployment (reusable)  </li> <li><code>docs.yml</code>: Documentation deployment</li> <li><code>nightly.yml</code>: Nightly Dev Build &amp; Deploy (publishes <code>:nightly</code> for testing)</li> <li><code>deploy-version.yml</code>: Manually deploy a specific version/tag to cloud and/or smoker</li> <li><code>release.yml</code>: Build, publish, and deploy. Supports manual version input and Release tag trigger</li> </ul>"},{"location":"CI-CD/github-actions/#branch-protection","title":"Branch Protection","text":"<p>To enforce CI requirements: 1. See <code>.github/BRANCH_PROTECTION_SETUP.md</code> for setup instructions 2. Configure required status checks in GitHub repository settings 3. Require all CI jobs to pass before allowing PR merges</p>"},{"location":"CI-CD/github-actions/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create Feature Branch: <code>feature/SS2-XX-description</code></li> <li>Make Changes: Develop and commit your changes</li> <li>Create PR: Open Pull Request to <code>master</code></li> <li>CI Runs Automatically: All tests run on your PR</li> <li>Review Process: Address any failing tests + get code review</li> <li>Merge: Once CI passes and approved, PR can be merged</li> </ol>"},{"location":"CI-CD/github-actions/#ci-status-checks","title":"CI Status Checks","text":"<p>The following status checks must pass: - <code>Run Jest Tests (backend)</code> - <code>Run Jest Tests (device-service)</code> - <code>Run Jest Tests (frontend)</code> - <code>Run Jest Tests (smoker)</code> - <code>Test Packages</code> - <code>Lint Check</code> - <code>Build Check (frontend)</code> - <code>Build Check (smoker)</code> - <code>All Tests Status</code></p>"},{"location":"CI-CD/github-actions/#debugging-failed-ci","title":"Debugging Failed CI","text":"<ol> <li>Check Actions Tab: View detailed logs for failed jobs</li> <li>Local Testing: Run the same commands locally:    <pre><code>cd apps/[app-name]\nnpm ci --legacy-peer-deps\nnpm test\n</code></pre></li> <li>TypeScript Issues: Check compilation:    <pre><code>cd apps/[app-name]\nnpx tsc --noEmit\n</code></pre></li> <li>Build Issues: Test builds locally:    <pre><code>cd apps/frontend  # or apps/smoker\nnpm run build\n</code></pre></li> </ol>"},{"location":"CI-CD/github-actions/#performance","title":"Performance","text":"<ul> <li>Parallel Execution: Apps tested simultaneously for speed</li> <li>Caching: Node modules cached between runs</li> <li>Timeouts: Jobs timeout after 15 minutes to prevent hanging</li> <li>Artifacts: Test coverage and results preserved for 7 days</li> </ul>"},{"location":"CI-CD/github-actions/#adding-new-applications","title":"Adding New Applications","text":"<p>When adding new apps to the monorepo:</p> <ol> <li> <p>Update Matrix Strategy in <code>ci-tests.yml</code>:    <pre><code>strategy:\n  matrix:\n    app: [backend, device-service, frontend, smoker, new-app]\n    include:\n      - app: new-app\n        path: apps/new-app\n        test-command: npm test\n</code></pre></p> </li> <li> <p>Update Branch Protection to include new status checks</p> </li> <li> <p>Ensure Testing Setup follows the patterns in <code>docs/Packages/testing-template.md</code></p> </li> </ol>"},{"location":"CI-CD/github-actions/#documentation-dependencies","title":"Documentation Dependencies","text":"<p>MkDocs dependencies are managed through <code>mise</code> tasks: - MkDocs: 1.6.1 - Material Theme: 9.6.15 - Installation: <code>mise run docs-install</code> (defined in <code>mise.toml</code>)</p>"},{"location":"CI-CD/manual-version-deployment/","title":"Manual Version Deployment","text":""},{"location":"CI-CD/manual-version-deployment/#overview","title":"Overview","text":"<p>Deploy a specific container version to the cloud environment using Docker Compose or GitHub Actions. Images are tagged with immutable semantic versions (<code>vX.Y.Z</code>), and production deploys pin to a chosen version while development may use <code>nightly</code>.</p>"},{"location":"CI-CD/manual-version-deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed on the target host</li> <li>Access to the repository (Actions runner or shell on the cloud host)</li> <li>Environment values for <code>VAPID_PUBLIC_KEY</code> and <code>VAPID_PRIVATE_KEY</code></li> <li>Images published in Docker Hub with version tags (e.g., <code>v1.2.3</code>)</li> </ul>"},{"location":"CI-CD/manual-version-deployment/#option-a-github-release-preferred","title":"Option A: GitHub Release (Preferred)","text":"<ul> <li>Workflow: <code>.github/workflows/release.yml</code> (triggered by Release \u2192 Published)</li> <li>Tag format: <code>vX.Y.Z</code> (e.g., <code>v1.2.3</code>)</li> <li>Behavior: Builds from the tag, publishes Docker images with both <code>latest</code> and <code>vX.Y.Z</code>, and deploys cloud pinned to <code>vX.Y.Z</code>.</li> </ul> <p>Steps: 1) GitHub \u2192 Releases \u2192 \u201cDraft a new release\u201d 2) Set tag to <code>vX.Y.Z</code> and publish 3) The workflow runs automatically and deploys cloud with <code>VERSION=vX.Y.Z</code></p> <p>Notes: - Smoker devices auto-update from <code>latest</code> via Watchtower; the release also updates <code>latest</code>. - Use \u201cDeploy Version\u201d for redeploying an existing version without rebuilding.</p>"},{"location":"CI-CD/manual-version-deployment/#option-b-github-actions-deploy-version","title":"Option B: GitHub Actions (Deploy Version)","text":"<ul> <li>Workflow: <code>.github/workflows/deploy-version.yml</code></li> <li>Inputs: <code>version</code> (accepts <code>1.2.3</code>, <code>v1.2.3</code>, or <code>nightly</code>), toggles for cloud/smoker</li> <li>Runner: Cloud uses self-hosted <code>SmokeCloud</code>; smoker uses <code>Smoker</code></li> </ul> <p>Steps: 1) Actions \u2192 \u201cDeploy Version\u201d \u2192 Run workflow 2) Enter the version (<code>1.2.3</code>, <code>v1.2.3</code>, or <code>nightly</code>). The workflow normalizes to <code>vX.Y.Z</code> when needed. 3) Select deploy targets (cloud and/or smoker) 4) The workflow calls existing deploy jobs and executes on the target runners:    - <code>docker compose -f cloud.docker-compose.yml pull</code>    - <code>docker compose -f cloud.docker-compose.yml build</code>    - <code>docker compose -f cloud.docker-compose.yml down</code>    - <code>docker compose -f cloud.docker-compose.yml up -d --force-recreate</code></p> <p>Notes: - Secrets <code>VAPID_PUBLIC_KEY</code> and <code>VAPID_PRIVATE_KEY</code> are used by the workflow - Ensure the target version exists in Docker Hub for both backend and frontend images - Smoker deploys always use <code>latest</code> images (Watchtower), so the \u201cDeploy Version\u201d smoker option restarts services to pull <code>latest</code>.</p>"},{"location":"CI-CD/manual-version-deployment/#option-b-local-shell-on-cloud-host","title":"Option B: Local Shell on Cloud Host","text":"<p>The compose file supports a <code>VERSION</code> env var. Set it to a specific version tag to pin the deployment:</p> <p>Quick commands: <pre><code>VERSION=v1.2.3 \\\nVAPID_PUBLIC_KEY=&lt;your_public_key&gt; \\\nVAPID_PRIVATE_KEY=&lt;your_private_key&gt; \\\ndocker compose -f cloud.docker-compose.yml pull\n\nVERSION=v1.2.3 \\\nVAPID_PUBLIC_KEY=&lt;your_public_key&gt; \\\nVAPID_PRIVATE_KEY=&lt;your_private_key&gt; \\\ndocker compose -f cloud.docker-compose.yml up -d --force-recreate\n</code></pre></p> <p>Note: We previously supported a helper script and mise tasks, but deployment is now standardized via GitHub Actions or direct Docker Compose commands shown above.</p>"},{"location":"CI-CD/manual-version-deployment/#rollback","title":"Rollback","text":"<p>Rollback is identical to deployment\u2014pin to a previous version tag: <pre><code>VERSION=v1.2.2 docker compose -f cloud.docker-compose.yml up -d --force-recreate\n</code></pre></p>"},{"location":"CI-CD/manual-version-deployment/#verification","title":"Verification","text":"<p>After deployment: - <code>docker ps</code> shows updated containers - Backend reachable at configured port (default 8443) - Frontend reachable at configured port (default 80) - Check application logs for healthy startup</p>"},{"location":"CI-CD/manual-version-deployment/#related-references","title":"Related References","text":"<ul> <li><code>cloud.docker-compose.yml</code></li> <li><code>docs/Infrastructure/phase-1-container-standardization.md</code></li> <li><code>.github/workflows/cloud-deploy.yml</code></li> </ul>"},{"location":"CI-CD/test-coverage/","title":"Test Coverage Reports","text":"<p>This guide explains how to generate, view, and interpret test coverage reports for the Smart Smoker V2 project.</p>"},{"location":"CI-CD/test-coverage/#overview","title":"Overview","text":"<p>Test coverage measures how much of your code is executed when your test suite runs. It helps identify: - Untested code paths - Areas that need more comprehensive testing - Code quality metrics for CI/CD pipelines</p>"},{"location":"CI-CD/test-coverage/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Minimum: 70% code coverage across all applications</li> <li>Target: 80%+ code coverage</li> <li>Critical Components: State management, WebSocket communication, and hardware interfaces should have &gt;90% coverage</li> </ul>"},{"location":"CI-CD/test-coverage/#generating-coverage-reports","title":"Generating Coverage Reports","text":""},{"location":"CI-CD/test-coverage/#backend-service","title":"Backend Service","text":"<pre><code>cd apps/backend\nnpm run test:cov\n</code></pre>"},{"location":"CI-CD/test-coverage/#device-service","title":"Device Service","text":"<pre><code>cd apps/device-service\nnpm run test:cov\n</code></pre>"},{"location":"CI-CD/test-coverage/#frontend-react","title":"Frontend (React)","text":"<pre><code>cd apps/frontend\nnpm test -- --coverage --watchAll=false\n</code></pre>"},{"location":"CI-CD/test-coverage/#smoker-app-electron","title":"Smoker App (Electron)","text":"<pre><code>cd apps/smoker\nnpm test -- --coverage --watchAll=false\n</code></pre>"},{"location":"CI-CD/test-coverage/#temperaturechart-package","title":"TemperatureChart Package","text":"<pre><code>cd packages/TemperatureChart\nnpm run test:coverage\n</code></pre>"},{"location":"CI-CD/test-coverage/#viewing-coverage-reports","title":"Viewing Coverage Reports","text":""},{"location":"CI-CD/test-coverage/#terminal-output","title":"Terminal Output","text":"<p>When you run coverage tests, you'll see a table like this:</p> <pre><code>---------------|---------|----------|---------|---------|-------------------\nFile           | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s \n---------------|---------|----------|---------|---------|-------------------\nAll files      |   85.32 |    78.45 |   92.11 |   84.67 |                   \n src/          |   88.42 |    82.14 |   95.23 |   87.91 |                   \n  service.ts   |   92.15 |    85.71 |  100.00 |   91.43 | 45-48,92          \n  controller.ts|   84.62 |    75.00 |   90.00 |   83.33 | 12,34-36          \n---------------|---------|----------|---------|---------|-------------------\n</code></pre> <p>Metrics Explained: - % Stmts: Percentage of statements executed - % Branch: Percentage of conditional branches taken - % Funcs: Percentage of functions called - % Lines: Percentage of executable lines covered - Uncovered Line #s: Specific line numbers not covered by tests</p>"},{"location":"CI-CD/test-coverage/#html-reports","title":"HTML Reports","text":"<p>Coverage generates detailed HTML reports you can view in a browser:</p> <pre><code># After running coverage, open the HTML report\ncd apps/backend\n# Coverage report is in: coverage/lcov-report/index.html\n\n# Open in browser (Linux)\nxdg-open coverage/lcov-report/index.html\n\n# Or use VS Code Live Server extension\ncode coverage/lcov-report/index.html\n</code></pre>"},{"location":"CI-CD/test-coverage/#coverage-report-structure","title":"Coverage Report Structure","text":"<pre><code>coverage/\n\u251c\u2500\u2500 lcov-report/           # Interactive HTML reports\n\u2502   \u251c\u2500\u2500 index.html        # Main coverage dashboard\n\u2502   \u251c\u2500\u2500 [file].html       # Individual file reports\n\u2502   \u2514\u2500\u2500 assets/           # CSS, JS, and icons\n\u251c\u2500\u2500 coverage-final.json   # Raw coverage data\n\u251c\u2500\u2500 lcov.info            # LCOV format for CI tools\n\u2514\u2500\u2500 clover.xml           # Clover format for CI tools\n</code></pre>"},{"location":"CI-CD/test-coverage/#html-report-features","title":"HTML Report Features","text":""},{"location":"CI-CD/test-coverage/#main-dashboard-indexhtml","title":"Main Dashboard (<code>index.html</code>)","text":"<ul> <li>Overall Statistics: Project-wide coverage percentages</li> <li>File List: All files with individual coverage metrics</li> <li>Sortable Columns: Click headers to sort by different metrics</li> <li>Color Coding: </li> <li>\ud83d\udfe2 Green: Good coverage (&gt;80%)</li> <li>\ud83d\udfe1 Yellow: Moderate coverage (60-80%)</li> <li>\ud83d\udd34 Red: Poor coverage (&lt;60%)</li> </ul>"},{"location":"CI-CD/test-coverage/#individual-file-reports","title":"Individual File Reports","text":"<p>Click any file to see: - Line-by-line coverage: Shows which lines were executed - Branch coverage: Highlights conditional statements - Function coverage: Shows which functions were called - Source code view: Syntax-highlighted code with coverage overlay</p>"},{"location":"CI-CD/test-coverage/#coverage-indicators","title":"Coverage Indicators","text":"<ul> <li>Green highlight: Line was executed</li> <li>Red highlight: Line was not executed  </li> <li>Yellow highlight: Branch was partially executed</li> <li>Gray line numbers: Non-executable lines (comments, declarations)</li> </ul>"},{"location":"CI-CD/test-coverage/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"CI-CD/test-coverage/#github-actions-coverage","title":"GitHub Actions Coverage","text":"<p>The CI pipeline automatically generates coverage reports for all applications:</p> <pre><code># Excerpt from .github/workflows/ci-tests.yml\n- name: Test with coverage\n  run: npm run test:cov\n  working-directory: ./apps/${{ matrix.app }}\n</code></pre>"},{"location":"CI-CD/test-coverage/#viewing-ci-coverage","title":"Viewing CI Coverage","text":"<ol> <li>Go to your GitHub repository</li> <li>Navigate to Actions tab</li> <li>Click on a workflow run</li> <li>Expand the Test Coverage section</li> <li>Download artifacts containing coverage reports</li> </ol>"},{"location":"CI-CD/test-coverage/#coverage-artifacts","title":"Coverage Artifacts","text":"<p>CI preserves coverage reports as downloadable artifacts for 7 days: - <code>backend-coverage</code> - <code>device-service-coverage</code> - <code>frontend-coverage</code> - <code>smoker-coverage</code> - <code>packages-coverage</code></p>"},{"location":"CI-CD/test-coverage/#coverage-configuration","title":"Coverage Configuration","text":""},{"location":"CI-CD/test-coverage/#jest-configuration-backenddevice-service","title":"Jest Configuration (Backend/Device Service)","text":"<pre><code>{\n  \"jest\": {\n    \"collectCoverageFrom\": [\n      \"**/*.(t|j)s\",\n      \"!**/*.spec.ts\",\n      \"!**/*.interface.ts\",\n      \"!**/node_modules/**\"\n    ],\n    \"coverageDirectory\": \"../coverage\",\n    \"coverageReporters\": [\"html\", \"text\", \"lcov\", \"clover\"]\n  }\n}\n</code></pre>"},{"location":"CI-CD/test-coverage/#react-testing-configuration-frontendsmoker","title":"React Testing Configuration (Frontend/Smoker)","text":"<pre><code>{\n  \"collectCoverageFrom\": [\n    \"src/**/*.{js,jsx,ts,tsx}\",\n    \"!src/**/*.d.ts\",\n    \"!src/index.tsx\",\n    \"!src/reportWebVitals.ts\"\n  ]\n}\n</code></pre>"},{"location":"CI-CD/test-coverage/#package-testing-configuration","title":"Package Testing Configuration","text":"<pre><code>{\n  \"collectCoverageFrom\": [\n    \"src/**/*.{ts,tsx}\",\n    \"!src/**/*.d.ts\",\n    \"!src/setupTests.ts\",\n    \"!src/__mocks__/**\"\n  ]\n}\n</code></pre>"},{"location":"CI-CD/test-coverage/#improving-coverage","title":"Improving Coverage","text":""},{"location":"CI-CD/test-coverage/#identifying-gaps","title":"Identifying Gaps","text":"<ol> <li>Run coverage report</li> <li>Open HTML dashboard</li> <li>Sort by lowest coverage (click % Stmts column)</li> <li>Click on files with poor coverage</li> <li>Review highlighted code to see untested paths</li> </ol>"},{"location":"CI-CD/test-coverage/#common-untested-areas","title":"Common Untested Areas","text":"<ul> <li>Error handling: catch blocks, error callbacks</li> <li>Edge cases: boundary conditions, null checks</li> <li>Async operations: promise rejections, timeouts</li> <li>Event handlers: user interactions, WebSocket events</li> <li>Configuration: environment-specific code paths</li> </ul>"},{"location":"CI-CD/test-coverage/#writing-tests-for-coverage","title":"Writing Tests for Coverage","text":"<pre><code>// Example: Testing error handling\ndescribe('UserService', () =&gt; {\n  it('should handle network errors gracefully', async () =&gt; {\n    // Mock network failure\n    jest.spyOn(axios, 'get').mockRejectedValue(new Error('Network error'));\n\n    // Test error path\n    await expect(userService.getUser('123')).rejects.toThrow('Network error');\n  });\n\n  it('should handle invalid user IDs', async () =&gt; {\n    // Test validation path\n    await expect(userService.getUser('')).rejects.toThrow('Invalid user ID');\n  });\n});\n</code></pre>"},{"location":"CI-CD/test-coverage/#coverage-best-practices","title":"Coverage Best Practices","text":""},{"location":"CI-CD/test-coverage/#1-focus-on-quality-not-just-quantity","title":"1. Focus on Quality, Not Just Quantity","text":"<ul> <li>100% coverage doesn't guarantee bug-free code</li> <li>Test meaningful scenarios, not just lines</li> <li>Prioritize critical business logic</li> </ul>"},{"location":"CI-CD/test-coverage/#2-test-different-code-paths","title":"2. Test Different Code Paths","text":"<pre><code>// Good: Test both success and failure paths\nit('should handle valid input', () =&gt; { /* test success */ });\nit('should handle invalid input', () =&gt; { /* test failure */ });\nit('should handle network timeout', () =&gt; { /* test timeout */ });\n</code></pre>"},{"location":"CI-CD/test-coverage/#3-use-coverage-to-guide-testing","title":"3. Use Coverage to Guide Testing","text":"<ul> <li>Identify untested functions</li> <li>Add tests for complex logic</li> <li>Verify error handling</li> </ul>"},{"location":"CI-CD/test-coverage/#4-regular-coverage-monitoring","title":"4. Regular Coverage Monitoring","text":"<ul> <li>Check coverage in code reviews</li> <li>Set up coverage thresholds in CI</li> <li>Track coverage trends over time</li> </ul>"},{"location":"CI-CD/test-coverage/#troubleshooting-coverage-issues","title":"Troubleshooting Coverage Issues","text":""},{"location":"CI-CD/test-coverage/#low-coverage-due-to-generated-code","title":"Low Coverage Due to Generated Code","text":"<pre><code>{\n  \"collectCoverageFrom\": [\n    \"src/**/*.{ts,tsx}\",\n    \"!src/**/*.generated.ts\",\n    \"!src/migrations/**\",\n    \"!src/**/*.interface.ts\"\n  ]\n}\n</code></pre>"},{"location":"CI-CD/test-coverage/#typescript-declaration-files","title":"TypeScript Declaration Files","text":"<p>Exclude <code>.d.ts</code> files as they're not executable: <pre><code>{\n  \"collectCoverageFrom\": [\n    \"src/**/*.{ts,tsx}\",\n    \"!src/**/*.d.ts\"\n  ]\n}\n</code></pre></p>"},{"location":"CI-CD/test-coverage/#node-modules-inclusion","title":"Node Modules Inclusion","text":"<p>Ensure <code>node_modules</code> are excluded: <pre><code>{\n  \"coveragePathIgnorePatterns\": [\n    \"/node_modules/\",\n    \"/coverage/\"\n  ]\n}\n</code></pre></p>"},{"location":"CI-CD/test-coverage/#coverage-thresholds","title":"Coverage Thresholds","text":""},{"location":"CI-CD/test-coverage/#setting-minimum-thresholds","title":"Setting Minimum Thresholds","text":"<pre><code>{\n  \"jest\": {\n    \"coverageThreshold\": {\n      \"global\": {\n        \"branches\": 70,\n        \"functions\": 70,\n        \"lines\": 70,\n        \"statements\": 70\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"CI-CD/test-coverage/#per-directory-thresholds","title":"Per-Directory Thresholds","text":"<pre><code>{\n  \"coverageThreshold\": {\n    \"global\": {\n      \"statements\": 80\n    },\n    \"./src/critical/\": {\n      \"statements\": 95\n    }\n  }\n}\n</code></pre>"},{"location":"CI-CD/test-coverage/#advanced-coverage-analysis","title":"Advanced Coverage Analysis","text":""},{"location":"CI-CD/test-coverage/#branch-coverage-deep-dive","title":"Branch Coverage Deep Dive","text":"<pre><code>// This function has 4 possible branches\nfunction processUser(user: User | null, isAdmin: boolean) {\n  if (!user) return null;           // Branch 1: user is null\n  if (isAdmin) {                    // Branch 2: isAdmin is true\n    return user.adminData;          // Branch 3: admin path\n  }\n  return user.userData;             // Branch 4: user path\n}\n\n// Tests needed for 100% branch coverage:\n// 1. processUser(null, false)     -&gt; Branch 1\n// 2. processUser(user, true)      -&gt; Branch 2 + 3  \n// 3. processUser(user, false)     -&gt; Branch 4\n</code></pre>"},{"location":"CI-CD/test-coverage/#function-coverage","title":"Function Coverage","text":"<ul> <li>Ensure every function is called at least once</li> <li>Constructor functions need instantiation tests</li> <li>Arrow functions need execution tests</li> </ul>"},{"location":"CI-CD/test-coverage/#statement-coverage","title":"Statement Coverage","text":"<ul> <li>Every executable line should run</li> <li>Variable declarations with complex initializers</li> <li>Return statements in all code paths</li> </ul> <p>By following this guide, you'll have comprehensive visibility into your code's test coverage and can make informed decisions about where to focus your testing efforts.</p>"},{"location":"CI-CD/testing-library-migration/","title":"Testing Library Best Practices Migration Guide","text":""},{"location":"CI-CD/testing-library-migration/#overview","title":"Overview","text":"<p>This guide addresses the systematic migration needed to fix Testing Library rule violations across the Frontend and Smoker applications. We currently have 240+ violations that need to be addressed.</p>"},{"location":"CI-CD/testing-library-migration/#issue-categories","title":"Issue Categories","text":""},{"location":"CI-CD/testing-library-migration/#1-multiple-assertions-in-waitfor-87-violations","title":"1. Multiple Assertions in <code>waitFor</code> (87 violations)","text":"<p>Problem: Multiple assertions within a single <code>waitFor</code> callback <pre><code>// \u274c Bad\nawait waitFor(() =&gt; {\n  expect(element1).toBeInTheDocument();\n  expect(element2).toHaveTextContent('test');\n  expect(element3).toBeVisible();\n});\n</code></pre></p> <p>Solution: Split into separate <code>waitFor</code> calls or use separate assertions <pre><code>// \u2705 Good\nawait waitFor(() =&gt; expect(element1).toBeInTheDocument());\nawait waitFor(() =&gt; expect(element2).toHaveTextContent('test'));\nawait waitFor(() =&gt; expect(element3).toBeVisible());\n\n// \u2705 Or if they're related to the same condition\nawait waitFor(() =&gt; expect(element1).toBeInTheDocument());\nexpect(element2).toHaveTextContent('test');\nexpect(element3).toBeVisible();\n</code></pre></p>"},{"location":"CI-CD/testing-library-migration/#2-unnecessary-act-wrapping-65-violations","title":"2. Unnecessary <code>act</code> Wrapping (65 violations)","text":"<p>Problem: Wrapping Testing Library utilities in <code>act</code> unnecessarily <pre><code>// \u274c Bad\nawait act(async () =&gt; {\n  fireEvent.click(button);\n});\n</code></pre></p> <p>Solution: Remove <code>act</code> wrapper for Testing Library utilities <pre><code>// \u2705 Good\nfireEvent.click(button);\n// or for user events\nawait user.click(button);\n</code></pre></p>"},{"location":"CI-CD/testing-library-migration/#3-direct-node-access-45-violations","title":"3. Direct Node Access (45 violations)","text":"<p>Problem: Accessing DOM nodes directly instead of using Testing Library queries <pre><code>// \u274c Bad\nconst element = container.firstChild;\nconst buttons = container.querySelectorAll('button');\n</code></pre></p> <p>Solution: Use Testing Library queries <pre><code>// \u2705 Good\nconst element = screen.getByRole('button');\nconst buttons = screen.getAllByRole('button');\n</code></pre></p>"},{"location":"CI-CD/testing-library-migration/#4-side-effects-in-waitfor-35-violations","title":"4. Side Effects in <code>waitFor</code> (35 violations)","text":"<p>Problem: Performing side effects inside <code>waitFor</code> callbacks <pre><code>// \u274c Bad\nawait waitFor(() =&gt; {\n  fireEvent.click(button);\n  expect(result).toBe(true);\n});\n</code></pre></p> <p>Solution: Move side effects outside <code>waitFor</code> <pre><code>// \u2705 Good\nfireEvent.click(button);\nawait waitFor(() =&gt; expect(result).toBe(true));\n</code></pre></p>"},{"location":"CI-CD/testing-library-migration/#5-using-container-methods-8-violations","title":"5. Using Container Methods (8 violations)","text":"<p>Problem: Using <code>container</code> methods instead of <code>screen</code> queries <pre><code>// \u274c Bad\nconst { container } = render(&lt;Component /&gt;);\nconst element = container.querySelector('.class-name');\n</code></pre></p> <p>Solution: Use <code>screen</code> queries with appropriate roles/labels <pre><code>// \u2705 Good\nrender(&lt;Component /&gt;);\nconst element = screen.getByRole('button', { name: /submit/i });\n// or\nconst element = screen.getByTestId('submit-button');\n</code></pre></p>"},{"location":"CI-CD/testing-library-migration/#migration-strategy","title":"Migration Strategy","text":""},{"location":"CI-CD/testing-library-migration/#phase-1-quick-wins-low-risk","title":"Phase 1: Quick Wins (Low Risk)","text":"<ol> <li>Remove unnecessary <code>act</code> wrappers</li> <li>Fix unused variable warnings</li> <li>Remove unnecessary imports</li> <li>Fix escape character issues</li> </ol>"},{"location":"CI-CD/testing-library-migration/#phase-2-testing-library-queries-medium-risk","title":"Phase 2: Testing Library Queries (Medium Risk)","text":"<ol> <li>Replace <code>container.querySelector</code> with <code>screen.getBy*</code> queries</li> <li>Replace direct node access with proper queries</li> <li>Add missing <code>data-testid</code> attributes where needed</li> </ol>"},{"location":"CI-CD/testing-library-migration/#phase-3-async-testing-patterns-high-risk","title":"Phase 3: Async Testing Patterns (High Risk)","text":"<ol> <li>Split multiple assertions in <code>waitFor</code></li> <li>Move side effects out of <code>waitFor</code> callbacks</li> <li>Review and optimize async test patterns</li> </ol>"},{"location":"CI-CD/testing-library-migration/#phase-4-validation","title":"Phase 4: Validation","text":"<ol> <li>Run full test suite to ensure no regressions</li> <li>Verify coverage thresholds are maintained</li> <li>Update test documentation</li> </ol>"},{"location":"CI-CD/testing-library-migration/#file-priority-order","title":"File Priority Order","text":""},{"location":"CI-CD/testing-library-migration/#frontend-app-148-violations","title":"Frontend App (148 violations)","text":"<ol> <li>High Priority (&gt;20 violations):</li> <li><code>components/settings/notifications.test.tsx</code> (36 violations)</li> <li><code>components/history/history.test.tsx</code> (11 violations)</li> <li> <p><code>components/common/components/DynamicList.test.tsx</code> (25 violations)</p> </li> <li> <p>Medium Priority (5-20 violations):</p> </li> <li><code>components/smoke/smoke-simple.test.tsx</code> (7 violations)</li> <li><code>components/smoke/smoke.test.tsx</code> (11 violations)</li> <li> <p><code>components/smoke/smokeStep/smokeStep.test.tsx</code> (19 violations)</p> </li> <li> <p>Low Priority (&lt;5 violations):</p> </li> <li><code>App.test.tsx</code> (fixed)</li> <li><code>components/history/smokeCards/ratingsCard.test.tsx</code> (1 violation)</li> <li>Various component files with unused imports</li> </ol>"},{"location":"CI-CD/testing-library-migration/#smoker-app-92-violations","title":"Smoker App (92 violations)","text":"<ol> <li>High Priority:</li> <li><code>components/home/home.test.tsx</code> (43 violations)</li> <li><code>components/home/wifi/wifi.test.tsx</code> (46 violations)</li> </ol>"},{"location":"CI-CD/testing-library-migration/#tools-and-commands","title":"Tools and Commands","text":""},{"location":"CI-CD/testing-library-migration/#lint-specific-files","title":"Lint specific files","text":"<pre><code># Check specific file\ncd apps/frontend &amp;&amp; npx eslint src/components/settings/notifications.test.tsx\n\n# Fix auto-fixable issues\ncd apps/frontend &amp;&amp; npx eslint src/components/settings/notifications.test.tsx --fix\n</code></pre>"},{"location":"CI-CD/testing-library-migration/#run-tests-for-specific-files","title":"Run tests for specific files","text":"<pre><code># Test specific file\ncd apps/frontend &amp;&amp; npm test -- notifications.test.tsx\n\n# Test with coverage\ncd apps/frontend &amp;&amp; npm run test:cov -- notifications.test.tsx\n</code></pre>"},{"location":"CI-CD/testing-library-migration/#best-practices-going-forward","title":"Best Practices Going Forward","text":""},{"location":"CI-CD/testing-library-migration/#1-use-modern-testing-library-patterns","title":"1. Use Modern Testing Library Patterns","text":"<pre><code>// Use user events for interactions\nimport { user } from '@testing-library/user-event';\nawait user.click(button);\nawait user.type(input, 'text');\n\n// Use proper queries with good selectors\nscreen.getByRole('button', { name: /submit/i })\nscreen.getByLabelText(/email address/i)\nscreen.getByText(/loading/i)\n</code></pre>"},{"location":"CI-CD/testing-library-migration/#2-structure-async-tests-properly","title":"2. Structure Async Tests Properly","text":"<pre><code>test('async behavior', async () =&gt; {\n  render(&lt;Component /&gt;);\n\n  // Perform actions\n  fireEvent.click(screen.getByRole('button'));\n\n  // Wait for results\n  await waitFor(() =&gt; \n    expect(screen.getByText('success')).toBeInTheDocument()\n  );\n\n  // Additional synchronous assertions\n  expect(screen.getByText('completed')).toBeInTheDocument();\n});\n</code></pre>"},{"location":"CI-CD/testing-library-migration/#3-use-appropriate-queries","title":"3. Use Appropriate Queries","text":"<ul> <li><code>getByRole</code> - Primary choice for interactive elements</li> <li><code>getByLabelText</code> - Form inputs</li> <li><code>getByText</code> - Text content</li> <li><code>getByTestId</code> - Last resort for complex scenarios</li> </ul>"},{"location":"CI-CD/testing-library-migration/#4-avoid-common-anti-patterns","title":"4. Avoid Common Anti-patterns","text":"<ul> <li>Don't use <code>act</code> with Testing Library utilities</li> <li>Don't access DOM nodes directly</li> <li>Don't put multiple assertions in <code>waitFor</code></li> <li>Don't perform side effects in <code>waitFor</code></li> </ul>"},{"location":"CI-CD/testing-library-migration/#implementation-timeline","title":"Implementation Timeline","text":"<p>Week 1: Quick wins (unused imports, simple fixes) Week 2-3: Testing Library queries migration Week 4-5: Async testing patterns refactor Week 6: Validation and documentation update</p> <p>This systematic approach ensures we maintain test quality while improving code standards.</p>"},{"location":"CI-CD/workflow-architecture/","title":"GitHub Actions Workflow Architecture","text":""},{"location":"CI-CD/workflow-architecture/#overview","title":"Overview","text":"<p>The Smart Smoker v2 project uses a clean, reusable workflow architecture that eliminates redundancy and provides clear separation of concerns. Each workflow has a single responsibility and can be composed together as needed.</p>"},{"location":"CI-CD/workflow-architecture/#current-workflow-architecture","title":"Current Workflow Architecture","text":""},{"location":"CI-CD/workflow-architecture/#core-reusable-workflows","title":"Core Reusable Workflows","text":""},{"location":"CI-CD/workflow-architecture/#1-installyml-dependency-management","title":"1. <code>install.yml</code> - Dependency Management","text":"<ul> <li>Purpose: Sets up Node.js environment and installs all dependencies</li> <li>Features:</li> <li>Workspace artifact upload for reuse across jobs</li> <li>Dependency caching for faster builds</li> <li>Single source of truth for environment setup</li> <li>Used by: Called internally by <code>build.yml</code></li> </ul>"},{"location":"CI-CD/workflow-architecture/#2-buildyml-application-builder","title":"2. <code>build.yml</code> - Application Builder","text":"<ul> <li>Purpose: Builds applications and optionally creates Docker images</li> <li>Modes:</li> <li><code>test</code>: Run Jest tests only</li> <li><code>build</code>: Build applications without Docker export</li> <li><code>build-and-export</code>: Build applications and export Docker images as artifacts</li> <li>Features:</li> <li>Calls <code>install.yml</code> internally for dependencies</li> <li>Matrix strategy for parallel builds</li> <li>Configurable app selection via JSON array</li> <li>Conditional Docker image export</li> </ul>"},{"location":"CI-CD/workflow-architecture/#3-publishyml-docker-hub-publisher","title":"3. <code>publish.yml</code> - Docker Hub Publisher","text":"<ul> <li>Purpose: Publishes Docker images to Docker Hub</li> <li>Features:</li> <li>Downloads image artifacts from build jobs</li> <li>Pushes to Docker Hub with version tags</li> <li>Automatic <code>latest</code> tagging for release versions</li> <li>Matrix strategy for parallel publishing</li> </ul>"},{"location":"CI-CD/workflow-architecture/#orchestrator-workflows","title":"Orchestrator Workflows","text":""},{"location":"CI-CD/workflow-architecture/#4-ci-testsyml-pull-request-validation","title":"4. <code>ci-tests.yml</code> - Pull Request Validation","text":"<ul> <li>Purpose: Validates code changes on pull requests</li> <li>Process:</li> <li>Run tests for all applications (calls <code>build.yml</code> with mode=\"test\")</li> <li>Build validation (calls <code>build.yml</code> with mode=\"build\")</li> <li>Benefits: Fast feedback, parallel execution, no redundant installs</li> </ul>"},{"location":"CI-CD/workflow-architecture/#5-releaseyml-production-deployment","title":"5. <code>release.yml</code> - Production Deployment","text":"<ul> <li>Purpose: Complete release pipeline for production</li> <li>Process:</li> <li>Build smoker apps (calls <code>build.yml</code> with mode=\"build-and-export\")</li> <li>Build cloud apps (calls <code>build.yml</code> with mode=\"build-and-export\") </li> <li>Publish all Docker images (calls <code>publish.yml</code>)</li> <li>Deploy to smoker devices (conditional)</li> <li>Deploy to cloud infrastructure (conditional)</li> </ul>"},{"location":"CI-CD/workflow-architecture/#deployment-workflows","title":"Deployment Workflows","text":""},{"location":"CI-CD/workflow-architecture/#6-smoker-deployyml-smoker-deployment","title":"6. <code>smoker-deploy.yml</code> - Smoker Deployment","text":"<ul> <li>Purpose: Deploys to smoker devices</li> <li>Unchanged: Existing deployment logic</li> </ul>"},{"location":"CI-CD/workflow-architecture/#7-cloud-deployyml-cloud-deployment","title":"7. <code>cloud-deploy.yml</code> - Cloud Deployment","text":"<ul> <li>Purpose: Deploys to cloud infrastructure</li> <li>Unchanged: Existing deployment logic</li> </ul>"},{"location":"CI-CD/workflow-architecture/#8-docsyml-documentation","title":"8. <code>docs.yml</code> - Documentation","text":"<ul> <li>Purpose: Builds and deploys documentation</li> <li>Unchanged: Existing MkDocs deployment</li> </ul>"},{"location":"CI-CD/workflow-architecture/#benefits-of-current-architecture","title":"Benefits of Current Architecture","text":""},{"location":"CI-CD/workflow-architecture/#1-resource-efficiency","title":"1. Resource Efficiency","text":"<ul> <li>Single <code>npm run bootstrap</code> per workflow execution (no redundant installs)</li> <li>Parallel builds with shared dependencies</li> <li>Efficient artifact-based image sharing</li> </ul>"},{"location":"CI-CD/workflow-architecture/#2-maintainability","title":"2. Maintainability","text":"<ul> <li>Single source of truth for setup logic (<code>install.yml</code>)</li> <li>Reusable components with clear responsibilities</li> <li>Clean separation of concerns (install \u2192 build \u2192 publish \u2192 deploy)</li> </ul>"},{"location":"CI-CD/workflow-architecture/#3-flexibility","title":"3. Flexibility","text":"<ul> <li>Easy to add new applications to build matrix</li> <li>Conditional publishing and deployment</li> <li>Composable workflows for different scenarios</li> </ul>"},{"location":"CI-CD/workflow-architecture/#4-developer-experience","title":"4. Developer Experience","text":"<ul> <li>Fast CI feedback through parallelization</li> <li>Clear workflow visualization in GitHub Actions</li> <li>Easy to debug specific stages independently</li> </ul>"},{"location":"CI-CD/workflow-architecture/#workflow-composition-examples","title":"Workflow Composition Examples","text":""},{"location":"CI-CD/workflow-architecture/#pull-request-testing","title":"Pull Request Testing","text":"<pre><code># ci-tests.yml calls:\nbuild.yml (mode: \"test\") \u2192 Tests all apps\nbuild.yml (mode: \"build\") \u2192 Validates builds\n</code></pre>"},{"location":"CI-CD/workflow-architecture/#production-release","title":"Production Release","text":"<pre><code># release.yml calls:\nbuild.yml (smoker apps, mode: \"build-and-export\") \u2192 Creates artifacts\nbuild.yml (cloud apps, mode: \"build-and-export\") \u2192 Creates artifacts\npublish.yml \u2192 Pushes all Docker images\nsmoker-deploy.yml \u2192 Deploys to devices (conditional)\ncloud-deploy.yml \u2192 Deploys to cloud (conditional)\n</code></pre>"},{"location":"CI-CD/workflow-architecture/#usage-examples","title":"Usage Examples","text":""},{"location":"CI-CD/workflow-architecture/#running-tests-only","title":"Running Tests Only","text":"<pre><code>uses: ./.github/workflows/build.yml\nwith:\n  apps: '[\"backend\", \"frontend\"]'\n  mode: \"test\"\n  ref: ${{ github.ref }}\n</code></pre>"},{"location":"CI-CD/workflow-architecture/#building-and-exporting-docker-images","title":"Building and Exporting Docker Images","text":"<pre><code>uses: ./.github/workflows/build.yml\nwith:\n  apps: '[\"smoker\", \"device-service\"]'\n  mode: \"build-and-export\"\n  version: \"1.0.0\"\n  ref: \"v1.0.0\"\n</code></pre>"},{"location":"CI-CD/workflow-architecture/#publishing-docker-images","title":"Publishing Docker Images","text":"<pre><code>uses: ./.github/workflows/publish.yml\nwith:\n  images: '[\"smoker_image\", \"backend_image\"]'\n  version: \"1.0.0\"\nsecrets: inherit\n</code></pre>"},{"location":"CI-CD/workflow-architecture/#current-file-structure","title":"Current File Structure","text":"<pre><code>.github/workflows/\n\u251c\u2500\u2500 # Core Reusable Workflows\n\u251c\u2500\u2500 install.yml              # Dependency setup &amp; workspace artifacts\n\u251c\u2500\u2500 build.yml                # Application building &amp; Docker image creation\n\u251c\u2500\u2500 publish.yml              # Docker Hub publishing\n\u251c\u2500\u2500 \n\u251c\u2500\u2500 # Orchestrator Workflows  \n\u251c\u2500\u2500 ci-tests.yml             # PR validation &amp; testing\n\u251c\u2500\u2500 release.yml              # Production release pipeline\n\u251c\u2500\u2500 \n\u251c\u2500\u2500 # Deployment Workflows\n\u251c\u2500\u2500 smoker-deploy.yml        # Smoker device deployment\n\u251c\u2500\u2500 cloud-deploy.yml         # Cloud infrastructure deployment\n\u2514\u2500\u2500 docs.yml                 # Documentation deployment\n</code></pre>"},{"location":"CI-CD/workflow-architecture/#architecture-principles","title":"Architecture Principles","text":"<ol> <li>Single Responsibility: Each workflow does one thing well</li> <li>Composable: Workflows can be combined for different scenarios  </li> <li>Reusable: No duplicate logic across workflows</li> <li>Testable: Each component can be tested independently</li> <li>Maintainable: Clear ownership and minimal interdependencies</li> </ol>"},{"location":"CI-CD/workflow-architecture/#adding-new-applications","title":"Adding New Applications","text":"<p>To add a new application to the build pipeline:</p> <ol> <li>Add to build matrix: Include app name in the <code>apps</code> JSON array</li> <li>Update build.yml: Add build commands for the new app if needed</li> <li>Update Dockerfiles: Ensure proper Dockerfile exists</li> <li>Test locally: Run the workflow with the new app included</li> </ol> <p>No changes needed to core workflow logic - the architecture is designed to scale.</p>"},{"location":"Device%20Service/","title":"Getting Started","text":"<p>after installing (on the welcome page)</p> <p>just run  <code>npm run start</code> </p> <p>This will start the device service using fake generated temps, these temps can be changed in the <code>generateTemps</code> function in the <code>serial.service.ts</code> file</p>"},{"location":"Device%20Service/#components","title":"Components","text":""},{"location":"Device%20Service/#wifi","title":"Wifi","text":"<p>This service uses the network-manager linux package to manage the wifi changing. This required network-manager to be install on the pi and to disable the wpa service that is on by default.</p>"},{"location":"Device%20Service/#serial-port","title":"Serial port","text":"<p>This stuff can be found in the <code>serial.service.ts</code>. if you go there it probs tells you what you need  defaults to <code>/dev/ttyS0</code> port, this is the port for the Pi's RX and Tx pins.</p>"},{"location":"Device%20Service/#websocket","title":"Websocket","text":"<p>takes the serial read an uses a websocket to shoot the temps over to the smoker frontend</p>"},{"location":"Epics/googleAssistant/","title":"Google Assistant","text":"<p>I think I need to implement a cloud to cloud Action Cloud to cloud checklist</p> <p>From link above need to implement.</p> <ul> <li>O Auth 2.0 authentication server</li> <li>update fulfillment to process intents</li> <li>Test &amp; Field Trial via Google Home Test Suite</li> <li>Certify &amp; Launch</li> </ul> <p>Helpful Links</p> <ul> <li>getting started</li> <li>cloud to cloud lunch action</li> </ul>"},{"location":"Epics/pushNotifications/","title":"Push Notifications","text":""},{"location":"Epics/pushNotifications/#set-up","title":"Set up","text":"<p>This app uses the npm web push module</p> <p>This requires you to generate VAPID keys. These keys are one time generated and stored in github secrets. to generate these key run this code and copy the printed keys <pre><code>const vapidKeys = webpush.generateVAPIDKeys();\n\n// Prints 2 URL Safe Base64 Encoded Strings\nconsole.log(vapidKeys.publicKey, vapidKeys.privateKey);\n</code></pre></p> <p>Once that is done put those keys into your .env.local and you should be good to test</p>"},{"location":"Epics/pushNotifications/#how-it-works","title":"How it works","text":"<p>In the frontend add on start up we:</p> <ol> <li> <p>The code checks if the serviceWorker property is in the navigator object. This is to ensure that the user's browser supports service workers.</p> </li> <li> <p>If service workers are supported, the service worker file located at /sw.js is registered.</p> </li> <li> <p>Once the service worker is registered successfully, the code checks if the PushManager property is in the window object. This is to ensure that the user's browser supports push notifications.</p> </li> <li> <p>If push notifications are supported, the code subscribes to push notifications using the pushManager.subscribe method. The userVisibleOnly: true option means that the push subscription will only be used for messages whose effect is made visible to the user. The applicationServerKey is set to the result of the urlBase64ToUint8Array method called with the VAPID_PUBLIC_KEY environment variable. This key is used to identify your server to the push service.</p> </li> <li> <p>Once the user is subscribed to push notifications, the subscription is sent to the server. This is done by making a POST request to the /notifications/subscribe endpoint on your server, with the subscription as the request body.</p> </li> <li> <p>The backend server will then save the subscription into the DB if it is unique</p> </li> <li> <p>In the events.gateway for the websocket lives our pushNotification function that will detect when and what to send as a push notification to the user </p> </li> </ol> <p>Note</p> <p>Push notifications are set on a 10 minute timeout, once you get a notification it will not send that same one for another 10 minutes</p>"},{"location":"Frontend/","title":"Getting Started","text":"<p>after installing (on the welcome page)</p> <p>just run  <code>npm run start</code>  and  visit <code>http://localhost:3000</code> </p> <p>will need to run the backend in order for app to function and save data  so go to that page to set that up</p> <p>You will need to create a .env.local for this app. The values you need for this are as follows: * REACT_APP_CLOUD_URL=http://localhost:3001/api/ * WS_URL=http://localhost:3001 * VAPID_PUBLIC_KEY="},{"location":"Hardware/","title":"Hardware","text":""},{"location":"Hardware/#hardware-list","title":"Hardware list","text":""},{"location":"Hardware/#smoker","title":"Smoker","text":"<ul> <li>Raspberry pi 3</li> <li>Element 14 7\" Touchscreen display</li> <li>Arduino Nano</li> </ul>"},{"location":"Hardware/#cloud-server","title":"Cloud Server","text":"<ul> <li>Raspberry pi 4</li> </ul>"},{"location":"Hardware/#micro-controller","title":"Micro Controller","text":"<p>Breadboard diagram</p> <p></p> <p>PCB Schematics</p> <p></p> <p>PCB Diagram</p> <p></p>"},{"location":"Infrastructure/","title":"Infrastructure Documentation","text":""},{"location":"Infrastructure/#overview","title":"Overview","text":"<p>This documentation covers all aspects of Smart Smoker V2 infrastructure, organized by feature for easy navigation.</p>"},{"location":"Infrastructure/#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started - Quick start guide</li> <li>Architecture - System architecture overview</li> </ul>"},{"location":"Infrastructure/#features","title":"Features","text":""},{"location":"Infrastructure/#database","title":"Database","text":"<ul> <li>MongoDB - MongoDB configuration and authentication</li> <li>Backups - Automated backup system</li> </ul>"},{"location":"Infrastructure/#deployment","title":"Deployment","text":"<ul> <li>Automation - CI/CD and deployment workflows</li> <li>Health Checks - Docker health check configuration</li> <li>Rollback - Deployment rollback procedures</li> <li>Environments - Environment configuration</li> </ul>"},{"location":"Infrastructure/#infrastructure","title":"Infrastructure","text":"<ul> <li>Terraform - Infrastructure provisioning</li> <li>Proxmox - Proxmox infrastructure setup</li> <li>Containers - Container standardization</li> </ul>"},{"location":"Infrastructure/#configuration","title":"Configuration","text":"<ul> <li>Ansible - Ansible operations and playbooks</li> <li>System Setup - Base system configuration</li> </ul>"},{"location":"Infrastructure/#networking","title":"Networking","text":"<ul> <li>Tailscale - Tailscale setup and troubleshooting</li> <li>Network Config - Network configuration</li> </ul>"},{"location":"Infrastructure/#security","title":"Security","text":"<ul> <li>Secrets Management - Secrets and credential management</li> <li>Authentication - Authentication configuration</li> </ul>"},{"location":"Infrastructure/#operations","title":"Operations","text":"<ul> <li>Testing - Testing infrastructure and procedures</li> <li>Disaster Recovery - Recovery procedures</li> <li>Monitoring - Health monitoring and logging</li> </ul>"},{"location":"Infrastructure/#documentation-structure","title":"Documentation Structure","text":"<p>Documentation is organized by feature rather than by phase/story to make it easier to find information about specific functionality.</p> <p>Each feature document is self-contained and includes: - Overview - Configuration - Usage - Troubleshooting - Related documentation</p>"},{"location":"Infrastructure/#contributing","title":"Contributing","text":"<p>When adding new infrastructure features: 1. Create documentation in appropriate feature directory 2. Update this README with link to new documentation 3. Update mkdocs.yml navigation 4. Cross-reference related features</p> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/features/configuration/ansible/","title":"Ansible Operations Guide","text":"<p>This guide provides operational procedures for managing Smart Smoker infrastructure using Ansible.</p>"},{"location":"Infrastructure/features/configuration/ansible/#overview","title":"Overview","text":"<p>Ansible is used to configure and maintain all Proxmox LXC containers with Infrastructure as Code principles. All infrastructure changes should be made through Ansible playbooks rather than manual SSH configuration.</p>"},{"location":"Infrastructure/features/configuration/ansible/#infrastructure-components","title":"Infrastructure Components","text":""},{"location":"Infrastructure/features/configuration/ansible/#ansible-roles","title":"Ansible Roles","text":"<p>The infrastructure is managed through 7 specialized Ansible roles:</p> <ol> <li>common - Base system configuration</li> <li>SSH hardening (key-only authentication)</li> <li>UFW firewall configuration</li> <li>fail2ban for brute force protection</li> <li> <p>Base package installation</p> </li> <li> <p>docker - Container runtime</p> </li> <li>Docker Engine installation</li> <li>Docker Compose plugin</li> <li> <p>User permissions and daemon configuration</p> </li> <li> <p>terraform - Infrastructure tool (GitHub runner only)</p> </li> <li>Terraform CLI from HashiCorp repository</li> <li> <p>Latest stable version</p> </li> <li> <p>nodejs - Application runtime</p> </li> <li>Node.js 20 LTS from NodeSource</li> <li> <p>npm package manager</p> </li> <li> <p>github-runner - CI/CD runner</p> </li> <li>GitHub Actions runner download &amp; setup</li> <li> <p>Service configuration and registration</p> </li> <li> <p>cloud-app - Cloud application environment</p> </li> <li>Application directories (<code>/opt/smart-smoker-{dev,prod}</code>)</li> <li>MongoDB data directories</li> <li> <p>User/group setup</p> </li> <li> <p>virtual-device - Virtual smoker device</p> </li> <li>Device directories</li> <li>Python tools for simulation</li> <li>Hardware mocking tools</li> </ol>"},{"location":"Infrastructure/features/configuration/ansible/#inventory-structure","title":"Inventory Structure","text":"<p>Servers are organized into logical groups:</p> <ul> <li>runners: GitHub Actions self-hosted runners</li> <li>cloud_servers: Dev and production cloud servers</li> <li>devices: Virtual smoker device for testing</li> </ul> <p>See <code>infra/proxmox/ansible/inventory/hosts.yml</code> for current inventory.</p>"},{"location":"Infrastructure/features/configuration/ansible/#running-ansible-playbooks","title":"Running Ansible Playbooks","text":""},{"location":"Infrastructure/features/configuration/ansible/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Ansible\npip3 install ansible\n\n# Install required collections\nansible-galaxy collection install community.general\nansible-galaxy collection install ansible.posix\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#available-playbooks","title":"Available Playbooks","text":""},{"location":"Infrastructure/features/configuration/ansible/#master-playbook-configure-everything","title":"Master Playbook (Configure Everything)","text":"<pre><code>cd infra/proxmox/ansible\n\n# Configure all infrastructure\nansible-playbook playbooks/site.yml --extra-vars \"github_runner_token=YOUR_TOKEN\"\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#individual-server-playbooks","title":"Individual Server Playbooks","text":"<pre><code># GitHub runner only\nansible-playbook playbooks/setup-github-runner.yml \\\n  --extra-vars \"github_runner_token=YOUR_TOKEN\"\n\n# Development cloud server\nansible-playbook playbooks/setup-dev-cloud.yml\n\n# Production cloud server\nansible-playbook playbooks/setup-prod-cloud.yml\n\n# Virtual smoker device\nansible-playbook playbooks/setup-virtual-smoker.yml\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#verification-playbook","title":"Verification Playbook","text":"<pre><code># Verify all infrastructure is correctly configured\nansible-playbook playbooks/verify-all.yml\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#testing-connectivity","title":"Testing Connectivity","text":"<pre><code># Test SSH connectivity to all servers\nansible all -m ping\n\n# Test connectivity to specific group\nansible cloud_servers -m ping\nansible runners -m ping\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#common-operations","title":"Common Operations","text":""},{"location":"Infrastructure/features/configuration/ansible/#update-system-packages","title":"Update System Packages","text":"<pre><code># Update all servers\nansible all -m apt -a \"update_cache=yes upgrade=dist\" --become\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#restart-docker-service","title":"Restart Docker Service","text":"<pre><code># Restart Docker on all servers\nansible all -m systemd -a \"name=docker state=restarted\" --become\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#check-service-status","title":"Check Service Status","text":"<pre><code># Check Docker status on all servers\nansible all -m systemd -a \"name=docker\" --become\n\n# Check GitHub runner status\nansible runners -m systemd -a \"name=actions.runner.*\" --become\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#run-ad-hoc-commands","title":"Run Ad-hoc Commands","text":"<pre><code># Check disk space\nansible all -m shell -a \"df -h /\"\n\n# Check memory usage\nansible all -m shell -a \"free -h\"\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#github-runner-management","title":"GitHub Runner Management","text":""},{"location":"Infrastructure/features/configuration/ansible/#registering-a-runner-automatic","title":"Registering a Runner (Automatic)","text":"<p>Runner registration is now fully automated via a GitHub PAT (Personal Access Token). The Ansible role auto-generates short-lived registration tokens from the PAT -- no manual token generation is needed.</p> <p>In CI (recommended): The <code>ansible-provision.yml</code> workflow automatically passes the <code>RUNNER_PAT</code> GitHub Secret to the role.</p> <p>Local runs: Pass the PAT via <code>--extra-vars</code>:</p> <pre><code>ansible-playbook playbooks/setup-github-runner.yml \\\n  --extra-vars \"github_runner_pat=github_pat_YOUR_TOKEN\"\n</code></pre> <p>Fallback (manual token): You can still pass a manually generated token if needed:</p> <pre><code>ansible-playbook playbooks/setup-github-runner.yml \\\n  --extra-vars \"github_runner_token=YOUR_SHORT_LIVED_TOKEN\"\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#runner-self-healing","title":"Runner Self-Healing","text":"<p>The runner has a self-healing systemd timer (<code>runner-health-check.timer</code>) that runs every 5 minutes and automatically detects and fixes stale registrations without requiring Ansible or GitHub Actions.</p> <p>What it checks:</p> <ul> <li>Runner <code>.runner</code> config file exists</li> <li>Runner systemd service is active</li> <li>No error loops in recent service logs (&gt;3 errors in 5 min = unhealthy)</li> </ul> <p>What it does when unhealthy:</p> <ul> <li>Checks DNS resolution for <code>api.github.com</code> (falls back to <code>8.8.8.8</code> if needed)</li> <li>Auto-generates a registration token from a stored PAT (<code>/etc/github-runner/pat</code>)</li> <li>Stops and uninstalls the stale runner service</li> <li>Re-registers with <code>--replace --unattended</code></li> <li>Installs and starts the new service</li> </ul> <p>How to monitor:</p> <pre><code># Check timer status\nsystemctl status runner-health-check.timer\n\n# View recent health check logs\njournalctl -u runner-health-check --since \"1 hour ago\"\n\n# Manually trigger a health check\nsystemctl start runner-health-check.service\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#checking-runner-status","title":"Checking Runner Status","text":"<pre><code># Check runner service status\nssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'systemctl status actions.runner.* --no-pager'\n\n# Check runner logs\nssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'journalctl -u actions.runner.* -n 50'\n\n# Check self-healing timer status\nssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'systemctl status runner-health-check.timer'\n\n# Check runner status via GitHub API\ngh api repos/benjr70/Smart-Smoker-V2/actions/runners \\\n  --jq '.runners[] | select(.name==\"smart-smoker-runner-1\")'\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#removing-a-runner","title":"Removing a Runner","text":"<p>Note: The self-healing timer will automatically re-register the runner unless you also remove the PAT file at <code>/etc/github-runner/pat</code>.</p> <pre><code># Stop the runner service\nansible runners -m systemd -a \"name=actions.runner.* state=stopped\" --become\n\n# Remove runner from GitHub (via web UI or API)\ngh api -X DELETE repos/benjr70/Smart-Smoker-V2/actions/runners/RUNNER_ID\n\n# To prevent auto-re-registration, also remove the PAT file:\nssh -J root@192.168.1.151 root@10.20.0.10 'rm -f /etc/github-runner/pat'\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#security-best-practices","title":"Security Best Practices","text":""},{"location":"Infrastructure/features/configuration/ansible/#ssh-key-management","title":"SSH Key Management","text":"<p>Current Status: SSH public keys are configured in <code>inventory/group_vars/all.yml</code></p> <p>Recommendations: - Keep personal SSH keys out of the repository - Use environment variables or external files for team keys - Rotate SSH keys regularly</p>"},{"location":"Infrastructure/features/configuration/ansible/#secrets-management","title":"Secrets Management","text":"<ul> <li>Never commit sensitive values to the repository</li> <li>Use <code>--extra-vars</code> for sensitive data like GitHub tokens</li> <li>Consider using Ansible Vault for encrypted variables</li> </ul>"},{"location":"Infrastructure/features/configuration/ansible/#firewall-rules","title":"Firewall Rules","text":"<p>Default UFW configuration: - Default incoming: DENY - Default outgoing: ALLOW - Allowed ports: 22 (SSH), 80 (HTTP), 443 (HTTPS) - MongoDB port: Restricted to internal network only</p>"},{"location":"Infrastructure/features/configuration/ansible/#fail2ban-configuration","title":"fail2ban Configuration","text":"<ul> <li>Enabled on: All servers</li> <li>Protected services: SSH</li> <li>Default ban time: Based on Debian defaults</li> <li>Recommendation: Consider stricter settings for production</li> </ul>"},{"location":"Infrastructure/features/configuration/ansible/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Infrastructure/features/configuration/ansible/#ssh-connection-issues","title":"SSH Connection Issues","text":"<pre><code># Test SSH connectivity\nansible all -m ping -vvv\n\n# Manually test SSH\nssh -J root@192.168.1.151 root@10.20.0.10\n\n# Check SSH service status\nansible all -m systemd -a \"name=sshd\" --become\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#ansible-playbook-failures","title":"Ansible Playbook Failures","text":"<pre><code># Run playbook in check mode (dry run)\nansible-playbook playbooks/site.yml --check\n\n# Run with verbose output\nansible-playbook playbooks/site.yml -vvv\n\n# Run specific tasks with tags\nansible-playbook playbooks/site.yml --tags \"docker\"\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#github-runner-issues","title":"GitHub Runner Issues","text":"<pre><code># Check runner service status\nssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'systemctl restart actions.runner.* &amp;&amp; systemctl status actions.runner.*'\n\n# View runner logs\nssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'journalctl -u actions.runner.* -n 100'\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#docker-issues","title":"Docker Issues","text":"<pre><code># Restart Docker on all servers\nansible all -m systemd -a \"name=docker state=restarted\" --become\n\n# Check Docker status\nansible all -m shell -a \"docker ps\"\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"Infrastructure/features/configuration/ansible/#automated-ansible-validation","title":"Automated Ansible Validation","text":"<p>All Ansible code is validated in CI/CD via <code>.github/workflows/ansible-lint.yml</code>: - ansible-lint on all playbooks and roles - Syntax validation - Inventory verification</p>"},{"location":"Infrastructure/features/configuration/ansible/#future-automated-ansible-execution","title":"Future: Automated Ansible Execution","text":"<p>After bootstrap, Ansible can be executed automatically via GitHub Actions on infrastructure changes. This requires: 1. Dedicated automation SSH key 2. GitHub Actions workflow for Ansible execution 3. Secure secrets management in GitHub</p>"},{"location":"Infrastructure/features/configuration/ansible/#best-practices","title":"Best Practices","text":"<ol> <li>Idempotency: All playbooks are designed to be run multiple times safely</li> <li>Check Mode: Test changes with <code>--check</code> before applying</li> <li>Verification: Always run <code>verify-all.yml</code> after infrastructure changes</li> <li>Version Control: Commit all Ansible changes to git</li> <li>Documentation: Update this guide when adding new roles or playbooks</li> </ol>"},{"location":"Infrastructure/features/configuration/ansible/#directory-structure","title":"Directory Structure","text":"<pre><code>infra/proxmox/ansible/\n\u251c\u2500\u2500 ansible.cfg                    # Ansible configuration\n\u251c\u2500\u2500 inventory/\n\u2502   \u251c\u2500\u2500 hosts.yml                  # Server inventory\n\u2502   \u251c\u2500\u2500 group_vars/                # Group variables\n\u2502   \u2502   \u251c\u2500\u2500 all.yml                # Common variables\n\u2502   \u2502   \u251c\u2500\u2500 runners.yml            # Runner-specific vars\n\u2502   \u2502   \u251c\u2500\u2500 cloud_servers.yml      # Cloud server vars\n\u2502   \u2502   \u2514\u2500\u2500 devices.yml            # Device vars\n\u2502   \u2514\u2500\u2500 host_vars/                 # Host-specific variables\n\u251c\u2500\u2500 roles/                         # Ansible roles (7 total)\n\u2502   \u251c\u2500\u2500 common/\n\u2502   \u251c\u2500\u2500 docker/\n\u2502   \u251c\u2500\u2500 terraform/\n\u2502   \u251c\u2500\u2500 nodejs/\n\u2502   \u251c\u2500\u2500 github-runner/\n\u2502   \u251c\u2500\u2500 cloud-app/\n\u2502   \u2514\u2500\u2500 virtual-device/\n\u251c\u2500\u2500 playbooks/                     # Ansible playbooks\n\u2502   \u251c\u2500\u2500 site.yml                   # Master playbook\n\u2502   \u251c\u2500\u2500 setup-github-runner.yml\n\u2502   \u251c\u2500\u2500 setup-dev-cloud.yml\n\u2502   \u251c\u2500\u2500 setup-prod-cloud.yml\n\u2502   \u251c\u2500\u2500 setup-virtual-smoker.yml\n\u2502   \u2514\u2500\u2500 verify-all.yml             # Verification playbook\n\u2514\u2500\u2500 README.md                      # Quick reference\n</code></pre>"},{"location":"Infrastructure/features/configuration/ansible/#references","title":"References","text":"<ul> <li>Ansible Best Practices</li> <li>Infrastructure Testing Guide</li> <li>Terraform Configuration</li> <li>Secrets Management Guide</li> </ul>"},{"location":"Infrastructure/features/configuration/system-setup/","title":"System Setup","text":""},{"location":"Infrastructure/features/configuration/system-setup/#overview","title":"Overview","text":"<p>Base system configuration for all infrastructure containers, including SSH hardening, firewall configuration, and security measures.</p>"},{"location":"Infrastructure/features/configuration/system-setup/#common-role","title":"Common Role","text":"<p>The <code>common</code> Ansible role provides base system configuration for all containers.</p>"},{"location":"Infrastructure/features/configuration/system-setup/#features","title":"Features","text":"<ol> <li>SSH Hardening</li> <li>Key-only authentication (password auth disabled)</li> <li>Secure SSH configuration</li> <li> <p>Authorized keys management</p> </li> <li> <p>UFW Firewall</p> </li> <li>Default incoming: DENY</li> <li>Default outgoing: ALLOW</li> <li>Allowed ports: 22 (SSH), 80 (HTTP), 443 (HTTPS)</li> <li> <p>MongoDB port: Restricted to internal network only</p> </li> <li> <p>fail2ban</p> </li> <li>Brute force protection</li> <li>SSH protection</li> <li> <p>Configurable ban times</p> </li> <li> <p>Base Packages</p> </li> <li>Essential system packages</li> <li>Security updates</li> <li>System utilities</li> </ol>"},{"location":"Infrastructure/features/configuration/system-setup/#ssh-configuration","title":"SSH Configuration","text":""},{"location":"Infrastructure/features/configuration/system-setup/#key-only-authentication","title":"Key-Only Authentication","text":"<p>Configuration (<code>infra/proxmox/ansible/roles/common/templates/sshd_config.j2</code>):</p> <pre><code>PasswordAuthentication no\nPubkeyAuthentication yes\nAuthorizedKeysFile .ssh/authorized_keys\n</code></pre>"},{"location":"Infrastructure/features/configuration/system-setup/#ssh-key-management","title":"SSH Key Management","text":"<p>Current Status: SSH public keys configured in <code>inventory/group_vars/all.yml</code></p> <p>Recommendations: - Keep personal SSH keys out of repository - Use environment variables or external files for team keys - Rotate SSH keys regularly</p>"},{"location":"Infrastructure/features/configuration/system-setup/#firewall-configuration","title":"Firewall Configuration","text":""},{"location":"Infrastructure/features/configuration/system-setup/#ufw-rules","title":"UFW Rules","text":"<p>Default Rules: <pre><code># Default policies\nufw default deny incoming\nufw default allow outgoing\n\n# Allow SSH\nufw allow 22/tcp\n\n# Allow HTTP/HTTPS\nufw allow 80/tcp\nufw allow 443/tcp\n\n# MongoDB (internal only)\nufw allow from 10.20.0.0/24 to any port 27017\n</code></pre></p>"},{"location":"Infrastructure/features/configuration/system-setup/#firewall-management","title":"Firewall Management","text":"<pre><code># Check firewall status\nufw status\n\n# Enable firewall\nufw enable\n\n# Disable firewall (not recommended)\nufw disable\n\n# View firewall rules\nufw status verbose\n</code></pre>"},{"location":"Infrastructure/features/configuration/system-setup/#fail2ban-configuration","title":"fail2ban Configuration","text":""},{"location":"Infrastructure/features/configuration/system-setup/#protection","title":"Protection","text":"<p>Enabled on: All servers Protected services: SSH Default ban time: Based on Debian defaults</p>"},{"location":"Infrastructure/features/configuration/system-setup/#configuration","title":"Configuration","text":"<p>Template (<code>infra/proxmox/ansible/roles/common/templates/jail.local.j2</code>):</p> <pre><code>[sshd]\nenabled = true\nport = 22\nfilter = sshd\nlogpath = /var/log/auth.log\nmaxretry = 5\nbantime = 3600\n</code></pre>"},{"location":"Infrastructure/features/configuration/system-setup/#management","title":"Management","text":"<pre><code># Check fail2ban status\nsystemctl status fail2ban\n\n# View banned IPs\nfail2ban-client status sshd\n\n# Unban IP\nfail2ban-client set sshd unbanip &lt;IP_ADDRESS&gt;\n</code></pre>"},{"location":"Infrastructure/features/configuration/system-setup/#base-packages","title":"Base Packages","text":""},{"location":"Infrastructure/features/configuration/system-setup/#installed-packages","title":"Installed Packages","text":"<ul> <li><code>curl</code> - HTTP client</li> <li><code>wget</code> - File downloader</li> <li><code>git</code> - Version control</li> <li><code>vim</code> - Text editor</li> <li><code>htop</code> - Process monitor</li> <li><code>jq</code> - JSON processor</li> <li><code>unzip</code> - Archive utility</li> </ul>"},{"location":"Infrastructure/features/configuration/system-setup/#package-updates","title":"Package Updates","text":"<pre><code># Update package lists\napt update\n\n# Upgrade packages\napt upgrade -y\n\n# Or via Ansible\nansible all -m apt -a \"update_cache=yes upgrade=dist\" --become\n</code></pre>"},{"location":"Infrastructure/features/configuration/system-setup/#security-best-practices","title":"Security Best Practices","text":""},{"location":"Infrastructure/features/configuration/system-setup/#ssh-security","title":"SSH Security","text":"<ol> <li>Use Strong Keys: 4096-bit RSA or Ed25519 keys</li> <li>Disable Root Login: Use sudo instead</li> <li>Limit Access: Restrict SSH to specific IPs if possible</li> <li>Regular Rotation: Rotate SSH keys periodically</li> </ol>"},{"location":"Infrastructure/features/configuration/system-setup/#firewall-security","title":"Firewall Security","text":"<ol> <li>Minimal Ports: Only open necessary ports</li> <li>Internal Restrictions: Restrict sensitive ports to internal network</li> <li>Regular Review: Review firewall rules periodically</li> </ol>"},{"location":"Infrastructure/features/configuration/system-setup/#system-security","title":"System Security","text":"<ol> <li>Regular Updates: Keep system packages updated</li> <li>Monitor Logs: Review system logs regularly</li> <li>Fail2ban: Monitor and adjust fail2ban settings</li> <li>Audit: Regular security audits</li> </ol>"},{"location":"Infrastructure/features/configuration/system-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Infrastructure/features/configuration/system-setup/#ssh-connection-issues","title":"SSH Connection Issues","text":"<p>Symptoms: Cannot connect via SSH</p> <p>Solution: <pre><code># Check SSH service\nsystemctl status sshd\n\n# Check firewall\nufw status\n\n# Check SSH configuration\ncat /etc/ssh/sshd_config | grep -E \"PasswordAuthentication|PubkeyAuthentication\"\n\n# Test SSH connection\nssh -v user@host\n</code></pre></p>"},{"location":"Infrastructure/features/configuration/system-setup/#firewall-blocking-services","title":"Firewall Blocking Services","text":"<p>Symptoms: Services not accessible</p> <p>Solution: <pre><code># Check firewall rules\nufw status verbose\n\n# Check if service is listening\nnetstat -tulpn | grep &lt;PORT&gt;\n\n# Add firewall rule if needed\nufw allow &lt;PORT&gt;/tcp\n</code></pre></p>"},{"location":"Infrastructure/features/configuration/system-setup/#fail2ban-issues","title":"fail2ban Issues","text":"<p>Symptoms: Legitimate users blocked</p> <p>Solution: <pre><code># Check banned IPs\nfail2ban-client status sshd\n\n# Unban IP\nfail2ban-client set sshd unbanip &lt;IP_ADDRESS&gt;\n\n# Adjust fail2ban configuration\n# Edit /etc/fail2ban/jail.local\n</code></pre></p>"},{"location":"Infrastructure/features/configuration/system-setup/#related-documentation","title":"Related Documentation","text":"<ul> <li>Ansible Configuration - Ansible role management</li> <li>Authentication - Authentication details</li> <li>Terraform Configuration - Infrastructure setup</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/features/database/backups/","title":"Automated Backup System","text":""},{"location":"Infrastructure/features/database/backups/#overview","title":"Overview","text":"<p>The Smart Smoker V2 infrastructure uses an automated backup system for MongoDB with retention policies and validation. Backups are managed via Ansible roles and run automatically via cron jobs.</p>"},{"location":"Infrastructure/features/database/backups/#backup-system-architecture","title":"Backup System Architecture","text":""},{"location":"Infrastructure/features/database/backups/#components","title":"Components","text":"<p>Ansible Backup Role (<code>infra/proxmox/ansible/roles/backups/</code>): <pre><code>infra/proxmox/ansible/roles/backups/\n\u251c\u2500\u2500 defaults/main.yml                    # Configuration (7d/4w/12m retention)\n\u251c\u2500\u2500 tasks/main.yml                       # Ansible tasks + cron jobs\n\u251c\u2500\u2500 handlers/main.yml                    # Service handlers\n\u2514\u2500\u2500 templates/\n    \u251c\u2500\u2500 backup-mongodb.sh.j2             # MongoDB dump script\n    \u251c\u2500\u2500 backup-retention.sh.j2           # Cleanup old backups\n    \u2514\u2500\u2500 backup-validation.sh.j2          # Verify backup integrity\n</code></pre></p> <p>Playbooks Updated: <pre><code>infra/proxmox/ansible/playbooks/\n\u251c\u2500\u2500 setup-dev-cloud.yml                  # Added backups role\n\u2514\u2500\u2500 setup-prod-cloud.yml                 # Added backups role\n</code></pre></p>"},{"location":"Infrastructure/features/database/backups/#backup-schedule","title":"Backup Schedule","text":""},{"location":"Infrastructure/features/database/backups/#daily-mongodb-backups","title":"Daily MongoDB Backups","text":"<p>Time: 2:00 AM daily Script: <code>backup-mongodb.sh</code></p> <pre><code>mongodump \\\n  --username=smartsmoker \\\n  --password=\"${MONGO_APP_PASSWORD}\" \\\n  --authenticationDatabase=admin \\\n  --db=smartsmoker \\\n  --out=/opt/smart-smoker-{{env}}/backups/mongodb/backup-${TIMESTAMP} \\\n  --gzip\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#retention-cleanup","title":"Retention Cleanup","text":"<p>Time: 2:30 AM daily Script: <code>backup-retention.sh</code></p> <p>Retention Policy: - Daily: Keep 7 days - Weekly: Keep 4 weeks (Sundays) - Monthly: Keep 12 months (1st of month)</p>"},{"location":"Infrastructure/features/database/backups/#backup-validation","title":"Backup Validation","text":"<p>Time: Sundays at 3:00 AM Script: <code>backup-validation.sh</code></p> <p>Validation Checks: - Backup directory exists - Contains BSON files - File sizes reasonable - Timestamps within expected range - Gzip integrity</p>"},{"location":"Infrastructure/features/database/backups/#cron-schedule","title":"Cron Schedule","text":"<pre><code>0 2 * * * /opt/smart-smoker-{{env}}/scripts/backup-mongodb.sh\n30 2 * * * /opt/smart-smoker-{{env}}/scripts/backup-retention.sh\n0 3 * * 0 /opt/smart-smoker-{{env}}/scripts/backup-validation.sh\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#configuration","title":"Configuration","text":""},{"location":"Infrastructure/features/database/backups/#retention-policy","title":"Retention Policy","text":"<p>Default Configuration (<code>infra/proxmox/ansible/roles/backups/defaults/main.yml</code>):</p> <pre><code>backups_retention_daily: 7\nbackups_retention_weekly: 4\nbackups_retention_monthly: 12\nbackups_mongodb_user: smartsmoker\nbackups_mongodb_database: smartsmoker\n</code></pre> <p>Customization: Override in inventory <code>host_vars</code> or <code>group_vars</code>:</p> <pre><code># For production, keep more backups\nbackups_retention_daily: 14\nbackups_retention_weekly: 8\nbackups_retention_monthly: 24\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#backup-locations","title":"Backup Locations","text":"<ul> <li>Dev: <code>/opt/smart-smoker-dev/backups/mongodb/</code></li> <li>Prod: <code>/opt/smart-smoker-prod/backups/mongodb/</code></li> <li>Symlink: <code>latest</code> points to most recent backup</li> </ul>"},{"location":"Infrastructure/features/database/backups/#backup-directory-structure","title":"Backup Directory Structure","text":"<pre><code>/opt/smart-smoker-{env}/backups/\n\u251c\u2500\u2500 mongodb/\n\u2502   \u251c\u2500\u2500 backup-20251207-140000/      # Daily backups\n\u2502   \u2502   \u251c\u2500\u2500 smartsmoker/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 sessions.bson.gz\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 users.bson.gz\n\u2502   \u2502   \u2514\u2500\u2500 backup.log\n\u2502   \u251c\u2500\u2500 latest -&gt; backup-20251207-140000/\n\u2502   \u2514\u2500\u2500 validation-report.txt\n\u2514\u2500\u2500 deployments/\n    \u251c\u2500\u2500 backup-20251207-150000/\n    \u2502   \u251c\u2500\u2500 manifest.txt\n    \u2502   \u251c\u2500\u2500 docker-images.tar.gz\n    \u2502   \u251c\u2500\u2500 cloud.docker-compose.yml.backup\n    \u2502   \u2514\u2500\u2500 mongodb-data.tar.gz\n    \u2514\u2500\u2500 last-deployment-backup.txt\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#deployment","title":"Deployment","text":""},{"location":"Infrastructure/features/database/backups/#deploy-backup-role","title":"Deploy Backup Role","text":"<p>To Dev-Cloud: <pre><code>cd infra/proxmox/ansible\nansible-playbook playbooks/setup-dev-cloud.yml --tags backups\n</code></pre></p> <p>To Prod-Cloud: <pre><code>cd infra/proxmox/ansible\nansible-playbook playbooks/setup-prod-cloud.yml \\\n  --tags backups \\\n  --extra-vars \"mongo_app_password=${PROD_MONGO_APP_PASSWORD}\"\n</code></pre></p>"},{"location":"Infrastructure/features/database/backups/#verify-installation","title":"Verify Installation","text":"<pre><code># Check cron jobs\nssh root@smart-smoker-cloud-prod \"crontab -l | grep backup\"\n\n# Check scripts installed\nssh root@smart-smoker-cloud-prod \"ls -la /opt/smart-smoker-prod/scripts/backup-*\"\n\n# Verify backup directories created\nssh root@smart-smoker-cloud-prod \"ls -la /opt/smart-smoker-prod/backups/\"\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#manual-operations","title":"Manual Operations","text":""},{"location":"Infrastructure/features/database/backups/#create-manual-backup","title":"Create Manual Backup","text":"<pre><code># On production server\n/opt/smart-smoker-prod/scripts/backup-mongodb.sh\n\n# Verify backup created\nls -lh /opt/smart-smoker-prod/backups/mongodb/\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#verify-backup","title":"Verify Backup","text":"<pre><code># Run validation script\n/opt/smart-smoker-prod/scripts/backup-validation.sh\n\n# Check validation report\ncat /opt/smart-smoker-prod/backups/mongodb/validation-report.txt\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#test-restore-dry-run","title":"Test Restore (Dry-Run)","text":"<pre><code># Test restore capability without affecting production\ndocker exec mongo mongorestore \\\n  --dry-run \\\n  --username smartsmoker \\\n  --password \"${MONGO_APP_PASSWORD}\" \\\n  --authenticationDatabase admin \\\n  --drop \\\n  --gzip \\\n  /data/db/backups/mongodb/latest\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#restore-from-backup","title":"Restore from Backup","text":"<pre><code># Stop services\ndocker compose -f cloud.docker-compose.yml down\n\n# Restore from backup\ndocker exec mongo mongorestore \\\n  --username smartsmoker \\\n  --password \"${MONGO_APP_PASSWORD}\" \\\n  --authenticationDatabase admin \\\n  --drop \\\n  --gzip \\\n  /opt/smart-smoker-prod/backups/mongodb/backup-YYYYMMDD-HHMMSS/smartsmoker\n\n# Restart services\ndocker compose -f cloud.docker-compose.yml up -d\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#monitoring","title":"Monitoring","text":""},{"location":"Infrastructure/features/database/backups/#check-backup-status","title":"Check Backup Status","text":"<pre><code># List recent backups\nls -lth /opt/smart-smoker-prod/backups/mongodb/ | head -10\n\n# Check backup logs\ntail -50 /var/log/mongodb-backup.log\n\n# Verify latest symlink\nls -lh /opt/smart-smoker-prod/backups/mongodb/latest\n\n# Check validation report\ncat /opt/smart-smoker-prod/backups/mongodb/validation-report.txt\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#verify-cron-jobs","title":"Verify Cron Jobs","text":"<pre><code># Check cron jobs are installed\ncrontab -l | grep backup\n\n# Check cron service status\nsystemctl status cron\n\n# View cron logs\ngrep CRON /var/log/syslog | grep backup\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#testing","title":"Testing","text":""},{"location":"Infrastructure/features/database/backups/#test-backup-creation","title":"Test Backup Creation","text":"<pre><code># Manually trigger backup\n/opt/smart-smoker-prod/scripts/backup-mongodb.sh\n\n# Verify backup created\nls -lh /opt/smart-smoker-prod/backups/mongodb/backup-*\n\n# Check backup contents\nls -lh /opt/smart-smoker-prod/backups/mongodb/backup-*/smartsmoker/\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#test-retention-cleanup","title":"Test Retention Cleanup","text":"<pre><code># Create test backups to verify retention works\nfor i in {1..10}; do\n  /opt/smart-smoker-prod/scripts/backup-mongodb.sh\n  sleep 5\ndone\n\n# Run retention cleanup\n/opt/smart-smoker-prod/scripts/backup-retention.sh\n\n# Verify old backups removed according to policy\nls -lh /opt/smart-smoker-prod/backups/mongodb/\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#test-validation","title":"Test Validation","text":"<pre><code># Run validation script\n/opt/smart-smoker-prod/scripts/backup-validation.sh\n\n# Check validation report\ncat /opt/smart-smoker-prod/backups/mongodb/validation-report.txt\n</code></pre>"},{"location":"Infrastructure/features/database/backups/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Infrastructure/features/database/backups/#backup-fails","title":"Backup Fails","text":"<p>Symptoms: Empty backup directory or error in logs</p> <p>Solution: <pre><code># Check backup logs\ntail -50 /var/log/mongodb-backup.log\n\n# Verify MongoDB authentication works\ndocker exec mongo mongosh -u smartsmoker -p \"${MONGO_APP_PASSWORD}\" \\\n  --authenticationDatabase admin smartsmoker\n\n# Check disk space\ndf -h /opt/smart-smoker-prod/backups\n\n# Test backup manually with verbose output\ndocker exec mongo mongodump \\\n  --username=smartsmoker \\\n  --password=\"${MONGO_APP_PASSWORD}\" \\\n  --authenticationDatabase=admin \\\n  --db=smartsmoker \\\n  --out=/data/db/backups/mongodb/manual-test \\\n  --gzip \\\n  --verbose\n</code></pre></p>"},{"location":"Infrastructure/features/database/backups/#cron-jobs-not-running","title":"Cron Jobs Not Running","text":"<p>Symptoms: Backups not created automatically</p> <p>Solution: <pre><code># Check cron service\nsystemctl status cron\n\n# Verify cron jobs installed\ncrontab -l\n\n# Check cron logs\ngrep CRON /var/log/syslog | grep backup\n\n# Reinstall cron jobs via Ansible\nansible-playbook playbooks/setup-prod-cloud.yml --tags backups\n</code></pre></p>"},{"location":"Infrastructure/features/database/backups/#validation-fails","title":"Validation Fails","text":"<p>Symptoms: Validation report shows errors</p> <p>Solution: <pre><code># Check validation report\ncat /opt/smart-smoker-prod/backups/mongodb/validation-report.txt\n\n# Verify backup files exist\nls -lh /opt/smart-smoker-prod/backups/mongodb/backup-*/smartsmoker/\n\n# Test gzip integrity\ngunzip -t /opt/smart-smoker-prod/backups/mongodb/backup-*/smartsmoker/*.gz\n\n# Re-run validation\n/opt/smart-smoker-prod/scripts/backup-validation.sh\n</code></pre></p>"},{"location":"Infrastructure/features/database/backups/#proxmox-lxc-snapshots","title":"Proxmox LXC Snapshots","text":""},{"location":"Infrastructure/features/database/backups/#configure-automated-snapshots","title":"Configure Automated Snapshots","text":"<p>On Proxmox Host:</p> <pre><code># Via Proxmox Web UI\n# 1. Navigate to Datacenter \u2192 Backup\n# 2. Click \"Add\"\n# 3. Configure:\n#    - Storage: local (or your backup storage)\n#    - Schedule: Daily at 01:00 (1 AM)\n#    - Selection: VMID 106 (prod-cloud)\n#    - Mode: Snapshot\n#    - Compression: ZSTD\n#    - Retention:\n#      - Keep Daily: 7\n#      - Keep Weekly: 4\n#      - Keep Monthly: 12\n</code></pre> <p>Manual Snapshot (for testing): <pre><code># On Proxmox host\nvzdump 106 \\\n  --mode snapshot \\\n  --storage local \\\n  --compress zstd \\\n  --notes-template \"Smart Smoker Production Cloud Backup\"\n\n# Verify snapshot created\nls -lh /var/lib/vz/dump/ | grep 106\n</code></pre></p>"},{"location":"Infrastructure/features/database/backups/#best-practices","title":"Best Practices","text":""},{"location":"Infrastructure/features/database/backups/#backup-strategy","title":"Backup Strategy","text":"<ol> <li>Multiple Backup Types: Use both MongoDB dumps and LXC snapshots</li> <li>Off-Site Storage: Copy critical backups to remote location</li> <li>Regular Testing: Test restore procedures quarterly</li> <li>Monitoring: Set up alerts for backup failures</li> <li>Documentation: Keep restore procedures documented and tested</li> </ol>"},{"location":"Infrastructure/features/database/backups/#retention-policy_1","title":"Retention Policy","text":"<ul> <li>Daily Backups: 7 days (covers week-long issues)</li> <li>Weekly Backups: 4 weeks (covers monthly issues)</li> <li>Monthly Backups: 12 months (covers long-term recovery needs)</li> </ul>"},{"location":"Infrastructure/features/database/backups/#security","title":"Security","text":"<ul> <li>Encryption: Consider encrypting backups at rest</li> <li>Access Control: Limit backup access to authorized personnel</li> <li>Audit Logging: Log all backup and restore operations</li> <li>Password Protection: Secure backup storage with strong passwords</li> </ul>"},{"location":"Infrastructure/features/database/backups/#related-documentation","title":"Related Documentation","text":"<ul> <li>MongoDB Configuration - MongoDB setup and authentication</li> <li>Disaster Recovery - Recovery procedures</li> <li>Ansible Configuration - Ansible role management</li> <li>Deployment Rollback - Deployment backup procedures</li> </ul> <p>Last Updated: 2025-12-07 Retention Policy: 7d/4w/12m</p>"},{"location":"Infrastructure/features/deployment/automation/","title":"Deployment Automation","text":""},{"location":"Infrastructure/features/deployment/automation/#overview","title":"Overview","text":"<p>Deployment automation uses GitHub Actions with self-hosted runners to automate CI/CD pipelines for cloud environments and Raspberry Pi devices.</p>"},{"location":"Infrastructure/features/deployment/automation/#deployment-pipeline","title":"Deployment Pipeline","text":""},{"location":"Infrastructure/features/deployment/automation/#pipeline-flow","title":"Pipeline Flow","text":"<pre><code>GitHub Repository\n\u251c\u2500\u2500 Feature Branch Push\n\u2502   \u251c\u2500\u2500 Lint &amp; Test (GitHub Hosted)\n\u2502   \u251c\u2500\u2500 Build Docker Images (GitHub Hosted)\n\u2502   \u2514\u2500\u2500 Push to Development Registry\n\u2502\n\u251c\u2500\u2500 Master Branch Merge\n\u2502   \u251c\u2500\u2500 Run Full Test Suite (Self-hosted Runner)\n\u2502   \u251c\u2500\u2500 Build Production Images (Self-hosted Runner)\n\u2502   \u251c\u2500\u2500 Deploy to Dev Cloud (Proxmox LXC)\n\u2502   \u251c\u2500\u2500 Integration Tests (Virtual Smoker VM)\n\u2502   \u2514\u2500\u2500 Tag Release Candidate\n\u2502\n\u251c\u2500\u2500 Production Release\n\u2502   \u251c\u2500\u2500 Manual Approval Required\n\u2502   \u251c\u2500\u2500 Deploy to Production Cloud (Proxmox LXC)\n\u2502   \u251c\u2500\u2500 Update Raspberry Pi Devices\n\u2502   \u251c\u2500\u2500 Verify Deployment Health\n\u2502   \u2514\u2500\u2500 Send Notifications\n\u2502\n\u2514\u2500\u2500 Rollback Process\n    \u251c\u2500\u2500 Detect Deployment Issues\n    \u251c\u2500\u2500 Automatic/Manual Rollback\n    \u251c\u2500\u2500 Restore Previous Version\n    \u2514\u2500\u2500 Alert Team\n</code></pre>"},{"location":"Infrastructure/features/deployment/automation/#github-actions-workflow","title":"GitHub Actions Workflow","text":""},{"location":"Infrastructure/features/deployment/automation/#cloud-deployment-workflow","title":"Cloud Deployment Workflow","text":"<p>File: <code>.github/workflows/cloud-deploy.yml</code></p> <p>Workflow Steps:</p> <ol> <li> <p>Pre-Deployment Backup <pre><code>- name: Backup current deployment\n  run: |\n    sudo mkdir -p /opt/smart-smoker/backups/deployments\n    sudo ./scripts/deployment-backup.sh\n</code></pre></p> </li> <li> <p>URL-Encode MongoDB Password <pre><code>- name: URL-encode MongoDB password\n  run: |\n    ENCODED_PASSWORD=$(printf %s \"${{ secrets.MONGO_APP_PASSWORD }}\" | jq -sRr @uri)\n    echo \"ENCODED_MONGO_APP_PASSWORD=$ENCODED_PASSWORD\" &gt;&gt; $GITHUB_ENV\n</code></pre></p> </li> <li> <p>Pull Docker Images <pre><code>- name: docker pull\n  env:\n    VERSION: ${{ inputs.version }}\n    MONGO_ROOT_USER: admin\n    MONGO_ROOT_PASSWORD: ${{ secrets.MONGO_ROOT_PASSWORD }}\n    MONGO_APP_PASSWORD: ${{ secrets.MONGO_APP_PASSWORD }}\n    ENCODED_MONGO_APP_PASSWORD: ${{ env.ENCODED_MONGO_APP_PASSWORD }}\n  run: sudo -E docker compose -f cloud.docker-compose.yml pull\n</code></pre></p> </li> <li> <p>Build Docker Images <pre><code>- name: docker build\n  run: sudo -E docker compose -f cloud.docker-compose.yml build\n</code></pre></p> </li> <li> <p>Stop Services <pre><code>- name: docker compose down\n  run: sudo -E docker compose -f cloud.docker-compose.yml down\n</code></pre></p> </li> <li> <p>Restart Tailscale <pre><code>- name: kill tailscale\n  run: sudo systemctl stop tailscaled\n</code></pre></p> </li> <li> <p>Start Services <pre><code>- name: docker compose up\n  run: sudo -E docker compose -f cloud.docker-compose.yml up -d --force-recreate\n</code></pre></p> </li> <li> <p>Restart Tailscale <pre><code>- name: start tailscale\n  run: sudo systemctl start tailscaled\n</code></pre></p> </li> <li> <p>Wait for Startup <pre><code>- name: Wait for startup\n  run: sleep 60\n</code></pre></p> </li> <li> <p>Health Verification <pre><code>- name: Verify deployment health\n  id: health_check\n  run: |\n    if ! ./scripts/deployment-health-check.sh localhost 3; then\n      echo \"Health check failed after 3 retries\"\n      exit 1\n    fi\n</code></pre></p> </li> <li> <p>Rollback on Failure <pre><code>- name: Rollback on failure\n  if: failure() &amp;&amp; steps.health_check.outcome == 'failure'\n  run: |\n    echo \"\ud83d\udea8 Deployment failed health check, initiating rollback...\"\n    sudo ./scripts/rollback.sh\n    sleep 30\n    if ./scripts/deployment-health-check.sh localhost 1; then\n      echo \"\u2705 Rollback successful, system restored\"\n    else\n      echo \"\ud83d\udca5 Rollback failed - MANUAL INTERVENTION REQUIRED\"\n      exit 1\n    fi\n</code></pre></p> </li> <li> <p>Cleanup <pre><code>- name: docker compose remove old containers\n  if: success()\n  run: sudo docker system prune -a --volumes --force\n</code></pre></p> </li> </ol>"},{"location":"Infrastructure/features/deployment/automation/#deployment-targets","title":"Deployment Targets","text":""},{"location":"Infrastructure/features/deployment/automation/#development-environment","title":"Development Environment","text":"<p>Location: dev-cloud (VMID 104) Trigger: Auto-deploy on master merge Access: Tailscale internal access Features: - Latest container images - Integration testing - Development database</p>"},{"location":"Infrastructure/features/deployment/automation/#production-environment","title":"Production Environment","text":"<p>Location: prod-cloud (VMID 106) or Raspberry Pi Trigger: Manual deployment approval Access: Tailscale funnel (public access) Features: - Tagged stable releases - Health monitoring - Automated backups</p>"},{"location":"Infrastructure/features/deployment/automation/#raspberry-pi-devices","title":"Raspberry Pi Devices","text":"<p>Trigger: Watchtower auto-updates or manual deployment Features: - Standardized container names - Health monitoring - Remote management via Tailscale</p>"},{"location":"Infrastructure/features/deployment/automation/#cicd-improvements","title":"CI/CD Improvements","text":""},{"location":"Infrastructure/features/deployment/automation/#standardized-docker-compose","title":"Standardized Docker Compose","text":"<p>Changed all <code>docker-compose</code> \u2192 <code>docker compose</code> (hyphen vs space) for Docker Compose v2 compatibility.</p>"},{"location":"Infrastructure/features/deployment/automation/#secrets-management","title":"Secrets Management","text":"<p>Before: Secrets in <code>run:</code> commands (risk of logging) After: Secrets in <code>env:</code> blocks (secure)</p> <pre><code># Secure secrets handling\n- name: docker compose up\n  env:\n    MONGO_APP_PASSWORD: ${{ secrets.MONGO_APP_PASSWORD }}\n    ENCODED_MONGO_APP_PASSWORD: ${{ env.ENCODED_MONGO_APP_PASSWORD }}\n  run: sudo -E docker compose -f cloud.docker-compose.yml up -d\n</code></pre>"},{"location":"Infrastructure/features/deployment/automation/#mongodb-password-url-encoding","title":"MongoDB Password URL Encoding","text":"<p>Base64 passwords contain special characters that must be URL-encoded for connection strings:</p> <pre><code># URL-encode MongoDB password\n- name: URL-encode MongoDB password\n  run: |\n    ENCODED_PASSWORD=$(printf %s \"${{ secrets.MONGO_APP_PASSWORD }}\" | jq -sRr @uri)\n    echo \"ENCODED_MONGO_APP_PASSWORD=$ENCODED_PASSWORD\" &gt;&gt; $GITHUB_ENV\n</code></pre>"},{"location":"Infrastructure/features/deployment/automation/#self-hosted-runner","title":"Self-Hosted Runner","text":""},{"location":"Infrastructure/features/deployment/automation/#runner-configuration","title":"Runner Configuration","text":"<p>Location: GitHub runner (VMID 105) Resources: 2 CPU cores, 4GB RAM, 50GB storage Features: - GitHub Actions runner service - Terraform with Proxmox provider - Docker CLI for deployments - Tailscale client - Node.js/npm for builds</p>"},{"location":"Infrastructure/features/deployment/automation/#runner-setup","title":"Runner Setup","text":"<p>See Ansible Configuration for runner setup procedures.</p>"},{"location":"Infrastructure/features/deployment/automation/#deployment-safety","title":"Deployment Safety","text":""},{"location":"Infrastructure/features/deployment/automation/#pre-deployment-backup","title":"Pre-Deployment Backup","text":"<p>Always creates backup before deployment changes.</p>"},{"location":"Infrastructure/features/deployment/automation/#health-verification","title":"Health Verification","text":"<p>Verifies deployment succeeded with retries.</p>"},{"location":"Infrastructure/features/deployment/automation/#automated-rollback","title":"Automated Rollback","text":"<p>Automatically rolls back on health check failure.</p> <p>See Rollback for details.</p>"},{"location":"Infrastructure/features/deployment/automation/#manual-deployment","title":"Manual Deployment","text":""},{"location":"Infrastructure/features/deployment/automation/#deploy-to-dev-cloud","title":"Deploy to Dev-Cloud","text":"<pre><code># Sync code to dev-cloud\n./scripts/sync-to-dev-cloud.sh\n\n# SSH to dev-cloud\nssh root@smoker-dev-cloud\n\n# Navigate to deployment directory\ncd /opt/smart-smoker-dev\n\n# Pull latest code\ngit fetch origin\ngit checkout master\ngit pull origin master\n\n# Deploy\ndocker compose -f cloud.docker-compose.yml pull\ndocker compose -f cloud.docker-compose.yml up -d --force-recreate\n\n# Verify\n./scripts/deployment-health-check.sh localhost 3\n</code></pre>"},{"location":"Infrastructure/features/deployment/automation/#deploy-to-production","title":"Deploy to Production","text":"<pre><code># On production server\ncd /opt/smart-smoker-prod\n\n# Pull latest code\ngit fetch origin\ngit checkout &lt;tag-or-branch&gt;\ngit pull origin &lt;tag-or-branch&gt;\n\n# Deploy (with backup)\n./scripts/deployment-backup.sh\ndocker compose -f cloud.docker-compose.yml pull\ndocker compose -f cloud.docker-compose.yml up -d --force-recreate\n\n# Verify\n./scripts/deployment-health-check.sh localhost 3\n</code></pre>"},{"location":"Infrastructure/features/deployment/automation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Infrastructure/features/deployment/automation/#deployment-fails","title":"Deployment Fails","text":"<p>Symptoms: Workflow fails during deployment</p> <p>Solution: <pre><code># Check workflow logs in GitHub Actions\n# Verify secrets are configured\n# Check runner connectivity\n# Review deployment logs on server\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/automation/#health-checks-fail","title":"Health Checks Fail","text":"<p>Symptoms: Deployment succeeds but health checks fail</p> <p>Solution: <pre><code># Check service logs\ndocker logs backend_cloud --tail 50\ndocker logs mongo --tail 50\n\n# Verify health endpoints\ncurl http://localhost:8443/api/health\n\n# Check container health\ndocker ps --filter health=unhealthy\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/automation/#rollback-issues","title":"Rollback Issues","text":"<p>Symptoms: Rollback fails or doesn't trigger</p> <p>Solution: <pre><code># Verify backup exists\nls -la /opt/smart-smoker/backups/deployments/\n\n# Check backup location file\ncat /opt/smart-smoker/backups/deployments/last-deployment-backup.txt\n\n# Manual rollback if needed\nsudo ./scripts/rollback.sh\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/automation/#related-documentation","title":"Related Documentation","text":"<ul> <li>Health Checks - Health verification procedures</li> <li>Rollback - Rollback procedures</li> <li>Environments - Environment configuration</li> <li>Secrets Management - GitHub Secrets setup</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/features/deployment/environments/","title":"Deployment Environments","text":""},{"location":"Infrastructure/features/deployment/environments/#overview","title":"Overview","text":"<p>The Smart Smoker V2 infrastructure supports multiple deployment environments: development, production, and virtual device testing.</p>"},{"location":"Infrastructure/features/deployment/environments/#environment-types","title":"Environment Types","text":""},{"location":"Infrastructure/features/deployment/environments/#development-environment","title":"Development Environment","text":"<p>Location: dev-cloud (VMID 104) Hostname: smoker-dev-cloud IP Address: 10.20.0.20/24 Resources: 2 CPU cores, 4GB RAM, 20GB storage</p> <p>Configuration: - Ubuntu 22.04 LTS - Docker Engine - Docker Compose - Git for deployments - Tailscale client - MongoDB 7.0 with authentication - Automated backups</p> <p>Access: - SSH: <code>ssh root@smoker-dev-cloud</code> (via Tailscale) - Internal Tailscale: <code>smoker-dev-cloud.tail74646.ts.net</code> - Backend API: <code>http://localhost:8443</code> (internal) - Frontend: <code>http://localhost:80</code> (internal)</p> <p>Deployment: - Auto-deploy on master merge (planned) - Manual deployment via scripts - Latest container images - Development database</p> <p>Features: - Integration testing - Development data - Faster iteration cycles - Less strict monitoring</p>"},{"location":"Infrastructure/features/deployment/environments/#production-environment","title":"Production Environment","text":"<p>Location: prod-cloud (VMID 106) or Raspberry Pi Hostname: smart-smoker-cloud-prod or smokecloud-1 IP Address: 10.20.0.30/24 (prod-cloud) Resources: 4 CPU cores, 8GB RAM, 40GB storage (prod-cloud)</p> <p>Configuration: - Ubuntu 22.04 LTS - Docker Engine - Docker Compose - Git for deployments - Tailscale client with funnel - MongoDB 7.0 with authentication - Automated backups - LXC snapshots</p> <p>Access: - SSH: <code>ssh root@smart-smoker-cloud-prod</code> (via Tailscale) - Public: <code>https://smokecloud.tail74646.ts.net</code> (Tailscale funnel) - Backend API: <code>https://smokecloud.tail74646.ts.net:8443</code> - Frontend: <code>https://smokecloud.tail74646.ts.net</code></p> <p>Deployment: - Manual deployment with approval - Tagged stable releases - Production database - Health monitoring - Automated rollback</p> <p>Features: - Production data - Strict monitoring - Automated backups - Disaster recovery - SSL certificates via Tailscale</p>"},{"location":"Infrastructure/features/deployment/environments/#virtual-device-testing","title":"Virtual Device Testing","text":"<p>Location: virtual-smoker-device (VM - ARM64) VMID: 9001 Resources: 2 CPU cores (ARM64), 2GB RAM, 32GB storage</p> <p>Configuration: - Raspberry Pi OS Lite 64-bit - VNC Server for GUI access - Mock hardware simulation services - Python serial communication simulators - Node.js device service environment - GPIO simulation libraries - Temperature sensor mock data generators - Tailscale client</p> <p>Access: - SSH: <code>ssh root@virtual-smoker-device</code> (via Tailscale) - VNC: Port 5900 (for GUI access)</p> <p>Purpose: - Integration test execution - Mock hardware validation - Performance testing - User acceptance testing</p>"},{"location":"Infrastructure/features/deployment/environments/#environment-differences","title":"Environment Differences","text":""},{"location":"Infrastructure/features/deployment/environments/#development-vs-production","title":"Development vs Production","text":"Feature Development Production Resources 2 CPU, 4GB RAM 4 CPU, 8GB RAM Storage 20GB 40GB Deployment Auto/manual Manual approval Monitoring Basic Comprehensive Backups Daily Daily + LXC snapshots Access Internal only Public via Tailscale Database Dev data Production data SSL Internal Tailscale funnel"},{"location":"Infrastructure/features/deployment/environments/#configuration-files","title":"Configuration Files","text":"<p>Development: <code>cloud.docker-compose.dev.yml</code> Production: <code>cloud.docker-compose.yml</code></p> <p>Key Differences: - Environment variables - Resource limits - Logging levels - Monitoring configuration</p>"},{"location":"Infrastructure/features/deployment/environments/#environment-setup","title":"Environment Setup","text":""},{"location":"Infrastructure/features/deployment/environments/#initial-setup","title":"Initial Setup","text":"<p>Development: <pre><code># Deploy via Ansible\ncd infra/proxmox/ansible\nansible-playbook playbooks/setup-dev-cloud.yml\n</code></pre></p> <p>Production: <pre><code># Deploy via Ansible\ncd infra/proxmox/ansible\nansible-playbook playbooks/setup-prod-cloud.yml\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/environments/#environment-variables","title":"Environment Variables","text":"<p>Development: <pre><code>MONGO_ROOT_USER=admin\nMONGO_ROOT_PASSWORD=&lt;dev-password&gt;\nMONGO_APP_PASSWORD=&lt;dev-app-password&gt;\nENCODED_MONGO_APP_PASSWORD=&lt;url-encoded&gt;\nNODE_ENV=development\n</code></pre></p> <p>Production: <pre><code>MONGO_ROOT_USER=admin\nMONGO_ROOT_PASSWORD=&lt;prod-password&gt;\nMONGO_APP_PASSWORD=&lt;prod-app-password&gt;\nENCODED_MONGO_APP_PASSWORD=&lt;url-encoded&gt;\nNODE_ENV=production\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/environments/#environment-management","title":"Environment Management","text":""},{"location":"Infrastructure/features/deployment/environments/#switching-environments","title":"Switching Environments","text":"<pre><code># Development\ncd /opt/smart-smoker-dev\ndocker compose -f cloud.docker-compose.yml &lt;command&gt;\n\n# Production\ncd /opt/smart-smoker-prod\ndocker compose -f cloud.docker-compose.yml &lt;command&gt;\n</code></pre>"},{"location":"Infrastructure/features/deployment/environments/#environment-specific-scripts","title":"Environment-Specific Scripts","text":"<pre><code># Development scripts\n/opt/smart-smoker-dev/scripts/backup-mongodb.sh\n/opt/smart-smoker-dev/scripts/deployment-health-check.sh\n\n# Production scripts\n/opt/smart-smoker-prod/scripts/backup-mongodb.sh\n/opt/smart-smoker-prod/scripts/deployment-health-check.sh\n</code></pre>"},{"location":"Infrastructure/features/deployment/environments/#network-configuration","title":"Network Configuration","text":""},{"location":"Infrastructure/features/deployment/environments/#development-network","title":"Development Network","text":"<ul> <li>Internal Access: Tailscale mesh network</li> <li>No Public Access: Internal only</li> <li>Ports: Standard Docker ports (80, 8443, 27017)</li> </ul>"},{"location":"Infrastructure/features/deployment/environments/#production-network","title":"Production Network","text":"<ul> <li>Public Access: Tailscale funnel</li> <li>SSL: Automatic via Tailscale</li> <li>Ports: Standard Docker ports exposed via funnel</li> </ul> <p>See Tailscale Configuration for details.</p>"},{"location":"Infrastructure/features/deployment/environments/#monitoring","title":"Monitoring","text":""},{"location":"Infrastructure/features/deployment/environments/#development-monitoring","title":"Development Monitoring","text":"<ul> <li>Basic health checks</li> <li>Container status</li> <li>Log aggregation</li> </ul>"},{"location":"Infrastructure/features/deployment/environments/#production-monitoring","title":"Production Monitoring","text":"<ul> <li>Comprehensive health checks</li> <li>Performance metrics</li> <li>Alert notifications</li> <li>Backup verification</li> <li>Disaster recovery testing</li> </ul> <p>See Monitoring for details.</p>"},{"location":"Infrastructure/features/deployment/environments/#best-practices","title":"Best Practices","text":""},{"location":"Infrastructure/features/deployment/environments/#development","title":"Development","text":"<ol> <li>Fast Iteration: Deploy frequently for testing</li> <li>Test Data: Use development data, not production</li> <li>Experimentation: Safe to test new features</li> <li>Documentation: Document any issues found</li> </ol>"},{"location":"Infrastructure/features/deployment/environments/#production","title":"Production","text":"<ol> <li>Stability: Only deploy tested, stable releases</li> <li>Backups: Always backup before deployment</li> <li>Monitoring: Monitor closely after deployment</li> <li>Rollback: Be ready to rollback if needed</li> <li>Documentation: Document all production changes</li> </ol>"},{"location":"Infrastructure/features/deployment/environments/#related-documentation","title":"Related Documentation","text":"<ul> <li>Deployment Automation - CI/CD workflows</li> <li>Health Checks - Health monitoring</li> <li>Rollback - Rollback procedures</li> <li>Terraform Configuration - Infrastructure setup</li> <li>Proxmox Configuration - Container setup</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/features/deployment/health-checks/","title":"Docker Health Checks","text":""},{"location":"Infrastructure/features/deployment/health-checks/#overview","title":"Overview","text":"<p>Docker health checks ensure all services are running correctly and dependencies are available before dependent services start. This prevents bad deployments and enables automatic recovery.</p>"},{"location":"Infrastructure/features/deployment/health-checks/#health-check-configuration","title":"Health Check Configuration","text":""},{"location":"Infrastructure/features/deployment/health-checks/#mongodb-health-check","title":"MongoDB Health Check","text":"<p>Configuration (<code>cloud.docker-compose.yml</code>):</p> <pre><code>mongo:\n  healthcheck:\n    test: [\"CMD\", \"mongosh\", \"--eval\", \"db.adminCommand('ping')\"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n    start_period: 60s\n</code></pre> <p>What It Does: Pings MongoDB to verify it's accepting connections.</p>"},{"location":"Infrastructure/features/deployment/health-checks/#backend-health-check","title":"Backend Health Check","text":"<p>Configuration (<code>cloud.docker-compose.yml</code>):</p> <pre><code>backend:\n  healthcheck:\n    test: [\"CMD\", \"node\", \"/apps/backend/healthcheck.js\"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n    start_period: 60s\n</code></pre> <p>Health Check Script (<code>apps/backend/healthcheck.js</code>):</p> <pre><code>const http = require('http');\n\nconst options = {\n  hostname: 'localhost',\n  port: 3001,\n  path: '/api/health',\n  method: 'GET',\n  timeout: 5000\n};\n\nconst req = http.request(options, (res) =&gt; {\n  if (res.statusCode === 200) {\n    process.exit(0);\n  } else {\n    console.error(`Health check failed with status code: ${res.statusCode}`);\n    process.exit(1);\n  }\n});\n\nreq.on('error', (err) =&gt; {\n  console.error(`Health check failed: ${err.message}`);\n  process.exit(1);\n});\n\nreq.on('timeout', () =&gt; {\n  console.error('Health check timeout');\n  req.destroy();\n  process.exit(1);\n});\n\nreq.end();\n</code></pre> <p>Health Endpoint (<code>apps/backend/src/health/health.controller.ts</code>):</p> <pre><code>@Controller('api/health')\nexport class HealthController {\n  constructor(@InjectConnection() private connection: Connection) {}\n\n  @Get()\n  async check() {\n    const dbStatus =\n      this.connection.readyState === 1 ? 'connected' : 'disconnected';\n\n    return {\n      status: 'ok',\n      timestamp: new Date().toISOString(),\n      database: {\n        status: dbStatus,\n        name: this.connection.name,\n      },\n      uptime: process.uptime(),\n      environment: process.env.NODE_ENV || 'production',\n    };\n  }\n}\n</code></pre>"},{"location":"Infrastructure/features/deployment/health-checks/#frontend-health-check","title":"Frontend Health Check","text":"<p>Configuration (<code>cloud.docker-compose.yml</code>):</p> <pre><code>frontend:\n  healthcheck:\n    test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000\"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n    start_period: 40s\n</code></pre> <p>What It Does: Verifies frontend HTTP server is responding.</p>"},{"location":"Infrastructure/features/deployment/health-checks/#cascading-dependencies","title":"Cascading Dependencies","text":"<p>Health checks enable cascading service dependencies:</p> <pre><code>backend:\n  depends_on:\n    mongo:\n      condition: service_healthy  # Backend waits for healthy MongoDB\n\nfrontend:\n  depends_on:\n    backend:\n      condition: service_healthy  # Frontend waits for healthy backend\n</code></pre> <p>Flow: 1. MongoDB starts and becomes healthy (60s start period) 2. Backend starts only after MongoDB is healthy 3. Backend becomes healthy (60s start period) 4. Frontend starts only after backend is healthy 5. Frontend becomes healthy (40s start period)</p>"},{"location":"Infrastructure/features/deployment/health-checks/#health-check-states","title":"Health Check States","text":"<ul> <li><code>starting</code> - Container starting, health checks haven't passed yet</li> <li><code>healthy</code> - All health checks passing</li> <li><code>unhealthy</code> - Health checks failing</li> </ul>"},{"location":"Infrastructure/features/deployment/health-checks/#benefits","title":"Benefits","text":"<ul> <li>Prevents Bad Deployments: Services won't start if dependencies aren't healthy</li> <li>Automatic Recovery: Docker can restart unhealthy containers</li> <li>Monitoring Integration: Health status visible via <code>docker ps</code></li> <li>Deployment Verification: CI/CD can verify health before declaring success</li> </ul>"},{"location":"Infrastructure/features/deployment/health-checks/#monitoring-health-status","title":"Monitoring Health Status","text":""},{"location":"Infrastructure/features/deployment/health-checks/#check-container-health","title":"Check Container Health","text":"<pre><code># View health status of all containers\ndocker ps --format \"table {{.Names}}\\t{{.Status}}\\t{{.Health}}\"\n\n# Filter healthy containers\ndocker ps --filter health=healthy\n\n# Filter unhealthy containers\ndocker ps --filter health=unhealthy\n\n# Check specific container health\ndocker inspect --format='{{.State.Health.Status}}' mongo\ndocker inspect --format='{{.State.Health.Status}}' backend_cloud\ndocker inspect --format='{{.State.Health.Status}}' frontend_cloud\n</code></pre>"},{"location":"Infrastructure/features/deployment/health-checks/#view-health-check-logs","title":"View Health Check Logs","text":"<pre><code># View health check history\ndocker inspect --format='{{json .State.Health}}' backend_cloud | jq\n\n# View recent health check attempts\ndocker inspect --format='{{range .State.Health.Log}}{{.Output}}{{end}}' backend_cloud\n</code></pre>"},{"location":"Infrastructure/features/deployment/health-checks/#watch-health-status","title":"Watch Health Status","text":"<pre><code># Monitor health status continuously\nwatch -n 5 'docker ps --format \"table {{.Names}}\\t{{.Status}}\\t{{.Health}}\"'\n</code></pre>"},{"location":"Infrastructure/features/deployment/health-checks/#testing-health-checks","title":"Testing Health Checks","text":""},{"location":"Infrastructure/features/deployment/health-checks/#test-health-endpoints","title":"Test Health Endpoints","text":"<pre><code># Test backend health endpoint\ncurl http://localhost:8443/api/health | jq\n\n# Expected response:\n# {\n#   \"status\": \"ok\",\n#   \"timestamp\": \"2025-12-07T14:30:00.000Z\",\n#   \"database\": {\n#     \"status\": \"connected\",\n#     \"name\": \"smartsmoker\"\n#   },\n#   \"uptime\": 3600,\n#   \"environment\": \"production\"\n# }\n\n# Test frontend\ncurl -I http://localhost:80\n# Expected: HTTP/1.1 200 OK\n</code></pre>"},{"location":"Infrastructure/features/deployment/health-checks/#verify-health-check-scripts","title":"Verify Health Check Scripts","text":"<pre><code># Test backend health check script\ndocker exec backend_cloud node /apps/backend/healthcheck.js\n# Exit code 0 = healthy, 1 = unhealthy\n\n# Test MongoDB health check\ndocker exec mongo mongosh --eval \"db.adminCommand('ping')\"\n# Should return: { ok: 1 }\n</code></pre>"},{"location":"Infrastructure/features/deployment/health-checks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Infrastructure/features/deployment/health-checks/#health-checks-failing","title":"Health Checks Failing","text":"<p>Symptoms: Containers marked as unhealthy</p> <p>Solution: <pre><code># Check health check logs\ndocker inspect backend_cloud --format='{{json .State.Health}}' | jq\n\n# Look at recent health check attempts\ndocker inspect backend_cloud --format='{{range .State.Health.Log}}{{.Output}}{{end}}'\n\n# Common issues:\n# 1. Service not fully started yet (wait 60-120s)\n# 2. Health endpoint not accessible (check port mapping)\n# 3. Dependencies not healthy (check mongo health first)\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/health-checks/#service-not-starting","title":"Service Not Starting","text":"<p>Symptoms: Container stays in \"starting\" state</p> <p>Solution: <pre><code># Check if dependency is healthy\ndocker ps --filter health=healthy\n\n# Check dependency logs\ndocker logs mongo --tail 50\n\n# Increase start_period if service needs more time\n# Edit cloud.docker-compose.yml and increase start_period value\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/health-checks/#health-endpoint-not-responding","title":"Health Endpoint Not Responding","text":"<p>Symptoms: Backend health check fails</p> <p>Solution: <pre><code># Check if backend is running\ndocker ps | grep backend\n\n# Check backend logs\ndocker logs backend_cloud --tail 50\n\n# Test health endpoint manually\ncurl http://localhost:8443/api/health\n\n# Check port mapping\ndocker port backend_cloud\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/health-checks/#performance-impact","title":"Performance Impact","text":""},{"location":"Infrastructure/features/deployment/health-checks/#startup-time","title":"Startup Time","text":"<p>Before (no health checks): - MongoDB ready: ~5 seconds - Backend ready: ~10 seconds - Total startup: ~10 seconds</p> <p>After (with health checks): - MongoDB ready: ~60 seconds (start_period) - Backend ready: ~60 seconds (waiting for healthy mongo) - Frontend ready: ~40 seconds (waiting for healthy backend) - Total startup: ~160 seconds</p> <p>Reason: Health check start periods allow services to initialize properly before being marked healthy.</p>"},{"location":"Infrastructure/features/deployment/health-checks/#runtime-performance","title":"Runtime Performance","text":"<ul> <li>Health Checks: Minimal CPU impact (~0.1% every 30s)</li> <li>Health Endpoints: Negligible impact on application performance</li> </ul>"},{"location":"Infrastructure/features/deployment/health-checks/#integration-with-cicd","title":"Integration with CI/CD","text":""},{"location":"Infrastructure/features/deployment/health-checks/#github-actions-health-verification","title":"GitHub Actions Health Verification","text":"<p>The deployment workflow includes health checks:</p> <pre><code># Wait for services to start (health checks have start_period of 60s)\n- name: Wait for startup\n  run: sleep 60\n\n# Health check with retry (3x per user preference)\n- name: Verify deployment health\n  id: health_check\n  run: |\n    if ! ./scripts/deployment-health-check.sh localhost 3; then\n      echo \"Health check failed after 3 retries\"\n      exit 1\n    fi\n</code></pre> <p>See Deployment Automation for full CI/CD integration.</p>"},{"location":"Infrastructure/features/deployment/health-checks/#related-documentation","title":"Related Documentation","text":"<ul> <li>Deployment Automation - CI/CD workflows with health checks</li> <li>Deployment Rollback - Rollback on health check failure</li> <li>MongoDB Configuration - MongoDB health check setup</li> <li>Monitoring - Health monitoring and alerts</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/features/deployment/rollback/","title":"Deployment Rollback","text":""},{"location":"Infrastructure/features/deployment/rollback/#overview","title":"Overview","text":"<p>The deployment rollback system provides automated recovery from failed deployments. It uses a three-layer safety net: pre-deployment backup, health verification, and automated rollback.</p>"},{"location":"Infrastructure/features/deployment/rollback/#safety-mechanisms","title":"Safety Mechanisms","text":""},{"location":"Infrastructure/features/deployment/rollback/#1-pre-deployment-backup","title":"1. Pre-Deployment Backup","text":"<p>Before any deployment, the current state is backed up:</p> <p>Script: <code>scripts/deployment-backup.sh</code></p> <p>What Gets Backed Up: - Docker images (docker save) - Docker Compose file - MongoDB data directory - Container logs - Environment files - Manifest with metadata</p> <p>Backup Location: <code>/opt/smart-smoker/backups/deployments/backup-YYYYMMDD-HHMMSS/</code></p>"},{"location":"Infrastructure/features/deployment/rollback/#2-health-verification","title":"2. Health Verification","text":"<p>After deployment, health checks verify the deployment succeeded:</p> <p>Script: <code>scripts/deployment-health-check.sh</code></p> <p>Verifies: 1. Backend <code>/api/health</code> endpoint responding 2. Frontend HTTP 200 response 3. All Docker containers healthy 4. Disk usage &lt; 90%</p> <p>Retries: 3 attempts with 10s delay Exit Codes: 0 = success, 1 = failure</p>"},{"location":"Infrastructure/features/deployment/rollback/#3-automated-rollback","title":"3. Automated Rollback","text":"<p>If health checks fail, automatic rollback restores the previous state:</p> <p>Script: <code>scripts/rollback.sh</code></p> <p>Restores: 1. Stop all containers 2. Restore Docker images (docker load) 3. Restore Docker Compose file 4. Restore MongoDB data 5. Restart services 6. Generate rollback report</p>"},{"location":"Infrastructure/features/deployment/rollback/#usage","title":"Usage","text":""},{"location":"Infrastructure/features/deployment/rollback/#manual-rollback","title":"Manual Rollback","text":"<pre><code># Execute rollback script\nsudo /opt/smart-smoker-prod/scripts/rollback.sh\n\n# Verify rollback success\nsleep 30\nsudo /opt/smart-smoker-prod/scripts/deployment-health-check.sh localhost 1\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#automated-rollback-github-actions","title":"Automated Rollback (GitHub Actions)","text":"<p>Rollback is automatically triggered on deployment failure:</p> <pre><code># Pre-deployment backup\n- name: Create pre-deployment backup\n  run: sudo /opt/smart-smoker-prod/scripts/deployment-backup.sh\n\n# Deployment\n- name: docker compose up\n  run: sudo -E docker compose -f cloud.docker-compose.yml up -d\n\n# Health verification (3 retries, 10s delay)\n- name: Health check\n  run: sudo /opt/smart-smoker-prod/scripts/deployment-health-check.sh smoker-cloud-prod 3\n\n# Automatic rollback on failure\n- name: Rollback on failure\n  if: failure() &amp;&amp; steps.health_check.outcome == 'failure'\n  run: |\n    echo \"\ud83d\udea8 Deployment failed health check, initiating rollback...\"\n    sudo /opt/smart-smoker-prod/scripts/rollback.sh\n\n    # Verify rollback success\n    sleep 30\n    if sudo /opt/smart-smoker-prod/scripts/deployment-health-check.sh localhost 1; then\n      echo \"\u2705 Rollback successful, system restored\"\n    else\n      echo \"\ud83d\udca5 Rollback failed - MANUAL INTERVENTION REQUIRED\"\n      exit 1\n    fi\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#rollback-process","title":"Rollback Process","text":""},{"location":"Infrastructure/features/deployment/rollback/#step-1-locate-backup","title":"Step 1: Locate Backup","text":"<p>The rollback script reads the backup location from: <pre><code>/opt/smart-smoker/backups/deployments/last-deployment-backup.txt\n</code></pre></p> <p>This file contains the path to the most recent deployment backup.</p>"},{"location":"Infrastructure/features/deployment/rollback/#step-2-verify-backup","title":"Step 2: Verify Backup","text":"<pre><code># Check backup directory exists\nls -la /opt/smart-smoker/backups/deployments/backup-YYYYMMDD-HHMMSS/\n\n# Verify backup contents\ncat /opt/smart-smoker/backups/deployments/backup-YYYYMMDD-HHMMSS/manifest.txt\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#step-3-stop-services","title":"Step 3: Stop Services","text":"<pre><code># Stop all containers (preserves volumes for data safety)\ndocker compose -f cloud.docker-compose.yml down\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#step-4-restore-images","title":"Step 4: Restore Images","text":"<pre><code># Restore Docker images from backup\ngunzip -c backup-YYYYMMDD-HHMMSS/docker-images.tar.gz | docker load\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#step-5-restore-configuration","title":"Step 5: Restore Configuration","text":"<pre><code># Restore Docker Compose file\ncp backup-YYYYMMDD-HHMMSS/cloud.docker-compose.yml.backup cloud.docker-compose.yml\n\n# Restore environment files if needed\ncp backup-YYYYMMDD-HHMMSS/.env.prod.backup .env.prod\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#step-6-restore-data","title":"Step 6: Restore Data","text":"<pre><code># Restore MongoDB data\ntar -xzf backup-YYYYMMDD-HHMMSS/mongodb-data.tar.gz\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#step-7-restart-services","title":"Step 7: Restart Services","text":"<pre><code># Start services with restored configuration\ndocker compose -f cloud.docker-compose.yml up -d\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#step-8-verify-rollback","title":"Step 8: Verify Rollback","text":"<pre><code># Run health checks\n./scripts/deployment-health-check.sh localhost 3\n\n# Check service logs\ndocker logs backend_cloud --tail 50\ndocker logs mongo --tail 50\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#backup-contents","title":"Backup Contents","text":""},{"location":"Infrastructure/features/deployment/rollback/#backup-directory-structure","title":"Backup Directory Structure","text":"<pre><code>/opt/smart-smoker/backups/deployments/backup-YYYYMMDD-HHMMSS/\n\u251c\u2500\u2500 manifest.txt                    # Backup metadata\n\u251c\u2500\u2500 docker-images.tar.gz            # Docker images backup\n\u251c\u2500\u2500 cloud.docker-compose.yml.backup # Docker Compose file\n\u251c\u2500\u2500 mongodb-data.tar.gz             # MongoDB data backup\n\u251c\u2500\u2500 image-info.txt                  # Image information\n\u251c\u2500\u2500 compose-state.txt               # Container state\n\u251c\u2500\u2500 running-containers.txt          # Running containers info\n\u251c\u2500\u2500 backend_cloud.log               # Backend logs\n\u251c\u2500\u2500 frontend_cloud.log              # Frontend logs\n\u2514\u2500\u2500 mongo.log                       # MongoDB logs\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#manifest-file","title":"Manifest File","text":"<p>The manifest contains backup metadata:</p> <pre><code>Backup Timestamp: 20251207-143000\nBackup Location: /opt/smart-smoker/backups/deployments/backup-20251207-143000\nDocker Images: benjr70/smart-smoker-backend:latest, benjr70/smart-smoker-frontend:latest\nMongoDB Data: ./database\nCompose File: cloud.docker-compose.yml\n</code></pre>"},{"location":"Infrastructure/features/deployment/rollback/#rollback-scenarios","title":"Rollback Scenarios","text":""},{"location":"Infrastructure/features/deployment/rollback/#scenario-1-failed-health-check","title":"Scenario 1: Failed Health Check","text":"<p>Trigger: Health checks fail after deployment</p> <p>Process: 1. Health check script exits with code 1 2. GitHub Actions workflow detects failure 3. Rollback script executes automatically 4. System restored to previous state 5. Rollback report generated</p>"},{"location":"Infrastructure/features/deployment/rollback/#scenario-2-manual-rollback","title":"Scenario 2: Manual Rollback","text":"<p>Trigger: Manual intervention needed</p> <p>Process: 1. Identify backup to restore 2. Run rollback script manually 3. Verify restoration 4. Investigate deployment issue</p>"},{"location":"Infrastructure/features/deployment/rollback/#scenario-3-partial-rollback","title":"Scenario 3: Partial Rollback","text":"<p>Trigger: Only specific services need rollback</p> <p>Process: 1. Stop specific service 2. Restore service-specific data/config 3. Restart service 4. Verify service health</p>"},{"location":"Infrastructure/features/deployment/rollback/#benefits","title":"Benefits","text":"<ul> <li>Zero Data Loss: Always backup before changes</li> <li>Fast Recovery: Automated rollback in &lt; 2 minutes</li> <li>Deployment Confidence: Health checks prevent bad deployments</li> <li>Audit Trail: Manifests and reports for every deployment</li> </ul>"},{"location":"Infrastructure/features/deployment/rollback/#limitations","title":"Limitations","text":""},{"location":"Infrastructure/features/deployment/rollback/#docker-image-backup","title":"Docker Image Backup","text":"<p>Issue: <code>deployment-backup.sh</code> may create incomplete backups when using locally-built images.</p> <p>Impact: Missing <code>docker-images.tar.gz</code> in backup archives.</p> <p>Workaround: Backup still includes MongoDB data and compose file (sufficient for most rollbacks).</p> <p>Status: Investigate in future improvements.</p>"},{"location":"Infrastructure/features/deployment/rollback/#rollback-time","title":"Rollback Time","text":"<p>Typical Rollback Duration: 1-2 minutes</p> <p>Factors: - Docker image size - MongoDB data size - Network speed (if pulling images)</p>"},{"location":"Infrastructure/features/deployment/rollback/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Infrastructure/features/deployment/rollback/#rollback-fails","title":"Rollback Fails","text":"<p>Symptoms: Rollback script exits with error</p> <p>Solution: <pre><code># Check backup directory exists\nls -la /opt/smart-smoker/backups/deployments/\n\n# Verify backup location file\ncat /opt/smart-smoker/backups/deployments/last-deployment-backup.txt\n\n# Check backup contents\nls -la /opt/smart-smoker/backups/deployments/backup-YYYYMMDD-HHMMSS/\n\n# Manually verify backup integrity\ntar -tzf backup-YYYYMMDD-HHMMSS/mongodb-data.tar.gz | head -10\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/rollback/#services-dont-start-after-rollback","title":"Services Don't Start After Rollback","text":"<p>Symptoms: Containers fail to start after rollback</p> <p>Solution: <pre><code># Check container logs\ndocker logs backend_cloud --tail 50\ndocker logs mongo --tail 50\n\n# Verify Docker Compose file\ndocker compose -f cloud.docker-compose.yml config\n\n# Check environment variables\ndocker compose -f cloud.docker-compose.yml config | grep -i env\n\n# Verify MongoDB data restored\nls -la ./database/\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/rollback/#missing-backup","title":"Missing Backup","text":"<p>Symptoms: No backup found for rollback</p> <p>Solution: <pre><code># Check if backup was created\nls -la /opt/smart-smoker/backups/deployments/\n\n# Check backup location file\ncat /opt/smart-smoker/backups/deployments/last-deployment-backup.txt\n\n# If no backup exists, manual recovery required\n# 1. Restore from MongoDB backup (see Backups documentation)\n# 2. Restore from Proxmox snapshot\n# 3. Rebuild from source code\n</code></pre></p>"},{"location":"Infrastructure/features/deployment/rollback/#best-practices","title":"Best Practices","text":""},{"location":"Infrastructure/features/deployment/rollback/#before-deployment","title":"Before Deployment","text":"<ol> <li>Verify Backup Location: Ensure backup directory exists and is writable</li> <li>Check Disk Space: Ensure sufficient space for backup</li> <li>Test Rollback: Periodically test rollback procedure</li> </ol>"},{"location":"Infrastructure/features/deployment/rollback/#during-deployment","title":"During Deployment","text":"<ol> <li>Monitor Health Checks: Watch health check results</li> <li>Review Logs: Check deployment logs for issues</li> <li>Be Ready: Have rollback plan ready if needed</li> </ol>"},{"location":"Infrastructure/features/deployment/rollback/#after-rollback","title":"After Rollback","text":"<ol> <li>Investigate Root Cause: Determine why deployment failed</li> <li>Fix Issues: Address problems before next deployment</li> <li>Update Documentation: Document any issues encountered</li> <li>Test Fixes: Verify fixes work before next deployment</li> </ol>"},{"location":"Infrastructure/features/deployment/rollback/#related-documentation","title":"Related Documentation","text":"<ul> <li>Deployment Automation - CI/CD workflows with rollback</li> <li>Health Checks - Health verification procedures</li> <li>Backups - Backup system details</li> <li>Disaster Recovery - Recovery procedures</li> </ul> <p>Last Updated: 2025-12-07 Rollback Time: &lt; 2 minutes</p>"},{"location":"Infrastructure/features/infrastructure/containers/","title":"Container Standardization","text":""},{"location":"Infrastructure/features/infrastructure/containers/#overview","title":"Overview","text":"<p>All Docker containers follow a standardized naming convention for Watchtower compatibility and version control. This document covers container naming, tagging strategy, and image management.</p>"},{"location":"Infrastructure/features/infrastructure/containers/#container-naming-convention","title":"Container Naming Convention","text":""},{"location":"Infrastructure/features/infrastructure/containers/#new-naming-standardized","title":"New Naming (Standardized)","text":"<pre><code>benjr70/smart-smoker-backend:latest\nbenjr70/smart-smoker-backend:v1.2.3\nbenjr70/smart-smoker-backend:nightly\n\nbenjr70/smart-smoker-frontend:latest\nbenjr70/smart-smoker-frontend:v1.2.3\nbenjr70/smart-smoker-frontend:nightly\n\nbenjr70/smart-smoker-device-service:latest\nbenjr70/smart-smoker-device-service:v1.2.3\nbenjr70/smart-smoker-device-service:nightly\n\nbenjr70/smart-smoker-smoker:latest\nbenjr70/smart-smoker-smoker:v1.2.3\nbenjr70/smart-smoker-smoker:nightly\n\nbenjr70/smart-smoker-electron-shell:latest\nbenjr70/smart-smoker-electron-shell:v1.2.3\nbenjr70/smart-smoker-electron-shell:nightly\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#benefits","title":"Benefits","text":"<ul> <li>\u2705 Watchtower can update <code>:latest</code> tags automatically</li> <li>\u2705 Separate repository per service for clarity</li> <li>\u2705 Semantic versioning with <code>v</code> prefix</li> <li>\u2705 Consistent hyphen-separated naming</li> <li>\u2705 Clear environment strategy</li> </ul>"},{"location":"Infrastructure/features/infrastructure/containers/#tagging-strategy","title":"Tagging Strategy","text":""},{"location":"Infrastructure/features/infrastructure/containers/#floating-tags","title":"Floating Tags","text":"<ul> <li><code>nightly</code>: Development builds (auto-deployed on master merge)</li> <li><code>latest</code>: Production releases (used by Watchtower on Raspberry Pi)</li> </ul>"},{"location":"Infrastructure/features/infrastructure/containers/#immutable-tags","title":"Immutable Tags","text":"<ul> <li><code>vX.Y.Z</code>: Semantic version tags (never re-used, for rollback)</li> <li><code>vX.Y</code>: Minor version tags (optional)</li> <li><code>vX</code>: Major version tags (optional)</li> </ul>"},{"location":"Infrastructure/features/infrastructure/containers/#promotion-flow","title":"Promotion Flow","text":"<ol> <li>Build once on master merge \u2192 push <code>:nightly</code></li> <li>When cutting a release, retag the same image digest to <code>:vX.Y.Z</code> and <code>:latest</code></li> <li>Verify <code>:vX.Y.Z</code> and <code>:latest</code> point to the same digest for that release</li> </ol> <p>Why This Matters: - Rollback: Switch to <code>:vX.Y.Z</code> instantly; floating tags move - Reproducibility: Exact artifacts for audits and bug reproduction - Operational Clarity: Dev uses <code>:nightly</code>, smoker uses <code>:latest</code>, cloud prod pins <code>:vX.Y.Z</code></p>"},{"location":"Infrastructure/features/infrastructure/containers/#watchtower-integration","title":"Watchtower Integration","text":""},{"location":"Infrastructure/features/infrastructure/containers/#raspberry-pi-auto-updates","title":"Raspberry Pi Auto-Updates","text":"<p>Watchtower on Raspberry Pi devices automatically updates containers using <code>:latest</code> tags:</p> <pre><code># docker-compose.yml on Raspberry Pi\nservices:\n  backend:\n    image: benjr70/smart-smoker-backend:latest\n    # Watchtower monitors this and updates automatically\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#watchtower-configuration","title":"Watchtower Configuration","text":"<pre><code>watchtower:\n  image: containrrr/watchtower\n  volumes:\n    - /var/run/docker.sock:/var/run/docker.sock\n  environment:\n    - WATCHTOWER_CLEANUP=true\n    - WATCHTOWER_POLL_INTERVAL=3600\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#environment-tag-usage","title":"Environment Tag Usage","text":""},{"location":"Infrastructure/features/infrastructure/containers/#development","title":"Development","text":"<p>Tag: <code>nightly</code> Deployment: Auto-deploy on master merge Location: dev-cloud</p> <pre><code>services:\n  backend:\n    image: benjr70/smart-smoker-backend:nightly\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#production-cloud","title":"Production (Cloud)","text":"<p>Tag: <code>vX.Y.Z</code> (pinned versions) Deployment: Manual deployment with approval Location: prod-cloud</p> <pre><code>services:\n  backend:\n    image: benjr70/smart-smoker-backend:v1.2.3\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#production-raspberry-pi","title":"Production (Raspberry Pi)","text":"<p>Tag: <code>latest</code> (Watchtower auto-updates) Deployment: Automatic via Watchtower Location: Raspberry Pi devices</p> <pre><code>services:\n  backend:\n    image: benjr70/smart-smoker-backend:latest\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#image-management","title":"Image Management","text":""},{"location":"Infrastructure/features/infrastructure/containers/#building-images","title":"Building Images","text":"<pre><code># Build all images\ndocker compose -f docker-compose.build.yml build\n\n# Build specific service\ndocker compose -f docker-compose.build.yml build backend\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#tagging-images","title":"Tagging Images","text":"<pre><code># Tag for nightly\ndocker tag benjr70/smart-smoker-backend:build benjr70/smart-smoker-backend:nightly\n\n# Tag for release\ndocker tag benjr70/smart-smoker-backend:build benjr70/smart-smoker-backend:v1.2.3\ndocker tag benjr70/smart-smoker-backend:build benjr70/smart-smoker-backend:latest\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#publishing-images","title":"Publishing Images","text":"<pre><code># Push nightly\ndocker push benjr70/smart-smoker-backend:nightly\n\n# Push release tags\ndocker push benjr70/smart-smoker-backend:v1.2.3\ndocker push benjr70/smart-smoker-backend:latest\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#version-management","title":"Version Management","text":""},{"location":"Infrastructure/features/infrastructure/containers/#semantic-versioning","title":"Semantic Versioning","text":"<p>Follow Semantic Versioning: - MAJOR: Incompatible API changes - MINOR: Backward-compatible functionality - PATCH: Backward-compatible bug fixes</p>"},{"location":"Infrastructure/features/infrastructure/containers/#version-tags","title":"Version Tags","text":"<pre><code># Create version tag\ngit tag -a v1.2.3 -m \"Release version 1.2.3\"\ngit push origin v1.2.3\n\n# GitHub Actions automatically:\n# 1. Builds images\n# 2. Tags with v1.2.3 and latest\n# 3. Pushes to registry\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#rollback-strategy","title":"Rollback Strategy","text":""},{"location":"Infrastructure/features/infrastructure/containers/#using-version-tags","title":"Using Version Tags","text":"<pre><code># Rollback to specific version\ndocker compose -f cloud.docker-compose.yml pull benjr70/smart-smoker-backend:v1.2.2\ndocker compose -f cloud.docker-compose.yml up -d backend\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/containers/#using-deployment-backup","title":"Using Deployment Backup","text":"<p>See Rollback for automated rollback procedures.</p>"},{"location":"Infrastructure/features/infrastructure/containers/#best-practices","title":"Best Practices","text":""},{"location":"Infrastructure/features/infrastructure/containers/#image-naming","title":"Image Naming","text":"<ol> <li>Use Hyphens: <code>smart-smoker-backend</code> not <code>smart_smoker_backend</code></li> <li>Be Descriptive: Clear service names</li> <li>Consistent: Same pattern across all services</li> </ol>"},{"location":"Infrastructure/features/infrastructure/containers/#tag-management","title":"Tag Management","text":"<ol> <li>Never Re-tag: Immutable tags (<code>vX.Y.Z</code>) should never change</li> <li>Promote, Don't Rebuild: Retag same digest for releases</li> <li>Document Versions: Keep changelog of versions</li> </ol>"},{"location":"Infrastructure/features/infrastructure/containers/#watchtower","title":"Watchtower","text":"<ol> <li>Use <code>:latest</code>: Only for Watchtower auto-updates</li> <li>Pin in Production: Use version tags for production cloud</li> <li>Test Updates: Verify Watchtower updates work correctly</li> </ol>"},{"location":"Infrastructure/features/infrastructure/containers/#related-documentation","title":"Related Documentation","text":"<ul> <li>Deployment Automation - CI/CD workflows</li> <li>Environments - Environment configuration</li> <li>Terraform Configuration - Infrastructure setup</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/","title":"Proxmox Infrastructure","text":""},{"location":"Infrastructure/features/infrastructure/proxmox/#overview","title":"Overview","text":"<p>The Smart Smoker V2 infrastructure runs on a local Proxmox server using LXC containers and VMs. This document covers Proxmox setup, container configuration, and infrastructure management.</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#infrastructure-layout","title":"Infrastructure Layout","text":"<pre><code>Proxmox Server\n\u251c\u2500\u2500 github-runner (LXC Container - VMID 105)\n\u2502   \u251c\u2500\u2500 Self-hosted GitHub Actions runner\n\u2502   \u251c\u2500\u2500 Terraform with Proxmox provider\n\u2502   \u251c\u2500\u2500 Docker CLI for deployment\n\u2502   \u251c\u2500\u2500 Tailscale client for network access\n\u2502   \u2514\u2500\u2500 Node.js/npm for builds\n\u2502\n\u251c\u2500\u2500 smart-smoker-dev-cloud (LXC Container - VMID 104)\n\u2502   \u251c\u2500\u2500 Auto-deployed on master merge\n\u2502   \u251c\u2500\u2500 Backend + Frontend + MongoDB\n\u2502   \u251c\u2500\u2500 Environment variables injection\n\u2502   \u251c\u2500\u2500 Health monitoring\n\u2502   \u2514\u2500\u2500 Internal Tailscale access\n\u2502\n\u251c\u2500\u2500 smart-smoker-cloud-prod (LXC Container - VMID 106)\n\u2502   \u251c\u2500\u2500 Manual deployment trigger\n\u2502   \u251c\u2500\u2500 Backend + Frontend + MongoDB\n\u2502   \u251c\u2500\u2500 Tailscale client with funnel configuration\n\u2502   \u251c\u2500\u2500 Production SSL certificates via Tailscale\n\u2502   \u251c\u2500\u2500 Public access: https://smokecloud.tail74646.ts.net\n\u2502   \u2514\u2500\u2500 Automated deployment workflow\n\u2502\n\u2514\u2500\u2500 virtual-smoker-device (VM - ARM64 - VMID 9001)\n    \u251c\u2500\u2500 Raspberry Pi OS with desktop\n    \u251c\u2500\u2500 VNC server for GUI access\n    \u251c\u2500\u2500 Mock hardware devices\n    \u251c\u2500\u2500 Device Service + Smoker UI + Electron Shell\n    \u2514\u2500\u2500 Complete smoker simulation environment\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/proxmox/#container-specifications","title":"Container Specifications","text":""},{"location":"Infrastructure/features/infrastructure/proxmox/#github-runner-vmid-105","title":"GitHub Runner (VMID 105)","text":"<p>Type: LXC Container Resources: 2 CPU cores, 4GB RAM, 50GB storage Network: vmbr0 (10.20.0.10/24) OS: Ubuntu 22.04 LTS</p> <p>Features: - Nesting enabled for Docker-in-Docker - GitHub Actions runner service - Terraform CLI - Docker CLI - Tailscale client - Node.js 20 LTS</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#development-cloud-vmid-104","title":"Development Cloud (VMID 104)","text":"<p>Type: LXC Container Resources: 2 CPU cores, 4GB RAM, 20GB storage Network: vmbr0 (10.20.0.20/24) OS: Ubuntu 22.04 LTS</p> <p>Features: - Docker Engine - Docker Compose - Git for deployments - Tailscale client - MongoDB 7.0 - Automated backups</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#production-cloud-vmid-106","title":"Production Cloud (VMID 106)","text":"<p>Type: LXC Container Resources: 4 CPU cores, 8GB RAM, 40GB storage Network: vmbr0 (10.20.0.30/24) OS: Ubuntu 22.04 LTS</p> <p>Features: - Docker Engine - Docker Compose - Git for deployments - Tailscale client with funnel - MongoDB 7.0 - Automated backups - LXC snapshots</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#virtual-smoker-device-vmid-9001","title":"Virtual Smoker Device (VMID 9001)","text":"<p>Type: VM (ARM64) Resources: 2 CPU cores (ARM64), 2GB RAM, 32GB storage Network: vmbr1 (10.30.0.40/24) OS: Raspberry Pi OS Lite 64-bit</p> <p>Features: - VNC Server for GUI access - Mock hardware simulation - Python serial communication simulators - Node.js device service environment - GPIO simulation libraries - Tailscale client</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#infrastructure-as-code","title":"Infrastructure as Code","text":""},{"location":"Infrastructure/features/infrastructure/proxmox/#terraform-management","title":"Terraform Management","text":"<p>All infrastructure is managed via Terraform. See Terraform Configuration for setup and usage.</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#ansible-configuration","title":"Ansible Configuration","text":"<p>All containers are configured via Ansible. See Ansible Configuration for setup and operations.</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#networking","title":"Networking","text":""},{"location":"Infrastructure/features/infrastructure/proxmox/#network-bridges","title":"Network Bridges","text":"<p>vmbr0 (Primary Network - 10.20.0.0/24): - GitHub Runner: 10.20.0.10/24 - Dev Cloud: 10.20.0.20/24 - Prod Cloud: 10.20.0.30/24</p> <p>vmbr1 (Isolated Network - 10.30.0.0/24): - Virtual Smoker Device: 10.30.0.40/24</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#tailscale-integration","title":"Tailscale Integration","text":"<p>All containers use Tailscale for secure networking: - Mesh networking between containers - Public access via Tailscale funnel (production) - Internal access for development</p> <p>See Tailscale Configuration for details.</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#container-features","title":"Container Features","text":""},{"location":"Infrastructure/features/infrastructure/proxmox/#common-features","title":"Common Features","text":"<ul> <li>Nesting: Enabled on all containers for Docker-in-Docker support</li> <li>Unprivileged: All containers run as unprivileged for security</li> <li>Auto-start: Containers start automatically on host boot</li> </ul>"},{"location":"Infrastructure/features/infrastructure/proxmox/#resource-pools","title":"Resource Pools","text":"<p>All infrastructure is organized in the <code>smart-smoker</code> resource pool for easier management.</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"Infrastructure/features/infrastructure/proxmox/#lxc-snapshots","title":"LXC Snapshots","text":"<p>Automated Snapshots (via Proxmox): - Schedule: Daily at 01:00 - Retention: 7 daily, 4 weekly, 12 monthly - Compression: ZSTD - Storage: Local or backup storage</p> <p>Manual Snapshot: <pre><code># On Proxmox host\nvzdump 106 \\\n  --mode snapshot \\\n  --storage local \\\n  --compress zstd \\\n  --notes-template \"Smart Smoker Production Cloud Backup\"\n</code></pre></p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#container-backups","title":"Container Backups","text":"<p>See Backups for MongoDB backup procedures.</p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#monitoring","title":"Monitoring","text":""},{"location":"Infrastructure/features/infrastructure/proxmox/#container-status","title":"Container Status","text":"<pre><code># Check container status\npct status 104  # Dev cloud\npct status 106  # Prod cloud\n\n# View container resources\npct config 104\npct config 106\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/proxmox/#resource-usage","title":"Resource Usage","text":"<pre><code># View resource usage\npct exec 104 -- df -h\npct exec 104 -- free -h\npct exec 104 -- top -bn1\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/proxmox/#maintenance","title":"Maintenance","text":""},{"location":"Infrastructure/features/infrastructure/proxmox/#container-updates","title":"Container Updates","text":"<pre><code># Update container packages\npct exec 104 -- apt update &amp;&amp; apt upgrade -y\n\n# Or use Ansible\nansible-playbook playbooks/setup-dev-cloud.yml --tags common\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/proxmox/#container-restart","title":"Container Restart","text":"<pre><code># Restart container\npct shutdown 104\npct start 104\n\n# Or via Proxmox Web UI\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/proxmox/#container-rebuild","title":"Container Rebuild","text":"<p>If a container needs to be rebuilt:</p> <pre><code># Backup data first\nvzdump 104 --mode snapshot\n\n# Destroy and recreate via Terraform\ncd infra/proxmox/terraform\nterraform destroy -target=module.dev_cloud[0]\nterraform apply -target=module.dev_cloud[0]\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/proxmox/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Infrastructure/features/infrastructure/proxmox/#container-wont-start","title":"Container Won't Start","text":"<p>Symptoms: Container fails to start</p> <p>Solution: <pre><code># Check container status\npct status 104\n\n# View container logs\npct config 104\n\n# Check Proxmox logs\njournalctl -u pve-container@104\n</code></pre></p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#network-issues","title":"Network Issues","text":"<p>Symptoms: Container cannot reach network</p> <p>Solution: <pre><code># Check network configuration\npct config 104 | grep net\n\n# Test network connectivity\npct exec 104 -- ping -c 3 8.8.8.8\n\n# Check DNS\npct exec 104 -- nslookup google.com\n</code></pre></p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#resource-exhaustion","title":"Resource Exhaustion","text":"<p>Symptoms: Container runs out of resources</p> <p>Solution: <pre><code># Check resource usage\npct exec 104 -- df -h\npct exec 104 -- free -h\n\n# Increase resources via Terraform\n# Edit terraform.tfvars and apply\n</code></pre></p>"},{"location":"Infrastructure/features/infrastructure/proxmox/#related-documentation","title":"Related Documentation","text":"<ul> <li>Terraform Configuration - Infrastructure provisioning</li> <li>Ansible Configuration - Container configuration</li> <li>Environments - Environment setup</li> <li>Disaster Recovery - Recovery procedures</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/features/infrastructure/terraform/","title":"Terraform Infrastructure Setup Guide","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#overview","title":"Overview","text":"<p>This guide covers the Terraform infrastructure setup for the Smart Smoker V2 project, implementing Infrastructure as Code (IaC) for managing Proxmox resources.</p>"},{"location":"Infrastructure/features/infrastructure/terraform/#prerequisites","title":"Prerequisites","text":"<ul> <li>Proxmox VE Server: Version 7.x or 8.x with API access</li> <li>Terraform: &gt;= 1.5.0</li> <li>Access: Proxmox API credentials with appropriate permissions</li> <li>Network: Connectivity to Proxmox API endpoint</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#directory-structure","title":"Directory Structure","text":"<pre><code>infra/proxmox/\n\u251c\u2500\u2500 README.md                          # Overview and usage instructions\n\u251c\u2500\u2500 terraform/\n\u2502   \u251c\u2500\u2500 main.tf                        # Root module composition\n\u2502   \u251c\u2500\u2500 variables.tf                   # Root variable definitions\n\u2502   \u251c\u2500\u2500 outputs.tf                     # Infrastructure outputs\n\u2502   \u251c\u2500\u2500 terraform.tfvars               # Environment-specific values (gitignored)\n\u2502   \u251c\u2500\u2500 terraform.tfvars.example       # Template for configuration\n\u2502   \u251c\u2500\u2500 modules/\n\u2502   \u2502   \u251c\u2500\u2500 lxc-container/            # Reusable LXC container module\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u2502   \u251c\u2500\u2500 arm64-vm/                 # VM module (supports x86_64 and ARM64)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u2502   \u2514\u2500\u2500 networking/               # Network bridge management\n\u2502   \u2502       \u251c\u2500\u2500 main.tf\n\u2502   \u2502       \u251c\u2500\u2500 variables.tf\n\u2502   \u2502       \u2514\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 environments/\n\u2502   \u2502   \u251c\u2500\u2500 github-runner/            # GitHub Actions runner environment\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u2502   \u251c\u2500\u2500 dev-cloud/                # Development cloud environment\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u2502   \u251c\u2500\u2500 prod-cloud/               # Production cloud environment\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u2502   \u2514\u2500\u2500 virtual-smoker/           # Virtual device testing environment\n\u2502   \u2502       \u251c\u2500\u2500 main.tf\n\u2502   \u2502       \u251c\u2500\u2500 variables.tf\n\u2502   \u2502       \u2514\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 shared/\n\u2502   \u2502   \u251c\u2500\u2500 providers.tf              # Proxmox provider configuration\n\u2502   \u2502   \u251c\u2500\u2500 versions.tf               # Terraform version constraints\n\u2502   \u2502   \u2514\u2500\u2500 backend.tf                # State backend configuration\n\u2502   \u2514\u2500\u2500 state/\n\u2502       \u2514\u2500\u2500 terraform.tfstate         # Local state file (gitignored)\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 create-cloud-init-template.sh # VM template creation script\n    \u251c\u2500\u2500 install-arm64-firmware.sh     # ARM64 firmware installation\n    \u2514\u2500\u2500 fix-repos-and-install-arm64.sh # Repository fix + ARM64 setup\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/terraform/#initial-setup","title":"Initial Setup","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#1-configure-proxmox-api-access","title":"1. Configure Proxmox API Access","text":"<p>Create an API token in Proxmox:</p> <pre><code># On Proxmox host\npveum user add terraform@pve\npveum passwd terraform@pve\n\n# Create role with required permissions\npveum role add TerraformRole -privs \"VM.Allocate VM.Clone VM.Config.CDROM VM.Config.CPU VM.Config.Cloudinit VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Monitor VM.Audit VM.PowerMgmt Datastore.AllocateSpace Datastore.Audit Pool.Allocate Sys.Audit Sys.Console Sys.Modify\"\n\n# Assign role to user\npveum aclmod / -user terraform@pve -role TerraformRole\n\n# Create API token\npveum user token add terraform@pve SmartSmoker --privsep=0\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/terraform/#2-create-terraform-configuration","title":"2. Create Terraform Configuration","text":"<pre><code>cd infra/proxmox/terraform\ncp terraform.tfvars.example terraform.tfvars\n</code></pre> <p>Edit <code>terraform.tfvars</code> with your environment-specific values:</p> <pre><code>proxmox = {\n  api_url          = \"https://your-proxmox-ip:8006/\"\n  api_token_id     = \"terraform@pve!SmartSmoker\"\n  api_token_secret = \"your-api-token-secret\"\n  username         = \"root@pam\"           # Optional: for password auth\n  password         = \"your-password\"       # Optional: for password auth\n  tls_insecure     = true                 # Set to false in production\n  default_storage  = \"local-lvm\"\n  default_bridge   = \"vmbr0\"\n  ssh_public_keys  = []                   # Optional: SSH keys for containers\n  dns_servers      = [\"10.0.0.1\", \"10.0.0.2\"]\n  search_domain    = \"smoker.local\"\n}\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/terraform/#3-initialize-terraform","title":"3. Initialize Terraform","text":"<pre><code>terraform init\n</code></pre> <p>This downloads the required providers and initializes the backend.</p>"},{"location":"Infrastructure/features/infrastructure/terraform/#4-validate-configuration","title":"4. Validate Configuration","text":"<pre><code>terraform validate\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/terraform/#5-plan-infrastructure","title":"5. Plan Infrastructure","text":"<pre><code>terraform plan -out=tfplan\n</code></pre> <p>Review the plan carefully to ensure it matches your expectations.</p>"},{"location":"Infrastructure/features/infrastructure/terraform/#6-apply-infrastructure","title":"6. Apply Infrastructure","text":"<pre><code>terraform apply tfplan\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/terraform/#deployed-infrastructure","title":"Deployed Infrastructure","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#lxc-containers","title":"LXC Containers","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#github-runner-id-105","title":"GitHub Runner (ID: 105)","text":"<ul> <li>Purpose: Self-hosted GitHub Actions runner</li> <li>Resources: 2 CPU cores, 4GB RAM, 50GB disk</li> <li>Network: vmbr0 (10.20.0.10/24)</li> <li>Features: Nesting enabled for Docker-in-Docker</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#development-cloud-id-104","title":"Development Cloud (ID: 104)","text":"<ul> <li>Purpose: Development environment for cloud services</li> <li>Resources: 2 CPU cores, 4GB RAM, 20GB disk</li> <li>Network: vmbr0 (10.20.0.20/24)</li> <li>Features: Docker, Docker Compose, Git</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#production-cloud-id-106","title":"Production Cloud (ID: 106)","text":"<ul> <li>Purpose: Production environment for cloud services</li> <li>Resources: 4 CPU cores, 8GB RAM, 40GB disk</li> <li>Network: vmbr0 (10.20.0.30/24)</li> <li>Features: Docker, Docker Compose, Git, automated backups</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#networking","title":"Networking","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#vmbr1-bridge","title":"vmbr1 Bridge","text":"<ul> <li>Purpose: Isolated network for virtual device testing</li> <li>Network: 10.30.0.0/24</li> <li>Used by: virtual-smoker-device (when enabled)</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#configuration-details","title":"Configuration Details","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#resource-pools","title":"Resource Pools","text":"<p>All infrastructure is organized in the <code>smart-smoker</code> resource pool for easier management.</p>"},{"location":"Infrastructure/features/infrastructure/terraform/#container-features","title":"Container Features","text":"<ul> <li>Nesting: Enabled on all containers for Docker-in-Docker support</li> <li>Unprivileged: All containers run as unprivileged for security</li> <li>Auto-start: Containers start automatically on host boot</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#networking-configuration","title":"Networking Configuration","text":"<ul> <li>Primary Network: vmbr0 (10.20.0.0/24)</li> <li>GitHub Runner: 10.20.0.10/24</li> <li>Dev Cloud: 10.20.0.20/24</li> <li> <p>Prod Cloud: 10.20.0.30/24</p> </li> <li> <p>Isolated Network: vmbr1 (10.30.0.0/24)</p> </li> <li>Virtual Smoker Device: 10.30.0.40/24</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#state-management","title":"State Management","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#local-state-current","title":"Local State (Current)","text":"<p>State is stored locally at <code>state/terraform.tfstate</code>. This is suitable for single-user development.</p> <p>\u26a0\ufe0f Important: The state file is gitignored and should never be committed to version control as it may contain sensitive information.</p>"},{"location":"Infrastructure/features/infrastructure/terraform/#migrating-to-remote-state-recommended-for-teams","title":"Migrating to Remote State (Recommended for Teams)","text":"<p>For team collaboration, migrate to a remote backend:</p> <pre><code># shared/backend.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"your-terraform-state-bucket\"\n    key            = \"smart-smoker/proxmox/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-state-lock\"\n  }\n}\n</code></pre> <p>After updating, run:</p> <pre><code>terraform init -migrate-state\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/terraform/#common-operations","title":"Common Operations","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#adding-a-new-environment","title":"Adding a New Environment","text":"<ol> <li> <p>Create a new environment directory: <pre><code>mkdir -p environments/new-env\n</code></pre></p> </li> <li> <p>Create environment-specific configuration: <pre><code># environments/new-env/main.tf\nmodule \"container\" {\n  source = \"../../modules/lxc-container\"\n\n  target_node     = var.target_node\n  hostname        = var.hostname\n  # ... other variables\n}\n</code></pre></p> </li> <li> <p>Add the environment to <code>main.tf</code>: <pre><code>module \"new_env\" {\n  count  = var.new_env.enabled ? 1 : 0\n  source = \"./environments/new-env\"\n\n  # Pass variables\n}\n</code></pre></p> </li> <li> <p>Add variables to <code>variables.tf</code> and <code>terraform.tfvars</code></p> </li> </ol>"},{"location":"Infrastructure/features/infrastructure/terraform/#updating-container-resources","title":"Updating Container Resources","text":"<ol> <li> <p>Modify the resource allocation in <code>terraform.tfvars</code>: <pre><code>dev_cloud = {\n  enabled   = true\n  cpu_cores = 4  # Changed from 2\n  memory_mb = 8192  # Changed from 4096\n  # ... other settings\n}\n</code></pre></p> </li> <li> <p>Plan and apply: <pre><code>terraform plan\nterraform apply\n</code></pre></p> </li> </ol>"},{"location":"Infrastructure/features/infrastructure/terraform/#destroying-infrastructure","title":"Destroying Infrastructure","text":"<p>\u26a0\ufe0f Warning: This will destroy all managed infrastructure.</p> <pre><code># Destroy specific resource\nterraform destroy -target=module.dev_cloud[0]\n\n# Destroy all infrastructure\nterraform destroy\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/terraform/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#tainted-resources","title":"Tainted Resources","text":"<p>If a resource fails to create properly, it may be marked as \"tainted\":</p> <pre><code># Check for tainted resources\nterraform show\n\n# Untaint a resource\nterraform untaint 'module.virtual_smoker[0].module.vm.proxmox_virtual_environment_vm.this'\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/terraform/#provider-timeouts","title":"Provider Timeouts","text":"<p>If operations timeout, you may need to manually clean up on Proxmox:</p> <pre><code># On Proxmox host\nqm status &lt;vmid&gt;\nqm stop &lt;vmid&gt;\nqm unlock &lt;vmid&gt;\nrm -f /var/lock/qemu-server/lock-&lt;vmid&gt;.conf\n</code></pre> <p>Then remove from terraform state and reapply:</p> <pre><code>terraform state rm 'module.resource.path'\nterraform apply\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/terraform/#repository-issues-proxmox","title":"Repository Issues (Proxmox)","text":"<p>If you encounter enterprise repository errors on Proxmox:</p> <pre><code># Disable enterprise repos\nmv /etc/apt/sources.list.d/pve-enterprise.list /etc/apt/sources.list.d/pve-enterprise.list.disabled\nmv /etc/apt/sources.list.d/ceph.list /etc/apt/sources.list.d/ceph.list.disabled\n\n# Add no-subscription repo\necho \"deb http://download.proxmox.com/debian/pve bookworm pve-no-subscription\" &gt; /etc/apt/sources.list.d/pve-no-subscription.list\n\n# Update\napt-get update\n</code></pre>"},{"location":"Infrastructure/features/infrastructure/terraform/#security-best-practices","title":"Security Best Practices","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#sensitive-data","title":"Sensitive Data","text":"<ul> <li>Never commit <code>terraform.tfvars</code> containing real credentials</li> <li>Use <code>.gitignore</code> to exclude state files and variable files</li> <li>Rotate API tokens regularly</li> <li>Use environment variables or secret management tools for CI/CD</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#access-control","title":"Access Control","text":"<ul> <li>Use API tokens instead of passwords when possible</li> <li>Apply principle of least privilege</li> <li>Create dedicated Terraform users with minimal required permissions</li> <li>Enable audit logging on Proxmox</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#network-security","title":"Network Security","text":"<ul> <li>Use TLS for Proxmox API access in production (<code>tls_insecure = false</code>)</li> <li>Implement firewall rules to restrict access to management interfaces</li> <li>Use VPN or Tailscale for remote access to infrastructure</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#maintenance","title":"Maintenance","text":""},{"location":"Infrastructure/features/infrastructure/terraform/#regular-tasks","title":"Regular Tasks","text":"<ol> <li> <p>Update Providers: Check for provider updates monthly    <pre><code>terraform init -upgrade\n</code></pre></p> </li> <li> <p>Validate State: Ensure state matches reality    <pre><code>terraform plan\n</code></pre></p> </li> <li> <p>Backup State: If using local backend, backup state files regularly    <pre><code>cp state/terraform.tfstate state/terraform.tfstate.backup-$(date +%Y%m%d)\n</code></pre></p> </li> <li> <p>Review Logs: Check Terraform logs and Proxmox task history</p> </li> </ol>"},{"location":"Infrastructure/features/infrastructure/terraform/#disaster-recovery","title":"Disaster Recovery","text":"<ol> <li>State File Recovery: Keep backups of state files</li> <li>Import Existing Resources: If state is lost, resources can be imported</li> <li>Documentation: Maintain up-to-date documentation of all infrastructure</li> </ol>"},{"location":"Infrastructure/features/infrastructure/terraform/#next-steps","title":"Next Steps","text":"<p>After completing Phase 2 Story 1, proceed to:</p> <ul> <li>Story 2: Configure GitHub Actions self-hosted runner</li> <li>Story 3: Set up Tailscale networking for secure access</li> <li>Story 4: Create virtual device testing environment</li> </ul>"},{"location":"Infrastructure/features/infrastructure/terraform/#references","title":"References","text":"<ul> <li>Terraform Documentation</li> <li>Proxmox Provider Documentation</li> <li>Proxmox Infrastructure</li> </ul>"},{"location":"Infrastructure/features/networking/network-config/","title":"Network Configuration","text":""},{"location":"Infrastructure/features/networking/network-config/#overview","title":"Overview","text":"<p>Network configuration for Smart Smoker V2 infrastructure, including network bridges, IP addressing, DNS, and connectivity.</p>"},{"location":"Infrastructure/features/networking/network-config/#network-bridges","title":"Network Bridges","text":""},{"location":"Infrastructure/features/networking/network-config/#vmbr0-primary-network","title":"vmbr0 (Primary Network)","text":"<p>Network: 10.20.0.0/24 Purpose: Primary network for all infrastructure containers</p> <p>IP Assignments: - GitHub Runner: 10.20.0.10/24 - Dev Cloud: 10.20.0.20/24 - Prod Cloud: 10.20.0.30/24</p>"},{"location":"Infrastructure/features/networking/network-config/#vmbr1-isolated-network","title":"vmbr1 (Isolated Network)","text":"<p>Network: 10.30.0.0/24 Purpose: Isolated network for virtual device testing</p> <p>IP Assignments: - Virtual Smoker Device: 10.30.0.40/24</p>"},{"location":"Infrastructure/features/networking/network-config/#ip-addressing","title":"IP Addressing","text":""},{"location":"Infrastructure/features/networking/network-config/#static-ip-configuration","title":"Static IP Configuration","text":"<p>All containers use static IP addresses configured via Terraform:</p> <pre><code>network {\n  name   = \"eth0\"\n  bridge = \"vmbr0\"\n  ip     = \"10.20.0.20/24\"\n  gw     = \"10.20.0.1\"\n}\n</code></pre>"},{"location":"Infrastructure/features/networking/network-config/#dns-configuration","title":"DNS Configuration","text":"<p>DNS Servers: Configured via Terraform or Ansible</p> <pre><code>dns_servers:\n  - 10.0.0.1\n  - 10.0.0.2\nsearch_domain: smoker.local\n</code></pre>"},{"location":"Infrastructure/features/networking/network-config/#network-connectivity","title":"Network Connectivity","text":""},{"location":"Infrastructure/features/networking/network-config/#internal-network","title":"Internal Network","text":"<p>All containers can communicate via: - Proxmox Bridge: Direct container-to-container communication - Tailscale Mesh: Secure mesh networking</p>"},{"location":"Infrastructure/features/networking/network-config/#external-access","title":"External Access","text":"<ul> <li>Development: Internal Tailscale access only</li> <li>Production: Public access via Tailscale funnel</li> </ul>"},{"location":"Infrastructure/features/networking/network-config/#network-troubleshooting","title":"Network Troubleshooting","text":""},{"location":"Infrastructure/features/networking/network-config/#connectivity-issues","title":"Connectivity Issues","text":"<pre><code># Test network connectivity\nping -c 3 8.8.8.8\n\n# Test DNS\nnslookup google.com\n\n# Check network interface\nip addr show\n\n# Check routing\nip route show\n</code></pre>"},{"location":"Infrastructure/features/networking/network-config/#container-network","title":"Container Network","text":"<pre><code># Check container network config\npct config 104 | grep net\n\n# Test container connectivity\npct exec 104 -- ping -c 3 10.20.0.1\n\n# Check container DNS\npct exec 104 -- nslookup google.com\n</code></pre>"},{"location":"Infrastructure/features/networking/network-config/#bridge-configuration","title":"Bridge Configuration","text":"<pre><code># Check bridge status\nip link show vmbr0\n\n# Check bridge members\nbridge link show\n\n# View bridge configuration\ncat /etc/network/interfaces\n</code></pre>"},{"location":"Infrastructure/features/networking/network-config/#related-documentation","title":"Related Documentation","text":"<ul> <li>Tailscale Configuration - Tailscale networking</li> <li>Terraform Configuration - Network setup</li> <li>Proxmox Configuration - Container networking</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/features/networking/tailscale/","title":"Proxmox LXC Tailscale Fix","text":"<p>Status: \u2705 RESOLVED (November 25, 2025) Phase 2 Story 3 Completion: Tailscale mesh network operational</p>"},{"location":"Infrastructure/features/networking/tailscale/#problem-identified","title":"Problem Identified","text":"<p>The Tailscale implementation failed on Proxmox LXC containers because they were missing the <code>/dev/net/tun</code> device required for Tailscale's WireGuard VPN.</p>"},{"location":"Infrastructure/features/networking/tailscale/#root-cause","title":"Root Cause","text":"<ul> <li>Tailscale requires <code>/dev/net/tun</code> to create VPN tunnels</li> <li>Proxmox LXC containers don't have this device by default</li> <li>The Ansible role didn't account for this Proxmox-specific requirement</li> </ul>"},{"location":"Infrastructure/features/networking/tailscale/#error-symptoms","title":"Error Symptoms","text":"<pre><code>tailscaled: /dev/net/tun does not exist\ntailscaled: tun module not loaded nor found on disk\ntailscaled: CreateTUN(\"tailscale0\") failed\n</code></pre>"},{"location":"Infrastructure/features/networking/tailscale/#fix-applied","title":"Fix Applied","text":""},{"location":"Infrastructure/features/networking/tailscale/#1-lxc-container-configuration-completed","title":"1. LXC Container Configuration (\u2705 COMPLETED)","text":"<p>Added TUN device support to all containers by modifying their Proxmox configuration files:</p> <pre><code># Added to /etc/pve/lxc/104.conf, 105.conf, 106.conf\nlxc.cgroup2.devices.allow: c 10:200 rwm\nlxc.mount.entry: /dev/net dev/net none bind,create=dir 0 0\n</code></pre> <p>Containers affected: - 104 (github-runner) - 105 (smart-smoker-dev-cloud) - 106 (smart-smoker-cloud-prod)</p>"},{"location":"Infrastructure/features/networking/tailscale/#2-ssh-keys-re-applied-completed","title":"2. SSH Keys Re-applied (\u2705 COMPLETED)","text":"<p>SSH keys were re-added to all containers after the restarts.</p>"},{"location":"Infrastructure/features/networking/tailscale/#resolution-steps-taken","title":"Resolution Steps Taken","text":""},{"location":"Infrastructure/features/networking/tailscale/#step-1-re-run-ansible-playbooks-with-tailscale-auth-key","title":"Step 1: Re-run Ansible Playbooks with Tailscale Auth Key","text":"<p>The TUN devices were configured, and playbooks were executed:</p> <pre><code>cd /home/benjr70/Dev/Smart-Smoker-V2/infra/proxmox/ansible\n\n# Get your Tailscale auth key from: https://login.tailscale.com/admin/settings/keys\n\n# Option 1: Configure all infrastructure at once\nansible-playbook playbooks/site.yml \\\n  --extra-vars \"tailscale_auth_key=YOUR_TAILSCALE_KEY_HERE\"\n\n# Option 2: Configure each server individually\nansible-playbook playbooks/setup-github-runner.yml \\\n  --extra-vars \"tailscale_auth_key=YOUR_KEY\"\n\nansible-playbook playbooks/setup-dev-cloud.yml \\\n  --extra-vars \"tailscale_auth_key=YOUR_KEY\"\n\nansible-playbook playbooks/setup-prod-cloud.yml \\\n  --extra-vars \"tailscale_auth_key=YOUR_KEY\"\n</code></pre>"},{"location":"Infrastructure/features/networking/tailscale/#step-2-verify-tailscale-is-running","title":"Step 2: Verify Tailscale is Running","text":"<pre><code># Check all containers\nssh root@192.168.1.151 'for ct in 104 105 106; do echo \"=== Container $ct ===\"; pct exec $ct -- tailscale status; echo \"\"; done'\n\n# Or use the verification playbook\nansible-playbook playbooks/verify-tailscale.yml\n</code></pre>"},{"location":"Infrastructure/features/networking/tailscale/#step-3-check-your-tailscale-admin-console","title":"Step 3: Check Your Tailscale Admin Console","text":"<p>Visit https://login.tailscale.com/admin/machines</p> <p>You should now see these machines: - \u2705 <code>smoker-runner</code> (github-runner) - \u2705 <code>smoker-dev-cloud</code> (dev environment) - \u2705 <code>smokecloud</code> (production environment)</p>"},{"location":"Infrastructure/features/networking/tailscale/#important-notes","title":"Important Notes","text":""},{"location":"Infrastructure/features/networking/tailscale/#hostname-clarification","title":"Hostname Clarification","text":"<ul> <li><code>smokecloud</code> is configured as the hostname for the NEW production container (smart-smoker-cloud-prod)</li> <li>If you have an OLD machine also called <code>smokecloud</code>, you should:</li> <li>Rename the old machine in Tailscale admin, OR</li> <li>Change the new hostname in the Ansible configuration</li> </ul> <p>To change the new production hostname: <pre><code># Edit this file\nvim infra/proxmox/ansible/inventory/host_vars/smart-smoker-cloud-prod.yml\n\n# Change this line:\ntailscale_hostname: \"smokecloud\"\n\n# To something like:\ntailscale_hostname: \"smokecloud-new\"\n</code></pre></p>"},{"location":"Infrastructure/features/networking/tailscale/#virtual-smoker-device-1030040","title":"Virtual Smoker Device (10.30.0.40)","text":"<p>The virtual smoker device at 10.30.0.40 is currently unreachable via SSH. You'll need to: 1. Check if the VM is running 2. Verify network configuration 3. Configure it separately once it's accessible</p>"},{"location":"Infrastructure/features/networking/tailscale/#terraform-integration-future-enhancement","title":"Terraform Integration (Future Enhancement)","text":"<p>To prevent this issue in future deployments, the Terraform configuration should be updated to automatically configure TUN devices when creating LXC containers.</p> <p>Recommended addition to Terraform LXC module:</p> <pre><code># In infra/proxmox/terraform/modules/lxc-container/main.tf\nresource \"proxmox_lxc\" \"container\" {\n  # ... existing configuration ...\n\n  # Enable TUN device for Tailscale support\n  features {\n    nesting = true\n  }\n\n  # Note: TUN device config must be added manually via Proxmox API\n  # or post-provisioning script as Terraform provider doesn't support\n  # lxc.cgroup2.devices.allow and lxc.mount.entry parameters\n}\n</code></pre> <p>Post-provisioning script approach (recommended):</p> <p>Create <code>infra/proxmox/scripts/configure-lxc-for-tailscale.sh</code>:</p> <pre><code>#!/bin/bash\n# Configure Proxmox LXC container for Tailscale support\n# Usage: ./configure-lxc-for-tailscale.sh &lt;container_id&gt;\n\nCT_ID=$1\n\nif [ -z \"$CT_ID\" ]; then\n    echo \"Usage: $0 &lt;container_id&gt;\"\n    exit 1\nfi\n\necho \"Configuring container $CT_ID for Tailscale...\"\n\n# Add TUN device configuration\nif ! grep -q \"lxc.cgroup2.devices.allow.*10:200\" /etc/pve/lxc/$CT_ID.conf 2&gt;/dev/null; then\n    echo \"lxc.cgroup2.devices.allow: c 10:200 rwm\" &gt;&gt; /etc/pve/lxc/$CT_ID.conf\n    echo \"lxc.mount.entry: /dev/net dev/net none bind,create=dir 0 0\" &gt;&gt; /etc/pve/lxc/$CT_ID.conf\n    echo \"\u2705 TUN device configured for container $CT_ID\"\nelse\n    echo \"\u2139\ufe0f Container $CT_ID already configured for TUN\"\nfi\n</code></pre> <p>Then call this from Terraform using a <code>local-exec</code> provisioner or Ansible.</p>"},{"location":"Infrastructure/features/networking/tailscale/#testing","title":"Testing","text":"<p>After re-running the Ansible playbooks, test connectivity:</p> <pre><code># Run comprehensive test suite\nbash infra/proxmox/scripts/test-tailscale-mesh.sh\n\n# Or manually test\nssh root@10.20.0.10  # github-runner\ntailscale status\nping -c 3 smoker-dev-cloud\nping -c 3 smokecloud\n\n# Test public access (production funnel)\ncurl https://smokecloud.tail74646.ts.net\n</code></pre>"},{"location":"Infrastructure/features/networking/tailscale/#status","title":"Status","text":"<ul> <li>[x] Identified root cause (missing /dev/net/tun)</li> <li>[x] Fixed LXC container configurations</li> <li>[x] Re-applied SSH keys</li> <li>[x] Re-ran Ansible playbooks with Tailscale auth key</li> <li>[x] Verified all machines appear in Tailscale admin (3 containers connected)</li> <li>[x] Tested mesh network connectivity</li> <li>[ ] Test production funnel (public HTTPS access) - Future work</li> </ul>"},{"location":"Infrastructure/features/networking/tailscale/#references","title":"References","text":"<ul> <li>Proxmox LXC TUN devices: https://pve.proxmox.com/wiki/Linux_Container#pct_options</li> <li>Tailscale on LXC: https://tailscale.com/kb/1130/lxc-unprivileged</li> <li>Terraform Proxmox provider: https://registry.terraform.io/providers/Telmate/proxmox/latest/docs</li> </ul>"},{"location":"Infrastructure/features/operations/disaster-recovery/","title":"Disaster Recovery Guide","text":""},{"location":"Infrastructure/features/operations/disaster-recovery/#overview","title":"Overview","text":"<p>This guide provides procedures for recovering the Smart Smoker V2 infrastructure in case of failures, data loss, or catastrophic events.</p>"},{"location":"Infrastructure/features/operations/disaster-recovery/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Backup Procedures</li> <li>Recovery Scenarios</li> <li>Terraform State Recovery</li> <li>Container Recovery</li> <li>Data Restoration</li> <li>Testing Recovery Procedures</li> </ol>"},{"location":"Infrastructure/features/operations/disaster-recovery/#backup-procedures","title":"Backup Procedures","text":""},{"location":"Infrastructure/features/operations/disaster-recovery/#terraform-state-backups","title":"Terraform State Backups","text":"<p>Automated Backup (Recommended): <pre><code># Add to cron job (daily at 2 AM)\n0 2 * * * cd /home/benjr70/Dev/Smart-Smoker-V2/infra/proxmox/terraform &amp;&amp; \\\n  cp state/terraform.tfstate state/terraform.tfstate.backup.$(date +\\%Y\\%m\\%d) &amp;&amp; \\\n  find state/ -name \"terraform.tfstate.backup.*\" -mtime +30 -delete\n</code></pre></p> <p>Manual Backup: <pre><code>cd infra/proxmox/terraform\ncp state/terraform.tfstate state/terraform.tfstate.backup.$(date +%Y%m%d-%H%M%S)\n</code></pre></p> <p>Remote Backup (For production): <pre><code># Sync to remote storage\naws s3 cp state/terraform.tfstate s3://smart-smoker-terraform/state/terraform.tfstate.$(date +%Y%m%d)\n# Or use rsync to remote server\nrsync -avz state/terraform.tfstate backup-server:/backups/terraform/\n</code></pre></p>"},{"location":"Infrastructure/features/operations/disaster-recovery/#proxmox-backups","title":"Proxmox Backups","text":"<p>LXC Container Backups: <pre><code># On Proxmox host - backup all containers\nvzdump 104 105 106 --mode snapshot --compress zstd --storage backup-storage\n\n# Automated backup (add to Proxmox Datacenter &gt; Backup)\n# Schedule: Daily at 1 AM\n# Retention: 7 daily, 4 weekly, 3 monthly\n</code></pre></p> <p>VM Backups: <pre><code># Backup virtual smoker VM\nvzdump 9001 --mode snapshot --compress zstd --storage backup-storage\n</code></pre></p>"},{"location":"Infrastructure/features/operations/disaster-recovery/#configuration-backups","title":"Configuration Backups","text":"<p>Backup terraform.tfvars (Encrypted): <pre><code># Encrypt and backup sensitive configuration\ngpg --symmetric --cipher-algo AES256 terraform.tfvars\ncp terraform.tfvars.gpg ~/secure-backups/terraform.tfvars.$(date +%Y%m%d).gpg\n\n# Decrypt when needed\ngpg --decrypt terraform.tfvars.gpg &gt; terraform.tfvars\n</code></pre></p> <p>Git Repository Backup: <pre><code># Clone mirror to backup location\ngit clone --mirror https://github.com/benjr70/Smart-Smoker-V2.git\ncd Smart-Smoker-V2.git\ngit remote update\n</code></pre></p>"},{"location":"Infrastructure/features/operations/disaster-recovery/#recovery-scenarios","title":"Recovery Scenarios","text":""},{"location":"Infrastructure/features/operations/disaster-recovery/#scenario-1-single-container-failure","title":"Scenario 1: Single Container Failure","text":"<p>Symptoms: One container is not responding or corrupted</p> <p>Recovery Steps: 1. Verify container status:    <pre><code>pct status 104  # Replace with affected VMID\n</code></pre></p> <ol> <li> <p>Attempt to restore from backup:    <pre><code># List available backups\nls /var/lib/vz/dump/ | grep 104\n\n# Restore from backup\npct restore 104 /var/lib/vz/dump/vzdump-lxc-104-YYYY_MM_DD-HH_MM_SS.tar.zst\n</code></pre></p> </li> <li> <p>If backup restoration fails, rebuild with Terraform:    <pre><code>cd infra/proxmox/terraform\n\n# Remove failed container from state\nterraform state rm module.dev_cloud[0].module.container.proxmox_virtual_environment_container.this\n\n# Re-create container\nterraform apply -target=module.dev_cloud\n</code></pre></p> </li> </ol>"},{"location":"Infrastructure/features/operations/disaster-recovery/#scenario-2-complete-proxmox-host-failure","title":"Scenario 2: Complete Proxmox Host Failure","text":"<p>Symptoms: Proxmox server is completely offline/unrecoverable</p> <p>Recovery Steps: 1. Install Fresh Proxmox VE:    - Install Proxmox VE on new hardware    - Configure network to match previous setup    - Set up storage pools matching original configuration</p> <ol> <li> <p>Restore Configuration:    <pre><code># Restore Terraform configuration from git\ngit clone https://github.com/benjr70/Smart-Smoker-V2.git\ncd Smart-Smoker-V2/infra/proxmox/terraform\n\n# Restore terraform.tfvars from encrypted backup\ngpg --decrypt ~/secure-backups/terraform.tfvars.YYYYMMDD.gpg &gt; terraform.tfvars\n</code></pre></p> </li> <li> <p>Rebuild Infrastructure:    <pre><code># Initialize Terraform\nterraform init\n\n# Review plan\nterraform plan\n\n# Apply infrastructure\nterraform apply\n</code></pre></p> </li> <li> <p>Restore Application Data (see Data Restoration section)</p> </li> </ol>"},{"location":"Infrastructure/features/operations/disaster-recovery/#scenario-3-terraform-state-file-corruption","title":"Scenario 3: Terraform State File Corruption","text":"<p>Symptoms: <code>terraform plan</code> shows unexpected changes or errors</p> <p>Recovery Steps: 1. Restore from backup:    <pre><code>cd infra/proxmox/terraform/state\n\n# List available backups\nls -lh terraform.tfstate.backup.*\n\n# Restore most recent valid backup\ncp terraform.tfstate.backup.YYYYMMDD terraform.tfstate\n\n# Verify\nterraform plan\n</code></pre></p> <ol> <li>If no backup available, rebuild state:    <pre><code># Import existing resources\nterraform import 'module.github_runner[0].module.container.proxmox_virtual_environment_container.this' 105\nterraform import 'module.dev_cloud[0].module.container.proxmox_virtual_environment_container.this' 104\nterraform import 'module.prod_cloud[0].module.container.proxmox_virtual_environment_container.this' 106\n\n# Verify state is correct\nterraform plan\n</code></pre></li> </ol>"},{"location":"Infrastructure/features/operations/disaster-recovery/#scenario-4-accidental-resource-deletion","title":"Scenario 4: Accidental Resource Deletion","text":"<p>Symptoms: Critical resources were destroyed (prevented by lifecycle policy in production)</p> <p>Recovery Steps: 1. If resources have <code>prevent_destroy = true</code>:    - Terraform will prevent deletion    - Error message: \"Error: Instance cannot be destroyed\"</p> <ol> <li> <p>If deletion occurred before lifecycle protection:    <pre><code># Check if resources still exist in Proxmox\npct list\nqm list\n\n# If resources exist, re-import to Terraform\nterraform import 'module.prod_cloud[0].module.container.proxmox_virtual_environment_container.this' 106\n\n# If resources don't exist, recreate\nterraform apply -target=module.prod_cloud\n</code></pre></p> </li> <li> <p>Restore data from backup</p> </li> </ol>"},{"location":"Infrastructure/features/operations/disaster-recovery/#terraform-state-recovery","title":"Terraform State Recovery","text":""},{"location":"Infrastructure/features/operations/disaster-recovery/#remote-backend-migration-recommended-for-production","title":"Remote Backend Migration (Recommended for Production)","text":"<p>Setup S3 Backend: <pre><code># infra/proxmox/terraform/backend.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"smart-smoker-terraform-state\"\n    key            = \"proxmox/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n</code></pre></p> <p>Migrate from Local to Remote: <pre><code># Backup current state\ncp state/terraform.tfstate state/terraform.tfstate.backup.pre-migration\n\n# Update backend.tf with remote configuration\n# Run migration\nterraform init -migrate-state\n\n# Verify migration\nterraform plan\n</code></pre></p>"},{"location":"Infrastructure/features/operations/disaster-recovery/#state-locking","title":"State Locking","text":"<p>PostgreSQL Backend with Locking: <pre><code>terraform {\n  backend \"pg\" {\n    conn_str = \"postgres://terraform:password@postgres.example.com/terraform_state?sslmode=require\"\n  }\n}\n</code></pre></p>"},{"location":"Infrastructure/features/operations/disaster-recovery/#container-recovery","title":"Container Recovery","text":""},{"location":"Infrastructure/features/operations/disaster-recovery/#github-runner-recovery","title":"GitHub Runner Recovery","text":"<p>Priority: HIGH - Required for CI/CD</p> <ol> <li>Restore container from backup or recreate with Terraform</li> <li> <p>Reinstall GitHub Actions runner:    <pre><code>pct enter 105\ncd /opt/actions-runner\n./config.sh --url https://github.com/benjr70/Smart-Smoker-V2 --token &lt;NEW_TOKEN&gt;\n./svc.sh install\n./svc.sh start\n</code></pre></p> </li> <li> <p>Verify runner connectivity in GitHub repository settings</p> </li> </ol>"},{"location":"Infrastructure/features/operations/disaster-recovery/#development-cloud-recovery","title":"Development Cloud Recovery","text":"<p>Priority: MEDIUM</p> <ol> <li>Restore container from backup or recreate</li> <li>Redeploy application:    <pre><code># Trigger deployment workflow or manual deploy\nssh root@10.20.0.20\ncd /opt/smart-smoker\ndocker-compose pull\ndocker-compose up -d\n</code></pre></li> </ol>"},{"location":"Infrastructure/features/operations/disaster-recovery/#production-cloud-recovery","title":"Production Cloud Recovery","text":"<p>Priority: CRITICAL</p> <ol> <li> <p>Restore from most recent backup:    <pre><code>pct restore 106 /var/lib/vz/dump/vzdump-lxc-106-latest.tar.zst\npct start 106\n</code></pre></p> </li> <li> <p>Verify application data integrity:    <pre><code>pct enter 106\ndocker ps\ndocker-compose logs\n</code></pre></p> </li> <li> <p>Restore database if needed (see Data Restoration)</p> </li> <li> <p>Update DNS/traffic routing after verification</p> </li> </ol>"},{"location":"Infrastructure/features/operations/disaster-recovery/#data-restoration","title":"Data Restoration","text":""},{"location":"Infrastructure/features/operations/disaster-recovery/#database-restoration","title":"Database Restoration","text":"<p>PostgreSQL/TimescaleDB (if used): <pre><code># Restore from backup\npct enter 106\ncd /var/lib/postgresql/backups\npg_restore -U postgres -d smart_smoker latest.dump\n\n# Or from continuous archive\nrestore_command = 'cp /var/lib/postgresql/wal_archive/%f %p'\n</code></pre></p>"},{"location":"Infrastructure/features/operations/disaster-recovery/#docker-volume-restoration","title":"Docker Volume Restoration","text":"<pre><code># Backup volumes\ndocker run --rm -v smart-smoker_data:/data -v $(pwd):/backup ubuntu \\\n  tar czf /backup/data-backup.tar.gz /data\n\n# Restore volumes\ndocker run --rm -v smart-smoker_data:/data -v $(pwd):/backup ubuntu \\\n  tar xzf /backup/data-backup.tar.gz -C /\n</code></pre>"},{"location":"Infrastructure/features/operations/disaster-recovery/#application-configuration","title":"Application Configuration","text":"<pre><code># Restore environment files\npct push 106 /secure-backups/.env /opt/smart-smoker/.env\n\n# Restore application configs\npct push 106 /secure-backups/config /opt/smart-smoker/config\n</code></pre>"},{"location":"Infrastructure/features/operations/disaster-recovery/#testing-recovery-procedures","title":"Testing Recovery Procedures","text":""},{"location":"Infrastructure/features/operations/disaster-recovery/#quarterly-dr-tests","title":"Quarterly DR Tests","text":"<p>Schedule: First weekend of each quarter</p> <p>Test Procedures:</p> <ol> <li> <p>Test Terraform State Backup/Restore:    <pre><code># Backup current state\ncp state/terraform.tfstate state/terraform.tfstate.CURRENT\n\n# Restore from old backup\ncp state/terraform.tfstate.backup.LAST_WEEK state/terraform.tfstate\n\n# Verify\nterraform plan\n\n# Restore current state\nmv state/terraform.tfstate.CURRENT state/terraform.tfstate\n</code></pre></p> </li> <li> <p>Test Container Recovery:    <pre><code># Create test container\nterraform apply -target=module.test_container\n\n# Destroy\nterraform destroy -target=module.test_container\n\n# Recover\nterraform apply -target=module.test_container\n</code></pre></p> </li> <li> <p>Test Backup Restoration:    <pre><code># Restore dev-cloud to test VMID\npct restore 199 /var/lib/vz/dump/vzdump-lxc-104-latest.tar.zst\n\n# Verify application starts\npct start 199\npct enter 199\ndocker-compose up -d\n\n# Cleanup\npct stop 199\npct destroy 199\n</code></pre></p> </li> </ol>"},{"location":"Infrastructure/features/operations/disaster-recovery/#dr-drill-checklist","title":"DR Drill Checklist","text":"<ul> <li>[ ] Terraform state backup exists and is recent (&lt; 24 hours)</li> <li>[ ] Can successfully restore Terraform state from backup</li> <li>[ ] Proxmox backups are configured and running</li> <li>[ ] Can restore LXC container from backup</li> <li>[ ] Can recreate container from Terraform</li> <li>[ ] terraform.tfvars is backed up securely</li> <li>[ ] Database backups are automated and tested</li> <li>[ ] Docker volumes are backed up</li> <li>[ ] Recovery procedures documentation is up-to-date</li> <li>[ ] All team members know where to find this guide</li> <li>[ ] Emergency contact information is current</li> </ul>"},{"location":"Infrastructure/features/operations/disaster-recovery/#emergency-contacts","title":"Emergency Contacts","text":"<p>Infrastructure Team: - Primary: [Name] - [Email] - [Phone] - Secondary: [Name] - [Email] - [Phone]</p> <p>Escalation Path: 1. Infrastructure Team 2. Development Lead 3. Technical Director</p> <p>External Vendors: - Proxmox Support: [Contact Info] - Cloud Provider: [Contact Info]</p>"},{"location":"Infrastructure/features/operations/disaster-recovery/#recovery-time-objectives-rto-recovery-point-objectives-rpo","title":"Recovery Time Objectives (RTO) &amp; Recovery Point Objectives (RPO)","text":"Component RTO RPO Priority Terraform State 15 minutes 24 hours HIGH GitHub Runner 30 minutes N/A HIGH Production Cloud 1 hour 1 hour CRITICAL Development Cloud 4 hours 24 hours MEDIUM Virtual Smoker VM 4 hours 24 hours LOW Application Data 1 hour 1 hour CRITICAL"},{"location":"Infrastructure/features/operations/disaster-recovery/#post-recovery-checklist","title":"Post-Recovery Checklist","text":"<p>After any recovery operation:</p> <ul> <li>[ ] Document what happened (incident report)</li> <li>[ ] Verify all services are operational</li> <li>[ ] Check monitoring and alerting</li> <li>[ ] Run <code>terraform plan</code> to verify infrastructure matches desired state</li> <li>[ ] Test application functionality</li> <li>[ ] Verify backups are working</li> <li>[ ] Update this document if procedures changed</li> <li>[ ] Conduct post-mortem meeting</li> <li>[ ] Identify preventative measures</li> </ul>"},{"location":"Infrastructure/features/operations/disaster-recovery/#related-documentation","title":"Related Documentation","text":"<ul> <li>Terraform Configuration</li> <li>Proxmox Infrastructure</li> <li>Proxmox Infrastructure README</li> <li>Secrets Management Guide</li> </ul> <p>Last Updated: 2025-10-05 Next Review Date: 2026-01-05 Document Owner: Infrastructure Team</p>"},{"location":"Infrastructure/features/operations/monitoring/","title":"Monitoring","text":""},{"location":"Infrastructure/features/operations/monitoring/#overview","title":"Overview","text":"<p>Health monitoring, logging, and alerting for Smart Smoker V2 infrastructure.</p>"},{"location":"Infrastructure/features/operations/monitoring/#health-monitoring","title":"Health Monitoring","text":""},{"location":"Infrastructure/features/operations/monitoring/#docker-health-checks","title":"Docker Health Checks","text":"<p>All services have Docker health checks configured:</p> <ul> <li>MongoDB: Ping check every 30s</li> <li>Backend: HTTP health endpoint check every 30s</li> <li>Frontend: HTTP check every 30s</li> </ul> <p>See Health Checks for details.</p>"},{"location":"Infrastructure/features/operations/monitoring/#health-endpoints","title":"Health Endpoints","text":"<p>Backend Health Endpoint: <code>/api/health</code></p> <pre><code># Check backend health\ncurl http://localhost:8443/api/health | jq\n\n# Expected response:\n# {\n#   \"status\": \"ok\",\n#   \"timestamp\": \"2025-12-07T14:30:00.000Z\",\n#   \"database\": {\n#     \"status\": \"connected\",\n#     \"name\": \"smartsmoker\"\n#   },\n#   \"uptime\": 3600,\n#   \"environment\": \"production\"\n# }\n</code></pre>"},{"location":"Infrastructure/features/operations/monitoring/#container-health-status","title":"Container Health Status","text":"<pre><code># View health status\ndocker ps --format \"table {{.Names}}\\t{{.Status}}\\t{{.Health}}\"\n\n# Filter healthy containers\ndocker ps --filter health=healthy\n\n# Filter unhealthy containers\ndocker ps --filter health=unhealthy\n</code></pre>"},{"location":"Infrastructure/features/operations/monitoring/#logging","title":"Logging","text":""},{"location":"Infrastructure/features/operations/monitoring/#container-logs","title":"Container Logs","text":"<pre><code># View backend logs\ndocker logs backend_cloud --tail 50 --follow\n\n# View MongoDB logs\ndocker logs mongo --tail 50 --follow\n\n# View all logs\ndocker compose -f cloud.docker-compose.yml logs -f --tail=100\n</code></pre>"},{"location":"Infrastructure/features/operations/monitoring/#system-logs","title":"System Logs","text":"<pre><code># View system logs\njournalctl -u docker -n 50\n\n# View SSH logs\njournalctl -u sshd -n 50\n\n# View backup logs\ntail -50 /var/log/mongodb-backup.log\n</code></pre>"},{"location":"Infrastructure/features/operations/monitoring/#log-management","title":"Log Management","text":"<p>Log Rotation: Configured via system logrotate</p> <p>Log Retention: 7 days for application logs, 30 days for system logs</p>"},{"location":"Infrastructure/features/operations/monitoring/#monitoring-tools","title":"Monitoring Tools","text":""},{"location":"Infrastructure/features/operations/monitoring/#resource-monitoring","title":"Resource Monitoring","text":"<pre><code># Check CPU and memory\ndocker stats\n\n# Check disk usage\ndf -h\n\n# Check container resources\npct exec 104 -- top -bn1\n</code></pre>"},{"location":"Infrastructure/features/operations/monitoring/#service-monitoring","title":"Service Monitoring","text":"<pre><code># Check service status\nsystemctl status docker\nsystemctl status tailscaled\n\n# Check container status\ndocker ps -a\n</code></pre>"},{"location":"Infrastructure/features/operations/monitoring/#runner-health-monitoring","title":"Runner Health Monitoring","text":"<p>The GitHub Actions runner has a self-healing systemd timer that auto-detects and fixes stale registrations every 5 minutes.</p> <pre><code># Check timer status\nsystemctl status runner-health-check.timer\n\n# View recent health check logs\njournalctl -u runner-health-check --since \"1 hour ago\"\n\n# Manually trigger a health check\nsystemctl start runner-health-check.service\n\n# Check runner service directly\nsystemctl status actions.runner.*\n</code></pre> <p>What the health check monitors:</p> <ul> <li>Runner <code>.runner</code> config file exists</li> <li>Runner systemd service is active</li> <li>No error loops in recent service logs (&gt;3 errors in 5 min = unhealthy)</li> </ul> <p>What it does when unhealthy:</p> <ul> <li>Checks DNS resolution for <code>api.github.com</code> (falls back to 8.8.8.8 if needed)</li> <li>Auto-generates a registration token from stored PAT</li> <li>Stops and uninstalls the stale runner service</li> <li>Re-registers with <code>--replace --unattended</code></li> <li>Installs and starts the new service</li> </ul>"},{"location":"Infrastructure/features/operations/monitoring/#alerting","title":"Alerting","text":""},{"location":"Infrastructure/features/operations/monitoring/#health-check-alerts","title":"Health Check Alerts","text":"<p>Health check failures trigger: - Automated rollback (in CI/CD) - Log entries - Deployment failure notifications</p>"},{"location":"Infrastructure/features/operations/monitoring/#backup-alerts","title":"Backup Alerts","text":"<p>Backup failures logged to: - <code>/var/log/mongodb-backup.log</code> - System logs</p>"},{"location":"Infrastructure/features/operations/monitoring/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"Infrastructure/features/operations/monitoring/#startup-time","title":"Startup Time","text":"<p>Before (no health checks): - Total startup: ~10 seconds</p> <p>After (with health checks): - Total startup: ~160 seconds</p> <p>Reason: Health check start periods allow services to initialize properly.</p>"},{"location":"Infrastructure/features/operations/monitoring/#runtime-performance","title":"Runtime Performance","text":"<ul> <li>Health Checks: Minimal CPU impact (~0.1% every 30s)</li> <li>Health Endpoints: Negligible impact on application performance</li> </ul>"},{"location":"Infrastructure/features/operations/monitoring/#monitoring-best-practices","title":"Monitoring Best Practices","text":""},{"location":"Infrastructure/features/operations/monitoring/#regular-checks","title":"Regular Checks","text":"<ol> <li>Daily: Review container health status</li> <li>Weekly: Review logs for errors</li> <li>Monthly: Review performance metrics</li> </ol>"},{"location":"Infrastructure/features/operations/monitoring/#alert-configuration","title":"Alert Configuration","text":"<ol> <li>Health Checks: Monitor container health status</li> <li>Backup Failures: Alert on backup failures</li> <li>Disk Space: Alert when disk usage &gt; 90%</li> <li>Service Failures: Alert on service crashes</li> </ol>"},{"location":"Infrastructure/features/operations/monitoring/#related-documentation","title":"Related Documentation","text":"<ul> <li>Health Checks - Health check configuration</li> <li>Deployment Automation - CI/CD monitoring</li> <li>Disaster Recovery - Recovery procedures</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/features/operations/testing/","title":"Infrastructure Testing &amp; Verification Guide","text":"<p>This guide provides procedures for testing and verifying Smart Smoker infrastructure deployments.</p>"},{"location":"Infrastructure/features/operations/testing/#overview","title":"Overview","text":"<p>Infrastructure testing ensures that all Proxmox containers are properly configured, secured, and operational. These tests should be run after any infrastructure changes or deployments.</p>"},{"location":"Infrastructure/features/operations/testing/#current-infrastructure","title":"Current Infrastructure","text":""},{"location":"Infrastructure/features/operations/testing/#lxc-containers","title":"LXC Containers","text":"Container ID Resources IP Address Purpose github-runner 104 2 CPU, 4GB RAM, 50GB 10.20.0.10 Self-hosted GitHub Actions runner smart-smoker-dev-cloud 105 2 CPU, 4GB RAM, 20GB 10.20.0.20 Development cloud environment smart-smoker-cloud-prod 106 4 CPU, 8GB RAM, 40GB 10.20.0.30 Production cloud environment"},{"location":"Infrastructure/features/operations/testing/#network-configuration","title":"Network Configuration","text":"Network CIDR Purpose vmbr0 192.168.1.0/24 External network vmbr0 (secondary) 10.20.0.0/24 Container internal network vmbr1 10.30.0.0/24 Isolated network for virtual devices <p>Proxmox Host: 192.168.1.151</p>"},{"location":"Infrastructure/features/operations/testing/#quick-verification","title":"Quick Verification","text":""},{"location":"Infrastructure/features/operations/testing/#automated-verification","title":"Automated Verification","text":"<p>The fastest way to verify infrastructure:</p> <pre><code>cd infra/proxmox/ansible\nansible-playbook playbooks/verify-all.yml\n</code></pre> <p>This playbook checks: - Docker installation and status - UFW firewall configuration - fail2ban service status - Docker and Node.js versions - GitHub runner configuration (if applicable) - Application directories</p>"},{"location":"Infrastructure/features/operations/testing/#quick-connectivity-test","title":"Quick Connectivity Test","text":"<pre><code># Test SSH connectivity to all servers\ncd infra/proxmox/ansible\nansible all -m ping\n</code></pre> <p>Expected: All servers return <code>pong</code> with SUCCESS status.</p>"},{"location":"Infrastructure/features/operations/testing/#detailed-testing-procedures","title":"Detailed Testing Procedures","text":""},{"location":"Infrastructure/features/operations/testing/#test-1-container-connectivity","title":"Test 1: Container Connectivity","text":""},{"location":"Infrastructure/features/operations/testing/#ssh-to-each-container","title":"SSH to Each Container","text":"<p>Test SSH jump host connectivity through Proxmox:</p> <pre><code># Test github-runner\nssh -J root@192.168.1.151 root@10.20.0.10 'hostname &amp;&amp; uptime'\n\n# Test dev-cloud\nssh -J root@192.168.1.151 root@10.20.0.20 'hostname &amp;&amp; uptime'\n\n# Test prod-cloud\nssh -J root@192.168.1.151 root@10.20.0.30 'hostname &amp;&amp; uptime'\n</code></pre> <p>Expected: All commands return hostname and uptime without errors.</p>"},{"location":"Infrastructure/features/operations/testing/#network-connectivity","title":"Network Connectivity","text":"<pre><code># Test internet connectivity\nssh -J root@192.168.1.151 root@10.20.0.10 'ping -c 3 8.8.8.8'\n\n# Test DNS resolution\nssh -J root@192.168.1.151 root@10.20.0.10 'ping -c 3 google.com'\n</code></pre> <p>Expected: 0% packet loss for both tests.</p>"},{"location":"Infrastructure/features/operations/testing/#test-2-docker-verification","title":"Test 2: Docker Verification","text":""},{"location":"Infrastructure/features/operations/testing/#check-docker-installation","title":"Check Docker Installation","text":"<pre><code># Check all containers\nfor ip in 10.20.0.10 10.20.0.20 10.20.0.30; do\n  echo \"=== Testing Docker on $ip ===\"\n  ssh -J root@192.168.1.151 root@$ip 'docker --version &amp;&amp; docker compose version'\ndone\n</code></pre> <p>Expected: - Docker version 28.5.1 or newer - Docker Compose version 2.x</p>"},{"location":"Infrastructure/features/operations/testing/#check-docker-service-status","title":"Check Docker Service Status","text":"<pre><code>for ip in 10.20.0.10 10.20.0.20 10.20.0.30; do\n  echo \"=== Testing $ip ===\"\n  ssh -J root@192.168.1.151 root@$ip 'systemctl is-active docker'\ndone\n</code></pre> <p>Expected: Output is <code>active</code> for all containers.</p>"},{"location":"Infrastructure/features/operations/testing/#test-docker-functionality","title":"Test Docker Functionality","text":"<pre><code>ssh -J root@192.168.1.151 root@10.20.0.10 'docker run --rm hello-world'\n</code></pre> <p>Expected: \"Hello from Docker!\" message appears.</p>"},{"location":"Infrastructure/features/operations/testing/#test-3-nodejs-verification","title":"Test 3: Node.js Verification","text":""},{"location":"Infrastructure/features/operations/testing/#check-nodejs-installation","title":"Check Node.js Installation","text":"<pre><code>for ip in 10.20.0.10 10.20.0.20 10.20.0.30; do\n  echo \"=== Node.js on $ip ===\"\n  ssh -J root@192.168.1.151 root@$ip 'node --version &amp;&amp; npm --version'\ndone\n</code></pre> <p>Expected: - Node.js: v20.x.x - npm: 10.x.x</p>"},{"location":"Infrastructure/features/operations/testing/#test-nodejs-functionality","title":"Test Node.js Functionality","text":"<pre><code>ssh -J root@192.168.1.151 root@10.20.0.10 'node -e \"console.log(\\\"Node.js works!\\\")\"'\n</code></pre> <p>Expected: Output is <code>Node.js works!</code></p>"},{"location":"Infrastructure/features/operations/testing/#test-4-terraform-verification-github-runner-only","title":"Test 4: Terraform Verification (GitHub Runner Only)","text":""},{"location":"Infrastructure/features/operations/testing/#check-terraform-installation","title":"Check Terraform Installation","text":"<pre><code>ssh -J root@192.168.1.151 root@10.20.0.10 'terraform version'\n</code></pre> <p>Expected: Terraform v1.13.3 or newer</p>"},{"location":"Infrastructure/features/operations/testing/#test-terraform-functionality","title":"Test Terraform Functionality","text":"<pre><code>ssh -J root@192.168.1.151 root@10.20.0.10 'cd /tmp &amp;&amp; terraform init'\n</code></pre> <p>Expected: Terraform initializes successfully.</p>"},{"location":"Infrastructure/features/operations/testing/#test-5-github-runner-verification","title":"Test 5: GitHub Runner Verification","text":""},{"location":"Infrastructure/features/operations/testing/#check-runner-service-status","title":"Check Runner Service Status","text":"<pre><code>ssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'systemctl status actions.runner.* --no-pager | head -20'\n</code></pre> <p>Expected: Service is <code>active (running)</code>.</p>"},{"location":"Infrastructure/features/operations/testing/#check-runner-registration","title":"Check Runner Registration","text":"<pre><code>gh api repos/benjr70/Smart-Smoker-V2/actions/runners \\\n  --jq '.runners[] | select(.name==\"smart-smoker-runner-1\") | {name, status, busy}'\n</code></pre> <p>Expected: <pre><code>{\n  \"name\": \"smart-smoker-runner-1\",\n  \"status\": \"online\",\n  \"busy\": false\n}\n</code></pre></p>"},{"location":"Infrastructure/features/operations/testing/#check-runner-logs","title":"Check Runner Logs","text":"<pre><code>ssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'journalctl -u actions.runner.* -n 50 --no-pager'\n</code></pre> <p>Expected: Recent activity with no errors.</p>"},{"location":"Infrastructure/features/operations/testing/#test-6-security-configuration","title":"Test 6: Security Configuration","text":""},{"location":"Infrastructure/features/operations/testing/#ufw-firewall-status","title":"UFW Firewall Status","text":"<pre><code>for ip in 10.20.0.10 10.20.0.20 10.20.0.30; do\n  echo \"=== Checking UFW on $ip ===\"\n  ssh -J root@192.168.1.151 root@$ip 'ufw status verbose | head -10'\ndone\n</code></pre> <p>Expected: - Status: <code>active</code> - SSH port 22: <code>ALLOW</code> - Default incoming: <code>deny</code> - Default outgoing: <code>allow</code></p>"},{"location":"Infrastructure/features/operations/testing/#fail2ban-status","title":"fail2ban Status","text":"<pre><code>for ip in 10.20.0.10 10.20.0.20 10.20.0.30; do\n  echo \"=== Checking fail2ban on $ip ===\"\n  ssh -J root@192.168.1.151 root@$ip 'systemctl is-active fail2ban'\ndone\n</code></pre> <p>Expected: Output is <code>active</code> for all containers.</p>"},{"location":"Infrastructure/features/operations/testing/#ssh-configuration","title":"SSH Configuration","text":"<pre><code>ssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'grep \"^PasswordAuthentication\" /etc/ssh/sshd_config'\n</code></pre> <p>Expected: <code>PasswordAuthentication no</code></p>"},{"location":"Infrastructure/features/operations/testing/#test-7-application-environment","title":"Test 7: Application Environment","text":""},{"location":"Infrastructure/features/operations/testing/#dev-cloud-directories","title":"Dev Cloud Directories","text":"<pre><code>ssh -J root@192.168.1.151 root@10.20.0.20 'ls -la /opt/smart-smoker-dev'\n</code></pre> <p>Expected: Directory exists with subdirectories: <code>data</code>, <code>logs</code>, <code>backups</code>, <code>config</code></p>"},{"location":"Infrastructure/features/operations/testing/#prod-cloud-directories","title":"Prod Cloud Directories","text":"<pre><code>ssh -J root@192.168.1.151 root@10.20.0.30 'ls -la /opt/smart-smoker-prod'\n</code></pre> <p>Expected: Directory exists with subdirectories: <code>data</code>, <code>logs</code>, <code>backups</code>, <code>config</code></p>"},{"location":"Infrastructure/features/operations/testing/#mongodb-data-directories","title":"MongoDB Data Directories","text":"<pre><code># Dev\nssh -J root@192.168.1.151 root@10.20.0.20 'ls -la /opt/smart-smoker-dev/data/mongodb'\n\n# Prod\nssh -J root@192.168.1.151 root@10.20.0.30 'ls -la /opt/smart-smoker-prod/data/mongodb'\n</code></pre> <p>Expected: Directories exist with proper permissions (owned by <code>smoker</code> user).</p>"},{"location":"Infrastructure/features/operations/testing/#test-8-container-resource-usage","title":"Test 8: Container Resource Usage","text":""},{"location":"Infrastructure/features/operations/testing/#check-cpu-and-memory","title":"Check CPU and Memory","text":"<pre><code>for ip in 10.20.0.10 10.20.0.20 10.20.0.30; do\n  echo \"=== Resources on $ip ===\"\n  ssh -J root@192.168.1.151 root@$ip 'free -h &amp;&amp; df -h /'\ndone\n</code></pre> <p>Expected: - Memory usage &lt; 80% under normal load - Disk usage has adequate free space - No swap usage under normal conditions</p>"},{"location":"Infrastructure/features/operations/testing/#test-9-inter-container-communication","title":"Test 9: Inter-Container Communication","text":""},{"location":"Infrastructure/features/operations/testing/#container-to-container-connectivity","title":"Container-to-Container Connectivity","text":"<pre><code># From github-runner, ping dev-cloud\nssh -J root@192.168.1.151 root@10.20.0.10 'ping -c 3 10.20.0.20'\n\n# From github-runner, ping prod-cloud\nssh -J root@192.168.1.151 root@10.20.0.10 'ping -c 3 10.20.0.30'\n</code></pre> <p>Expected: 0% packet loss between containers.</p>"},{"location":"Infrastructure/features/operations/testing/#test-10-proxmox-network-configuration","title":"Test 10: Proxmox Network Configuration","text":""},{"location":"Infrastructure/features/operations/testing/#verify-bridge-configuration","title":"Verify Bridge Configuration","text":"<pre><code>ssh root@192.168.1.151 'ip addr show vmbr0 | grep \"inet \"'\n</code></pre> <p>Expected: Shows both: - <code>inet 192.168.1.151/24</code> (external network) - <code>inet 10.20.0.1/24</code> (container network)</p>"},{"location":"Infrastructure/features/operations/testing/#verify-nat-configuration","title":"Verify NAT Configuration","text":"<pre><code>ssh root@192.168.1.151 'iptables -t nat -L POSTROUTING -n -v | grep 10.20.0.0'\n</code></pre> <p>Expected: MASQUERADE rule for 10.20.0.0/24 network.</p>"},{"location":"Infrastructure/features/operations/testing/#verify-ip-forwarding","title":"Verify IP Forwarding","text":"<pre><code>ssh root@192.168.1.151 'sysctl net.ipv4.ip_forward'\n</code></pre> <p>Expected: <code>net.ipv4.ip_forward = 1</code></p>"},{"location":"Infrastructure/features/operations/testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Infrastructure/features/operations/testing/#ssh-connection-failures","title":"SSH Connection Failures","text":"<p>Problem: Cannot connect to container via SSH jump host</p> <p>Solutions:</p> <pre><code># Check container is running\nssh root@192.168.1.151 'pct list | grep -E \"104|105|106\"'\n\n# Check network interface\nssh root@192.168.1.151 'pct exec 104 -- ip addr show'\n\n# Restart container if needed\nssh root@192.168.1.151 'pct reboot 104'\n</code></pre>"},{"location":"Infrastructure/features/operations/testing/#docker-service-not-running","title":"Docker Service Not Running","text":"<p>Problem: Docker service is inactive</p> <p>Solution:</p> <pre><code>ssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'systemctl restart docker &amp;&amp; systemctl status docker'\n</code></pre>"},{"location":"Infrastructure/features/operations/testing/#github-runner-offline","title":"GitHub Runner Offline","text":"<p>Problem: Runner shows as offline in GitHub</p> <p>Solutions:</p> <pre><code># Restart runner service\nssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'systemctl restart actions.runner.*'\n\n# Check service status\nssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'systemctl status actions.runner.* --no-pager'\n\n# View recent logs\nssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'journalctl -u actions.runner.* -n 100'\n</code></pre>"},{"location":"Infrastructure/features/operations/testing/#network-connectivity-issues","title":"Network Connectivity Issues","text":"<p>Problem: Containers cannot reach internet</p> <p>Solutions:</p> <pre><code># Check Proxmox gateway\nssh root@192.168.1.151 'ip addr show vmbr0 | grep 10.20.0.1'\n\n# Check NAT rules\nssh root@192.168.1.151 'iptables -t nat -L POSTROUTING -n'\n\n# Verify IP forwarding\nssh root@192.168.1.151 'sysctl net.ipv4.ip_forward'\n\n# Restart container networking\nssh root@192.168.1.151 'pct reboot 104'\n</code></pre>"},{"location":"Infrastructure/features/operations/testing/#ufw-blocks-required-ports","title":"UFW Blocks Required Ports","text":"<p>Problem: Firewall blocking necessary connections</p> <p>Solutions:</p> <pre><code># Check current UFW rules\nssh -J root@192.168.1.151 root@10.20.0.10 'ufw status verbose'\n\n# Allow specific port\nssh -J root@192.168.1.151 root@10.20.0.10 'ufw allow 8080/tcp'\n\n# Reload firewall\nssh -J root@192.168.1.151 root@10.20.0.10 'ufw reload'\n</code></pre>"},{"location":"Infrastructure/features/operations/testing/#testing-checklist","title":"Testing Checklist","text":"<p>Use this checklist after infrastructure changes:</p> <ul> <li>[ ] SSH connectivity to all containers works</li> <li>[ ] Docker installed and functional on all containers</li> <li>[ ] Node.js installed and functional on all containers</li> <li>[ ] Terraform installed on github-runner</li> <li>[ ] GitHub runner is online and registered</li> <li>[ ] UFW firewall active on all containers</li> <li>[ ] fail2ban running on all containers</li> <li>[ ] SSH hardened (password auth disabled)</li> <li>[ ] Application directories exist with proper structure</li> <li>[ ] Ansible verification playbook passes</li> <li>[ ] Container resources are healthy (CPU, memory, disk)</li> <li>[ ] Containers can communicate with each other</li> <li>[ ] Proxmox network configuration is correct</li> <li>[ ] NAT and IP forwarding configured</li> </ul>"},{"location":"Infrastructure/features/operations/testing/#automated-testing","title":"Automated Testing","text":""},{"location":"Infrastructure/features/operations/testing/#cicd-workflows","title":"CI/CD Workflows","text":"<p>The following GitHub Actions workflows provide automated testing:</p> <ul> <li>ansible-lint.yml: Validates Ansible syntax and best practices</li> <li>terraform-validate.yml: Validates Terraform configuration</li> <li>runner-test.yml: Tests self-hosted runner capabilities</li> </ul>"},{"location":"Infrastructure/features/operations/testing/#ansible-verification-playbook","title":"Ansible Verification Playbook","text":"<p>The <code>verify-all.yml</code> playbook provides automated verification:</p> <pre><code>cd infra/proxmox/ansible\nansible-playbook playbooks/verify-all.yml\n</code></pre> <p>This checks: - Docker installation and status - UFW and fail2ban services - Docker and Node.js versions - Application directories - GitHub runner configuration</p>"},{"location":"Infrastructure/features/operations/testing/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"Infrastructure/features/operations/testing/#resource-usage","title":"Resource Usage","text":"<pre><code># Monitor real-time resource usage\nssh -J root@192.168.1.151 root@10.20.0.10 'top -bn1 | head -20'\n\n# Check system load\nssh -J root@192.168.1.151 root@10.20.0.10 'uptime'\n\n# Check disk I/O\nssh -J root@192.168.1.151 root@10.20.0.10 'iostat -x 1 5'\n</code></pre>"},{"location":"Infrastructure/features/operations/testing/#network-performance","title":"Network Performance","text":"<pre><code># Test network throughput between containers\nssh -J root@192.168.1.151 root@10.20.0.10 \\\n  'ping -c 10 -i 0.2 10.20.0.20 | tail -1'\n</code></pre>"},{"location":"Infrastructure/features/operations/testing/#references","title":"References","text":"<ul> <li>Ansible Configuration</li> <li>Terraform Configuration</li> <li>Disaster Recovery Guide</li> <li>Secrets Management Guide</li> </ul>"},{"location":"Infrastructure/features/security/authentication/","title":"Authentication","text":""},{"location":"Infrastructure/features/security/authentication/#overview","title":"Overview","text":"<p>Authentication configuration for Smart Smoker V2 infrastructure, including MongoDB authentication, SSH keys, and access control.</p>"},{"location":"Infrastructure/features/security/authentication/#mongodb-authentication","title":"MongoDB Authentication","text":""},{"location":"Infrastructure/features/security/authentication/#two-user-security-model","title":"Two-User Security Model","text":"<ol> <li>Admin User (<code>admin</code>) - Full database access</li> <li>Created automatically by MongoDB from environment variables</li> <li>Used for administrative tasks</li> <li> <p>Never used by application code</p> </li> <li> <p>Application User (<code>smartsmoker</code>) - Limited readWrite access</p> </li> <li>Created by initialization script</li> <li>Only has readWrite permissions on <code>smartsmoker</code> database</li> <li>Used by backend application for all database operations</li> </ol>"},{"location":"Infrastructure/features/security/authentication/#user-creation","title":"User Creation","text":"<p>Admin User: <pre><code>MONGO_INITDB_ROOT_USERNAME=admin\nMONGO_INITDB_ROOT_PASSWORD=&lt;secure-password&gt;\n</code></pre></p> <p>Application User (<code>infra/mongodb-init/01-create-users.js</code>): <pre><code>db.createUser({\n  user: 'smartsmoker',\n  pwd: process.env.MONGO_APP_PASSWORD,\n  roles: [{ role: 'readWrite', db: 'smartsmoker' }]\n});\n</code></pre></p>"},{"location":"Infrastructure/features/security/authentication/#security-principles","title":"Security Principles","text":"<ul> <li>Principle of Least Privilege: Application user only has readWrite on smartsmoker database</li> <li>No Root Access: Backend never uses admin credentials</li> <li>Authentication Required: All connections must authenticate</li> <li>Strong Passwords: Base64-encoded 32-byte passwords (43 characters)</li> </ul> <p>See MongoDB Configuration for details.</p>"},{"location":"Infrastructure/features/security/authentication/#ssh-authentication","title":"SSH Authentication","text":""},{"location":"Infrastructure/features/security/authentication/#key-only-authentication","title":"Key-Only Authentication","text":"<p>Configuration: Password authentication disabled, key-only access</p> <p>SSH Key Setup: <pre><code># Generate SSH key pair\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n\n# Copy public key to server\nssh-copy-id user@host\n\n# Or manually\ncat ~/.ssh/id_ed25519.pub | ssh user@host \"mkdir -p ~/.ssh &amp;&amp; cat &gt;&gt; ~/.ssh/authorized_keys\"\n</code></pre></p>"},{"location":"Infrastructure/features/security/authentication/#ssh-key-management","title":"SSH Key Management","text":"<p>Current Status: SSH public keys configured in Ansible inventory</p> <p>Best Practices: - Use 4096-bit RSA or Ed25519 keys - Rotate SSH keys regularly - Use different keys for different environments - Store keys securely (password manager)</p>"},{"location":"Infrastructure/features/security/authentication/#ssh-configuration","title":"SSH Configuration","text":"<p>Hardened SSH Config: <pre><code>PasswordAuthentication no\nPubkeyAuthentication yes\nPermitRootLogin no\nAuthorizedKeysFile .ssh/authorized_keys\n</code></pre></p> <p>See System Setup for details.</p>"},{"location":"Infrastructure/features/security/authentication/#access-control","title":"Access Control","text":""},{"location":"Infrastructure/features/security/authentication/#user-management","title":"User Management","text":"<p>Container Users: - Root access for system administration - Application users for service execution - Limited permissions where possible</p>"},{"location":"Infrastructure/features/security/authentication/#service-accounts","title":"Service Accounts","text":"<p>MongoDB: - Admin user for database administration - Application user for service access</p> <p>Docker: - Root access required for Docker operations - Application users for container execution</p>"},{"location":"Infrastructure/features/security/authentication/#password-management","title":"Password Management","text":""},{"location":"Infrastructure/features/security/authentication/#password-generation","title":"Password Generation","text":"<p>MongoDB Passwords: <pre><code># Generate secure password\nopenssl rand -base64 32\n\n# URL-encode for connection strings\nENCODED_PASSWORD=$(printf %s \"$PASSWORD\" | jq -sRr @uri)\n</code></pre></p>"},{"location":"Infrastructure/features/security/authentication/#password-storage","title":"Password Storage","text":"<p>GitHub Secrets: Store passwords securely in GitHub Secrets</p> <p>Local Development: Use <code>.env</code> files (not committed to git)</p> <p>Production: Environment variables from GitHub Secrets</p> <p>See Secrets Management for details.</p>"},{"location":"Infrastructure/features/security/authentication/#authentication-testing","title":"Authentication Testing","text":""},{"location":"Infrastructure/features/security/authentication/#mongodb-authentication_1","title":"MongoDB Authentication","text":"<pre><code># Test admin authentication\ndocker exec mongo mongosh -u admin -p \"${MONGO_ROOT_PASSWORD}\" \\\n  --authenticationDatabase admin \\\n  --eval \"db.adminCommand({listDatabases: 1})\"\n\n# Test application user\ndocker exec mongo mongosh -u smartsmoker -p \"${MONGO_APP_PASSWORD}\" \\\n  --authenticationDatabase admin smartsmoker \\\n  --eval \"db.stats()\"\n</code></pre>"},{"location":"Infrastructure/features/security/authentication/#ssh-authentication_1","title":"SSH Authentication","text":"<pre><code># Test SSH connection\nssh -v user@host\n\n# Verify key authentication\nssh -o PreferredAuthentications=publickey user@host\n</code></pre>"},{"location":"Infrastructure/features/security/authentication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Infrastructure/features/security/authentication/#mongodb-authentication-failed","title":"MongoDB Authentication Failed","text":"<p>Symptoms: \"Authentication failed\" errors</p> <p>Solution: <pre><code># Verify environment variables\ndocker exec mongo env | grep MONGO\n\n# Test authentication manually\ndocker exec mongo mongosh -u admin -p \"${MONGO_ROOT_PASSWORD}\" \\\n  --authenticationDatabase admin\n\n# Check user exists\ndocker exec mongo mongosh -u admin -p \"${MONGO_ROOT_PASSWORD}\" \\\n  --authenticationDatabase admin \\\n  --eval \"db.getUsers()\"\n</code></pre></p>"},{"location":"Infrastructure/features/security/authentication/#ssh-key-not-working","title":"SSH Key Not Working","text":"<p>Symptoms: Cannot connect via SSH</p> <p>Solution: <pre><code># Check SSH service\nsystemctl status sshd\n\n# Verify key permissions\nchmod 600 ~/.ssh/authorized_keys\nchmod 700 ~/.ssh\n\n# Check SSH logs\njournalctl -u sshd -n 50\n\n# Test with verbose output\nssh -v user@host\n</code></pre></p>"},{"location":"Infrastructure/features/security/authentication/#related-documentation","title":"Related Documentation","text":"<ul> <li>MongoDB Configuration - MongoDB authentication</li> <li>Secrets Management - Password management</li> <li>System Setup - SSH configuration</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/features/security/secrets-management/","title":"Secrets Management Guide","text":""},{"location":"Infrastructure/features/security/secrets-management/#overview","title":"Overview","text":"<p>This guide covers secure secrets management for the Smart Smoker V2 infrastructure, including current practices and integration with HashiCorp Vault for enhanced security at scale.</p>"},{"location":"Infrastructure/features/security/secrets-management/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Current Secrets Management</li> <li>Security Best Practices</li> <li>Vault Integration (Recommended for Teams)</li> <li>GitHub Actions Secrets</li> <li>Rotating Secrets</li> <li>Secrets Audit</li> </ol>"},{"location":"Infrastructure/features/security/secrets-management/#current-secrets-management","title":"Current Secrets Management","text":""},{"location":"Infrastructure/features/security/secrets-management/#development-single-user","title":"Development (Single User)","text":"<p>terraform.tfvars (Local, gitignored): <pre><code>proxmox = {\n  api_url          = \"https://192.168.1.151:8006/\"\n  api_token_id     = \"terraform@pve!SmartSmoker\"\n  api_token_secret = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  # ...\n}\n</code></pre></p> <p>Security Measures: - \u2705 File excluded from git via <code>.gitignore</code> - \u2705 Stored locally with filesystem permissions (600) - \u2705 Backed up encrypted with GPG</p> <p>Limitations: - \u26a0\ufe0f No central secret rotation - \u26a0\ufe0f Manual sharing between team members - \u26a0\ufe0f No audit trail - \u26a0\ufe0f Difficult to rotate credentials</p>"},{"location":"Infrastructure/features/security/secrets-management/#security-best-practices","title":"Security Best Practices","text":""},{"location":"Infrastructure/features/security/secrets-management/#general-principles","title":"General Principles","text":"<ol> <li>Never Commit Secrets to Git</li> <li>Use <code>.gitignore</code> for sensitive files</li> <li> <p>Scan commits with tools like <code>git-secrets</code> or <code>gitleaks</code></p> </li> <li> <p>Use API Tokens Instead of Passwords <pre><code># Good: API token with limited scope\napi_token_id = \"terraform@pve!SmartSmoker\"\n\n# Bad: Root password\nusername = \"root@pam\"\npassword = \"MyPassword123\"\n</code></pre></p> </li> <li> <p>Principle of Least Privilege</p> </li> <li>Grant minimum required permissions</li> <li> <p>Use separate tokens for different environments</p> </li> <li> <p>Regular Rotation</p> </li> <li>Rotate API tokens quarterly</li> <li>Rotate production secrets monthly</li> <li>Immediate rotation if compromise suspected</li> </ol>"},{"location":"Infrastructure/features/security/secrets-management/#file-permissions","title":"File Permissions","text":"<p>Protect terraform.tfvars: <pre><code># Set restrictive permissions\nchmod 600 infra/proxmox/terraform/terraform.tfvars\n\n# Verify\nls -l infra/proxmox/terraform/terraform.tfvars\n# Should show: -rw------- (600)\n</code></pre></p>"},{"location":"Infrastructure/features/security/secrets-management/#encrypted-backups","title":"Encrypted Backups","text":"<p>Using GPG: <pre><code># Encrypt\ngpg --symmetric --cipher-algo AES256 terraform.tfvars\n# Creates: terraform.tfvars.gpg\n\n# Decrypt\ngpg --decrypt terraform.tfvars.gpg &gt; terraform.tfvars\n\n# Store encrypted file in secure location\nmv terraform.tfvars.gpg ~/secure-backups/\n</code></pre></p> <p>Using age (Modern alternative): <pre><code># Install age\nbrew install age  # macOS\nsudo apt install age  # Ubuntu\n\n# Generate key pair\nage-keygen -o key.txt\n\n# Encrypt\nage -r $(cat key.txt | grep public) -o terraform.tfvars.age terraform.tfvars\n\n# Decrypt\nage -d -i key.txt terraform.tfvars.age &gt; terraform.tfvars\n</code></pre></p>"},{"location":"Infrastructure/features/security/secrets-management/#vault-integration","title":"Vault Integration","text":""},{"location":"Infrastructure/features/security/secrets-management/#why-hashicorp-vault","title":"Why HashiCorp Vault?","text":"<p>Benefits: - \u2705 Centralized secret storage - \u2705 Automatic secret rotation - \u2705 Audit logging for all access - \u2705 Dynamic credentials - \u2705 Fine-grained access control - \u2705 Secret versioning</p>"},{"location":"Infrastructure/features/security/secrets-management/#setup-vault-server","title":"Setup Vault Server","text":"<p>Option 1: Vault in LXC Container (Recommended for self-hosted): <pre><code># Create vault container\ncd infra/proxmox/terraform\n# Add vault environment configuration\n\n# Install Vault\npct enter &lt;vault-container-id&gt;\nwget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | tee /etc/apt/sources.list.d/hashicorp.list\napt update &amp;&amp; apt install vault\n\n# Configure Vault\ncat &gt; /etc/vault.d/vault.hcl &lt;&lt;EOF\nstorage \"file\" {\n  path = \"/opt/vault/data\"\n}\n\nlistener \"tcp\" {\n  address     = \"0.0.0.0:8200\"\n  tls_disable = 1  # Use TLS in production!\n}\n\napi_addr = \"http://10.20.0.50:8200\"\ncluster_addr = \"https://10.20.0.50:8201\"\nui = true\nEOF\n\n# Start Vault\nsystemctl enable vault\nsystemctl start vault\n\n# Initialize (ONE TIME ONLY - SAVE OUTPUT!)\nvault operator init -key-shares=5 -key-threshold=3\n\n# Unseal Vault (required after restart)\nvault operator unseal &lt;UNSEAL_KEY_1&gt;\nvault operator unseal &lt;UNSEAL_KEY_2&gt;\nvault operator unseal &lt;UNSEAL_KEY_3&gt;\n</code></pre></p> <p>Option 2: HashiCorp Cloud Platform (HCP) Vault: - Managed service, no maintenance - Free tier available - Automatic backups and HA</p>"},{"location":"Infrastructure/features/security/secrets-management/#configure-vault-for-terraform","title":"Configure Vault for Terraform","text":"<p>Enable KV Secrets Engine: <pre><code>export VAULT_ADDR='http://10.20.0.50:8200'\nexport VAULT_TOKEN='&lt;root-token&gt;'\n\n# Enable KV v2 secrets engine\nvault secrets enable -path=smart-smoker kv-v2\n\n# Store Proxmox credentials\nvault kv put smart-smoker/proxmox \\\n  api_url=\"https://192.168.1.151:8006/\" \\\n  api_token_id=\"terraform@pve!SmartSmoker\" \\\n  api_token_secret=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n\n# Store per-environment secrets\nvault kv put smart-smoker/prod-cloud \\\n  initial_password=\"SuperSecurePassword123!\"\n\nvault kv put smart-smoker/dev-cloud \\\n  initial_password=\"DevPassword123!\"\n</code></pre></p> <p>Create Vault Policy for Terraform: <pre><code>vault policy write terraform-policy - &lt;&lt;EOF\n# Read Proxmox credentials\npath \"smart-smoker/data/proxmox\" {\n  capabilities = [\"read\"]\n}\n\n# Read environment secrets\npath \"smart-smoker/data/*\" {\n  capabilities = [\"read\", \"list\"]\n}\nEOF\n</code></pre></p> <p>Create Terraform Token: <pre><code>vault token create -policy=terraform-policy -ttl=8h\n# Save this token securely\n</code></pre></p>"},{"location":"Infrastructure/features/security/secrets-management/#update-terraform-for-vault","title":"Update Terraform for Vault","text":"<p>Add Vault Provider: <pre><code># infra/proxmox/terraform/providers.tf\nterraform {\n  required_providers {\n    proxmox = {\n      source  = \"bpg/proxmox\"\n      version = \"~&gt; 0.57.0\"\n    }\n    vault = {\n      source  = \"hashicorp/vault\"\n      version = \"~&gt; 3.23\"\n    }\n  }\n}\n\nprovider \"vault\" {\n  address = var.vault_addr\n  token   = var.vault_token\n}\n\ndata \"vault_kv_secret_v2\" \"proxmox\" {\n  mount = \"smart-smoker\"\n  name  = \"proxmox\"\n}\n\ndata \"vault_kv_secret_v2\" \"prod_cloud\" {\n  mount = \"smart-smoker\"\n  name  = \"prod-cloud\"\n}\n\nprovider \"proxmox\" {\n  endpoint  = data.vault_kv_secret_v2.proxmox.data[\"api_url\"]\n  api_token = format(\"%s=%s\",\n    data.vault_kv_secret_v2.proxmox.data[\"api_token_id\"],\n    data.vault_kv_secret_v2.proxmox.data[\"api_token_secret\"]\n  )\n  insecure = false\n}\n</code></pre></p> <p>Update variables.tf: <pre><code>variable \"vault_addr\" {\n  description = \"Vault server address\"\n  type        = string\n  default     = \"http://10.20.0.50:8200\"\n}\n\nvariable \"vault_token\" {\n  description = \"Vault authentication token\"\n  type        = string\n  sensitive   = true\n}\n</code></pre></p> <p>New terraform.tfvars (no secrets!): <pre><code># Only non-sensitive configuration\nvault_addr = \"http://10.20.0.50:8200\"\n\n# vault_token is provided via environment variable\n# export VAULT_TOKEN=s.xxxxxxxxxxxx\n</code></pre></p> <p>Run Terraform with Vault: <pre><code># Set Vault token\nexport VAULT_TOKEN=$(vault token create -policy=terraform-policy -ttl=8h -format=json | jq -r .auth.client_token)\n\n# Run Terraform\nterraform plan\nterraform apply\n</code></pre></p>"},{"location":"Infrastructure/features/security/secrets-management/#dynamic-proxmox-credentials","title":"Dynamic Proxmox Credentials","text":"<p>Enable Proxmox Secrets Engine (Future enhancement): <pre><code># This would require a Vault Proxmox plugin\n# Vault generates short-lived API tokens on-demand\nvault secrets enable proxmox\n\nvault write proxmox/config/access \\\n  proxmox_url=\"https://192.168.1.151:8006/\" \\\n  username=\"root@pam\" \\\n  password=\"admin-password\"\n\n# Terraform requests credentials\nvault read proxmox/creds/terraform\n# Returns: api_token with 1-hour TTL\n</code></pre></p>"},{"location":"Infrastructure/features/security/secrets-management/#github-actions-secrets","title":"GitHub Actions Secrets","text":""},{"location":"Infrastructure/features/security/secrets-management/#store-secrets-in-github","title":"Store Secrets in GitHub","text":"<p>Repository Secrets (Settings &gt; Secrets and variables &gt; Actions):</p> <p>MongoDB Secrets (Required for Phase 3 Story 0): <pre><code>MONGO_ROOT_USER: admin\nMONGO_ROOT_PASSWORD: &lt;strong-random-password&gt;  # Base64-encoded 32-byte\nMONGO_APP_PASSWORD: &lt;strong-random-password&gt;    # Base64-encoded 32-byte\n</code></pre></p> <p>How to Add MongoDB Secrets: 1. Navigate to repository: <code>https://github.com/YOUR_USERNAME/Smart-Smoker-V2</code> 2. Click Settings \u2192 Secrets and variables \u2192 Actions 3. Click New repository secret for each secret 4. Enter name and value 5. Click Add secret</p> <p>Generate Secure MongoDB Passwords: <pre><code># Generate MongoDB root password\nopenssl rand -base64 32\n\n# Generate MongoDB app password\nopenssl rand -base64 32\n\n# URL-encode for connection strings (done automatically in workflow)\n</code></pre></p> <p>SSH Secrets (Required for dev-deploy.yml workflow): <pre><code>SSH_PRIVATE_KEY: &lt;ed25519-private-key&gt;  # SSH key for deployment to dev-cloud\n</code></pre></p> <p>How to Set Up SSH_PRIVATE_KEY: 1. Generate an SSH key pair (if not already done):    <pre><code>ssh-keygen -t ed25519 -C \"github-actions-deploy\" -f ~/.ssh/github_deploy_key -N \"\"\n</code></pre> 2. Add the public key to the target server:    <pre><code>ssh-copy-id -i ~/.ssh/github_deploy_key.pub root@smoker-dev-cloud\n</code></pre> 3. Add the private key as a GitHub secret:    - Go to repository Settings &gt; Secrets and variables &gt; Actions    - Click New repository secret    - Name: <code>SSH_PRIVATE_KEY</code>    - Value: Contents of <code>~/.ssh/github_deploy_key</code> (the private key file, including the <code>-----BEGIN</code> and <code>-----END</code> lines)    - Click Add secret 4. Test SSH connectivity from the runner to verify setup</p> <p>Runner Auto-Registration (Required for runner self-healing):</p> <pre><code>RUNNER_PAT: &lt;fine-grained-pat&gt;  # GitHub PAT for auto-registering runners\n</code></pre> <p>How to Set Up RUNNER_PAT:</p> <ol> <li>Go to GitHub &gt; Settings &gt; Developer settings &gt; Fine-grained tokens &gt; Generate new token</li> <li>Token name: <code>smart-smoker-runner-autoregister</code></li> <li>Expiration: No expiration (or 1 year max)</li> <li>Repository access: Only <code>benjr70/Smart-Smoker-V2</code></li> <li>Permissions: Repository &gt; Administration: Read and write</li> <li>Copy the token and add as a GitHub Secret named <code>RUNNER_PAT</code></li> </ol> <p>On-Runner Storage: The Ansible role also deploys the PAT to <code>/etc/github-runner/pat</code> (0600, root-only) for the self-healing timer script that auto-re-registers stale runners.</p> <p>Other Secrets: <pre><code># Required for Terraform workflows\nVAULT_ADDR: http://10.20.0.50:8200\nVAULT_TOKEN: s.xxxxxxxxxxxx  # AppRole token with limited permissions\n\n# Or store Proxmox directly (without Vault)\nPROXMOX_API_URL: https://192.168.1.151:8006/\nPROXMOX_TOKEN_ID: terraform@pve!SmartSmoker\nPROXMOX_TOKEN_SECRET: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n\n# Discord notifications\nDISCORD_WEBHOOK_URL: &lt;discord-webhook-url&gt;  # For deployment notifications\n\n# Existing secrets\nVAPID_PUBLIC_KEY: &lt;existing&gt;\nVAPID_PRIVATE_KEY: &lt;existing&gt;\n</code></pre></p>"},{"location":"Infrastructure/features/security/secrets-management/#use-in-workflows","title":"Use in Workflows","text":"<p>Terraform Workflow with Vault: <pre><code># .github/workflows/terraform-apply.yml\nname: Terraform Apply\n\non:\n  workflow_dispatch:\n\njobs:\n  terraform:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v3\n\n      - name: Terraform Init\n        working-directory: infra/proxmox/terraform\n        run: terraform init\n\n      - name: Terraform Apply\n        working-directory: infra/proxmox/terraform\n        env:\n          VAULT_ADDR: ${{ secrets.VAULT_ADDR }}\n          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}\n        run: terraform apply -auto-approve\n</code></pre></p> <p>Alternative: Direct Secrets: <pre><code>      - name: Terraform Apply\n        working-directory: infra/proxmox/terraform\n        env:\n          TF_VAR_proxmox_api_url: ${{ secrets.PROXMOX_API_URL }}\n          TF_VAR_proxmox_token_id: ${{ secrets.PROXMOX_TOKEN_ID }}\n          TF_VAR_proxmox_token_secret: ${{ secrets.PROXMOX_TOKEN_SECRET }}\n        run: terraform apply -auto-approve\n</code></pre></p>"},{"location":"Infrastructure/features/security/secrets-management/#rotating-secrets","title":"Rotating Secrets","text":""},{"location":"Infrastructure/features/security/secrets-management/#proxmox-api-token-rotation","title":"Proxmox API Token Rotation","text":"<p>Quarterly Rotation Schedule: <pre><code># 1. Create new API token\npveum user token add terraform@pve SmartSmoker2 --privsep=0\n# Save output: terraform@pve!SmartSmoker2=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n\n# 2. Update Vault (if using)\nvault kv put smart-smoker/proxmox \\\n  api_url=\"https://192.168.1.151:8006/\" \\\n  api_token_id=\"terraform@pve!SmartSmoker2\" \\\n  api_token_secret=\"&lt;new-token&gt;\"\n\n# 3. Test with Terraform\nterraform plan\n\n# 4. Delete old token\npveum user token remove terraform@pve SmartSmoker\n\n# 5. Update documentation\necho \"$(date +%Y-%m-%d): Rotated Proxmox API token\" &gt;&gt; rotation-log.md\n</code></pre></p>"},{"location":"Infrastructure/features/security/secrets-management/#container-password-rotation","title":"Container Password Rotation","text":"<p>Update LXC Container Passwords: <pre><code># Generate new password\nNEW_PASSWORD=$(openssl rand -base64 32)\n\n# Update in Vault\nvault kv put smart-smoker/prod-cloud initial_password=\"$NEW_PASSWORD\"\n\n# Change password on running container\npct exec 106 -- passwd root\n# Enter new password twice\n\n# Update any scripts/automation using the password\n</code></pre></p>"},{"location":"Infrastructure/features/security/secrets-management/#secrets-audit","title":"Secrets Audit","text":""},{"location":"Infrastructure/features/security/secrets-management/#regular-audit-checklist","title":"Regular Audit Checklist","text":"<p>Monthly Review: - [ ] Review Vault audit logs for suspicious access - [ ] Verify no secrets in git history: <code>git log -p | grep -i \"password\\|token\\|secret\"</code> - [ ] Check file permissions on terraform.tfvars: <code>ls -l terraform.tfvars</code> - [ ] Verify encrypted backups are current - [ ] Review GitHub Actions secret usage - [ ] Confirm all team members have required access</p> <p>Tools for Secret Scanning: <pre><code># Install gitleaks\nbrew install gitleaks\n\n# Scan repository\ngitleaks detect --source . --verbose\n\n# Scan before commit (pre-commit hook)\ngitleaks protect --staged\n</code></pre></p>"},{"location":"Infrastructure/features/security/secrets-management/#vault-audit-log-analysis","title":"Vault Audit Log Analysis","text":"<pre><code># Enable audit logging\nvault audit enable file file_path=/var/log/vault_audit.log\n\n# Review recent access\ntail -f /var/log/vault_audit.log | jq '.request.path'\n\n# Find who accessed Proxmox credentials\ngrep \"smart-smoker/data/proxmox\" /var/log/vault_audit.log | jq '.auth.display_name'\n</code></pre>"},{"location":"Infrastructure/features/security/secrets-management/#migration-path","title":"Migration Path","text":""},{"location":"Infrastructure/features/security/secrets-management/#phase-1-current-single-developer","title":"Phase 1: Current (Single Developer)","text":"<ul> <li>\u2705 terraform.tfvars with gitignore</li> <li>\u2705 GPG encrypted backups</li> <li>\u2705 File permissions</li> </ul>"},{"location":"Infrastructure/features/security/secrets-management/#phase-2-team-expansion","title":"Phase 2: Team Expansion","text":"<ul> <li>\ud83d\udd04 Implement Vault server</li> <li>\ud83d\udd04 Migrate secrets to Vault</li> <li>\ud83d\udd04 Update Terraform to use Vault provider</li> <li>\ud83d\udd04 Document Vault operations</li> </ul>"},{"location":"Infrastructure/features/security/secrets-management/#phase-3-production-hardening","title":"Phase 3: Production Hardening","text":"<ul> <li>\u23f3 Enable Vault TLS</li> <li>\u23f3 Configure Vault auto-unseal</li> <li>\u23f3 Implement dynamic credentials</li> <li>\u23f3 Set up Vault replication</li> </ul>"},{"location":"Infrastructure/features/security/secrets-management/#related-documentation","title":"Related Documentation","text":"<ul> <li>Terraform Configuration</li> <li>Disaster Recovery</li> <li>Proxmox Infrastructure</li> </ul> <p>Last Updated: 2025-10-05 Next Review Date: 2026-01-05 Document Owner: Infrastructure Team</p>"},{"location":"Infrastructure/guides/architecture/","title":"Architecture Overview","text":""},{"location":"Infrastructure/guides/architecture/#system-architecture","title":"System Architecture","text":"<p>Smart Smoker V2 infrastructure runs on a local Proxmox server with the following components:</p>"},{"location":"Infrastructure/guides/architecture/#infrastructure-components","title":"Infrastructure Components","text":"<ul> <li>Proxmox Server: Host for all containers and VMs</li> <li>LXC Containers: Lightweight containers for services</li> <li>VMs: Virtual machines for device simulation</li> <li>Terraform: Infrastructure as Code provisioning</li> <li>Ansible: Configuration management</li> <li>Docker: Container runtime</li> <li>Tailscale: Secure networking</li> </ul>"},{"location":"Infrastructure/guides/architecture/#deployment-environments","title":"Deployment Environments","text":"<ul> <li>Development: dev-cloud (VMID 104) - Auto-deploy on master merge</li> <li>Production: prod-cloud (VMID 106) - Manual deployment with approval</li> <li>Testing: virtual-smoker-device (VMID 9001) - Device simulation</li> </ul>"},{"location":"Infrastructure/guides/architecture/#services","title":"Services","text":"<ul> <li>MongoDB 7.0: Database with authentication</li> <li>Backend: NestJS application</li> <li>Frontend: React application</li> <li>Device Service: Raspberry Pi device service</li> </ul>"},{"location":"Infrastructure/guides/architecture/#architecture-decisions","title":"Architecture Decisions","text":""},{"location":"Infrastructure/guides/architecture/#infrastructure-as-code","title":"Infrastructure as Code","text":"<p>Decision: Use Terraform for infrastructure provisioning</p> <p>Rationale: - Reproducible infrastructure - Version controlled - Easy to modify and scale</p>"},{"location":"Infrastructure/guides/architecture/#container-orchestration","title":"Container Orchestration","text":"<p>Decision: Use Docker Compose for service orchestration</p> <p>Rationale: - Simple deployment - Easy to manage - Good for single-server deployment</p>"},{"location":"Infrastructure/guides/architecture/#networking","title":"Networking","text":"<p>Decision: Use Tailscale for secure networking</p> <p>Rationale: - Secure mesh networking - Easy public access via funnels - No port forwarding required</p>"},{"location":"Infrastructure/guides/architecture/#backup-strategy","title":"Backup Strategy","text":"<p>Decision: Automated backups with retention policies</p> <p>Rationale: - Data protection - Disaster recovery - Compliance requirements</p>"},{"location":"Infrastructure/guides/architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"Infrastructure/guides/architecture/#authentication","title":"Authentication","text":"<ul> <li>MongoDB: Two-user model (admin + application)</li> <li>SSH: Key-only authentication</li> <li>Services: Authenticated connections</li> </ul>"},{"location":"Infrastructure/guides/architecture/#network-security","title":"Network Security","text":"<ul> <li>Firewall: UFW with minimal ports</li> <li>fail2ban: Brute force protection</li> <li>Tailscale: Encrypted mesh networking</li> </ul>"},{"location":"Infrastructure/guides/architecture/#secrets-management","title":"Secrets Management","text":"<ul> <li>GitHub Secrets: For CI/CD</li> <li>Environment Variables: For runtime</li> <li>No secrets in code: All secrets externalized</li> </ul>"},{"location":"Infrastructure/guides/architecture/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"Infrastructure/guides/architecture/#cicd-pipeline","title":"CI/CD Pipeline","text":"<ol> <li>Code pushed to GitHub</li> <li>GitHub Actions triggers workflow</li> <li>Self-hosted runner executes deployment</li> <li>Pre-deployment backup created</li> <li>Services deployed</li> <li>Health checks verify deployment</li> <li>Rollback on failure</li> </ol>"},{"location":"Infrastructure/guides/architecture/#deployment-safety","title":"Deployment Safety","text":"<ul> <li>Pre-deployment backups</li> <li>Health verification</li> <li>Automated rollback</li> <li>Audit trail</li> </ul>"},{"location":"Infrastructure/guides/architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>Getting Started - Quick start guide</li> <li>Infrastructure Features - Infrastructure details</li> <li>Deployment Features - Deployment details</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/guides/getting-started/","title":"Getting Started","text":""},{"location":"Infrastructure/guides/getting-started/#overview","title":"Overview","text":"<p>This guide provides a quick start for working with Smart Smoker V2 infrastructure. For detailed information, see the feature-specific documentation.</p>"},{"location":"Infrastructure/guides/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to Proxmox server</li> <li>SSH access to containers</li> <li>GitHub repository access</li> <li>Basic knowledge of Docker, Terraform, and Ansible</li> </ul>"},{"location":"Infrastructure/guides/getting-started/#quick-start","title":"Quick Start","text":""},{"location":"Infrastructure/guides/getting-started/#1-infrastructure-setup","title":"1. Infrastructure Setup","text":"<p>Terraform (provision infrastructure): <pre><code>cd infra/proxmox/terraform\ncp terraform.tfvars.example terraform.tfvars\n# Edit terraform.tfvars with your configuration\nterraform init\nterraform plan\nterraform apply\n</code></pre></p> <p>Ansible (configure infrastructure): <pre><code>cd infra/proxmox/ansible\nansible-playbook playbooks/site.yml\n</code></pre></p>"},{"location":"Infrastructure/guides/getting-started/#2-deploy-application","title":"2. Deploy Application","text":"<p>Development: <pre><code>cd /opt/smart-smoker-dev\ngit pull origin master\ndocker compose -f cloud.docker-compose.yml up -d\n</code></pre></p> <p>Production: <pre><code>cd /opt/smart-smoker-prod\ngit pull origin &lt;tag&gt;\n./scripts/deployment-backup.sh\ndocker compose -f cloud.docker-compose.yml up -d\n./scripts/deployment-health-check.sh localhost 3\n</code></pre></p>"},{"location":"Infrastructure/guides/getting-started/#3-verify-deployment","title":"3. Verify Deployment","text":"<pre><code># Check container health\ndocker ps --filter health=healthy\n\n# Test health endpoints\ncurl http://localhost:8443/api/health | jq\n\n# Check logs\ndocker compose -f cloud.docker-compose.yml logs --tail=50\n</code></pre>"},{"location":"Infrastructure/guides/getting-started/#common-tasks","title":"Common Tasks","text":""},{"location":"Infrastructure/guides/getting-started/#view-container-status","title":"View Container Status","text":"<pre><code># All containers\ndocker ps -a\n\n# Healthy containers\ndocker ps --filter health=healthy\n\n# Container logs\ndocker logs &lt;container-name&gt; --tail 50\n</code></pre>"},{"location":"Infrastructure/guides/getting-started/#backup-operations","title":"Backup Operations","text":"<pre><code># Create backup\n/opt/smart-smoker-prod/scripts/backup-mongodb.sh\n\n# Verify backup\n/opt/smart-smoker-prod/scripts/backup-validation.sh\n\n# Restore from backup\n# See Backups documentation\n</code></pre>"},{"location":"Infrastructure/guides/getting-started/#troubleshooting","title":"Troubleshooting","text":"<pre><code># Check service health\n./scripts/deployment-health-check.sh localhost 3\n\n# View logs\ndocker compose -f cloud.docker-compose.yml logs -f\n\n# Restart services\ndocker compose -f cloud.docker-compose.yml restart\n</code></pre>"},{"location":"Infrastructure/guides/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture - Understand the system architecture</li> <li>Database Features - MongoDB configuration</li> <li>Deployment Features - Deployment automation</li> <li>Infrastructure Features - Infrastructure setup</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/","title":"Phase 3: Deployment Automation","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#overview","title":"Overview","text":"<p>Phase 3 implements automated deployment workflows using GitHub Actions with the self-hosted runner infrastructure established in Phase 2. This phase focuses on creating robust CI/CD pipelines for both cloud environments and Raspberry Pi devices, integrating the new container naming convention and Tailscale networking.</p> <p>IMPORTANT: Based on architectural review findings, Phase 3 priorities have been adjusted to address critical security and reliability issues before proceeding with advanced automation features.</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#goals-objectives","title":"Goals &amp; Objectives","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#primary-goals","title":"Primary Goals","text":"<ul> <li>Automated CI/CD: Implement GitHub Actions workflows for all deployment targets</li> <li>Multi-Environment Deployment: Support dev, staging, and production environments</li> <li>Raspberry Pi Integration: Automated deployment to physical devices</li> <li>Security &amp; Reliability: Secure deployment processes with rollback capabilities</li> <li>Monitoring &amp; Alerting: Real-time deployment status and failure notifications</li> </ul>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#success-criteria","title":"Success Criteria","text":"<ul> <li>\u2705 Automated deployment to dev environment on master merge</li> <li>\u2705 Manual production deployment with approval gates</li> <li>\u2705 Raspberry Pi deployment automation working</li> <li>\u2705 Container registry integration functional</li> <li>\u2705 Deployment rollback mechanisms in place</li> <li>\u2705 Comprehensive monitoring and alerting</li> </ul>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#architecture-components","title":"Architecture Components","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#deployment-pipeline-overview","title":"Deployment Pipeline Overview","text":"<pre><code>GitHub Repository\n\u251c\u2500\u2500 Feature Branch Push (PR)\n\u2502   \u251c\u2500\u2500 Lint &amp; Test (GitHub Hosted) \u2705 Already implemented\n\u2502   \u2514\u2500\u2500 PR checks must pass before merge\n\u2502\n\u251c\u2500\u2500 Master Branch Merge\n\u2502   \u251c\u2500\u2500 nightly.yml (builds :nightly images)\n\u2502   \u251c\u2500\u2500 dev-deploy.yml (triggered)\n\u2502   \u2502   \u251c\u2500\u2500 Deploy to dev-cloud (Proxmox LXC)\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Backend, Frontend, MongoDB\n\u2502   \u2502   \u251c\u2500\u2500 Deploy to virtual-smoker-device (Proxmox VM)\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Device-service, Smoker, Electron-shell\n\u2502   \u2502   \u251c\u2500\u2500 Health checks\n\u2502   \u2502   \u2514\u2500\u2500 Discord notifications\n\u2502   \u2514\u2500\u2500 E2E Testing (DEFERRED - Story 7)\n\u2502\n\u251c\u2500\u2500 Production Release (Manual)\n\u2502   \u251c\u2500\u2500 Manual Approval Required (workflow_dispatch)\n\u2502   \u251c\u2500\u2500 Validate dev environment\n\u2502   \u251c\u2500\u2500 Build production images (versioned tags)\n\u2502   \u251c\u2500\u2500 Deploy to prod-cloud (Proxmox LXC)\n\u2502   \u2502   \u251c\u2500\u2500 Tailscale funnel setup (public access)\n\u2502   \u2502   \u2514\u2500\u2500 Health checks\n\u2502   \u251c\u2500\u2500 Deploy to production Raspberry Pi\n\u2502   \u251c\u2500\u2500 Verify Deployment Health\n\u2502   \u2514\u2500\u2500 Send Notifications\n\u2502\n\u2514\u2500\u2500 Production Pi Updates\n    \u2514\u2500\u2500 Watchtower (automatic updates when Pi comes online)\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#deployment-targets","title":"Deployment Targets","text":"<pre><code>Deployment Infrastructure\n\u251c\u2500\u2500 Cloud Environments (Proxmox)\n\u2502   \u251c\u2500\u2500 Development (smoker-dev-cloud)\n\u2502   \u2502   \u251c\u2500\u2500 Auto-deploy on master merge\n\u2502   \u2502   \u251c\u2500\u2500 Latest container images\n\u2502   \u2502   \u251c\u2500\u2500 Integration testing\n\u2502   \u2502   \u2514\u2500\u2500 Tailscale internal access\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 Production (smokecloud)\n\u2502       \u251c\u2500\u2500 Manual deployment approval\n\u2502       \u251c\u2500\u2500 Tagged stable releases\n\u2502       \u251c\u2500\u2500 Health monitoring\n\u2502       \u251c\u2500\u2500 Tailscale funnel (public access)\n\u2502       \u2514\u2500\u2500 Automated backups\n\u2502\n\u251c\u2500\u2500 Physical Devices (Raspberry Pi)\n\u2502   \u251c\u2500\u2500 Production Smokers\n\u2502   \u2502   \u251c\u2500\u2500 Watchtower auto-updates\n\u2502   \u2502   \u251c\u2500\u2500 Standardized container names\n\u2502   \u2502   \u251c\u2500\u2500 Health monitoring\n\u2502   \u2502   \u2514\u2500\u2500 Remote management via Tailscale\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 Development Devices\n\u2502       \u251c\u2500\u2500 Beta testing releases\n\u2502       \u251c\u2500\u2500 Manual update triggers\n\u2502       \u2514\u2500\u2500 Development branch deployments\n\u2502\n\u2514\u2500\u2500 Virtual Testing (Proxmox VM)\n    \u251c\u2500\u2500 Integration test execution\n    \u251c\u2500\u2500 Mock hardware validation\n    \u251c\u2500\u2500 Performance testing\n    \u2514\u2500\u2500 User acceptance testing\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#critical-pre-work-security-reliability-fixes","title":"Critical Pre-Work: Security &amp; Reliability Fixes","text":"<p>Before implementing the deployment automation stories below, the following critical issues must be addressed:</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#critical-fix-1-mongodb-security-version-upgrade","title":"Critical Fix 1: MongoDB Security &amp; Version Upgrade","text":"<p>Current State: - Running MongoDB 4.4.14-rc0-focal (release candidate, not stable) - No authentication configured - Outdated version (current stable is 7.x+)</p> <p>Required Actions: 1. Upgrade to MongoDB 7.x stable release 2. Implement authentication with dedicated service accounts 3. Configure RBAC with minimum required permissions 4. Update all service connection strings 5. Test authentication in dev environment before production</p> <p>Acceptance Criteria: - [x] MongoDB upgraded to version 7.x stable - [x] Authentication enabled with username/password - [x] Backend service uses authenticated connection - [x] Connection strings stored securely (GitHub Secrets) - [ ] Dev and prod environments tested and working (IN PROGRESS) - [ ] Zero data loss during upgrade - [ ] All services reconnect successfully</p> <p>Estimated Effort: 1-2 days Risk: Medium Priority: CRITICAL - Must complete before any production deployment</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#critical-fix-2-automated-backup-implementation","title":"Critical Fix 2: Automated Backup Implementation","text":"<p>Current State: - No automated backup system - No backup validation testing - No documented restore procedures</p> <p>Required Actions: 1. Implement automated LXC container backups via Proxmox 2. Configure automated MongoDB dumps 3. Set up backup retention policies 4. Implement backup validation and integrity checks 5. Document and test restore procedures</p> <p>Acceptance Criteria: - [x] Daily automated backups of MongoDB (via Ansible role) - [x] Daily MongoDB dumps with gzip compression - [x] Conservative retention: 7 daily, 4 weekly, 12 monthly - [x] Automated backup validation runs weekly - [x] Restore procedure documented and scripted - [x] Backup failure logging via syslog</p> <p>Estimated Effort: 2-3 days Risk: Low Priority: CRITICAL - Required for production reliability</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#critical-fix-3-deployment-health-checks-rollback","title":"Critical Fix 3: Deployment Health Checks &amp; Rollback","text":"<p>Current State: - Manual intervention required for failed deployments - No automated health check validation - No automated rollback mechanism</p> <p>Required Actions: 1. Implement comprehensive health check scripts 2. Add automated health validation to deployment workflows 3. Configure automated rollback on health check failure 4. Add deployment status notifications</p> <p>Acceptance Criteria: - [x] Health check script validates all critical services - [x] Deployment workflows run health checks automatically - [x] Failed health checks trigger automated rollback - [x] Deployment status notifications in workflow output - [x] Rollback completes within 5 minutes - [ ] Rollback procedure tested in dev environment</p> <p>Estimated Effort: 2-3 days Risk: Low Priority: HIGH - Required before production automation</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#cicd-vision","title":"CI/CD Vision","text":"<p>The deployment automation follows this flow:</p> <ol> <li>PR Checks - Every PR runs automated checks (\u2705 already implemented)</li> <li>Master Merge - Automatically deploys to:</li> <li><code>dev-cloud</code> (LXC container on Proxmox) - Backend, Frontend, MongoDB</li> <li><code>virtual-smoker-device</code> (VM on Proxmox) - Smoker code (device-service, smoker, electron-shell)</li> <li>E2E Testing - Full end-to-end tests run against both dev-cloud and virtual smoker device (DEFERRED - will add later)</li> <li>Production Release - Manual workflow that:</li> <li>Takes what's in dev environment</li> <li>Deploys to <code>prod-cloud</code> (LXC on Proxmox)</li> <li>Deploys to actual Raspberry Pi smoker device</li> <li>Production Pi Updates - Uses Watchtower container for automatic updates (Pi is often offline, can't rely on GitHub Actions runner)</li> </ol>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#user-stories","title":"User Stories","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#story-0-critical-infrastructure-fixes-new-priority","title":"Story 0: Critical Infrastructure Fixes (NEW - PRIORITY)","text":"<p>As a DevOps engineer I want to address critical security and reliability issues So that the infrastructure is production-ready and secure</p> <p>Acceptance Criteria: - [x] MongoDB 7.0 upgrade implementation complete - [x] MongoDB authentication configured (admin + app users) - [x] Automated backups Ansible role created and deployed - [x] Deployment health checks implemented - [x] Automated rollback mechanism in GitHub Actions - [x] Tested in dev-cloud environment - [ ] Deployed to production (Deferred - will be handled in Story 2/3 with automated deployment) - [ ] Production environment stable for 7 days (Deferred - after production deployment)</p> <p>Implementation Details: - MongoDB 7.0 LTS with authentication - User initialization scripts in <code>infra/mongodb-init/</code> - Backend health endpoint at <code>/api/health</code> - Docker health checks for all services (mongo, backend, frontend) - Ansible backups role with conservative retention (7d/4w/12m) - Deployment scripts: health check, backup, rollback - GitHub Actions workflow updated with safety mechanisms</p> <p>Documentation: - Testing &amp; Deployment Guide: <code>docs/Infrastructure/phase3-story0-testing-deployment.md</code> - GitHub Secrets Setup: <code>docs/Infrastructure/github-secrets-setup.md</code></p> <p>Dependencies: - Phase 2 infrastructure provisioned and accessible</p> <p>Status: \u2705 COMPLETE - Ready for Merge</p> <p>Branch: <code>feat/infra-phase3-story-0</code> Commit: <code>3011ea3</code> Testing Completed: Dev-cloud testing successful - all infrastructure fixes validated Production Deployment: Deferred to Story 2/3 (automated deployment workflow) Next Steps: Merge to main, proceed to Story 1 or Story 2</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#story-1-automated-development-deployment-dev-cloud","title":"Story 1: Automated Development Deployment (Dev-Cloud)","text":"<p>As a developer I want my code automatically deployed to dev-cloud when merged to master So that I can quickly test integration changes</p> <p>Acceptance Criteria: - Master merge triggers automatic deployment to <code>dev-cloud</code> LXC container - Backend, Frontend, and MongoDB deployed via Docker Compose - Uses <code>:nightly</code> images from Docker Hub (built by existing <code>nightly.yml</code> workflow) - Health checks run automatically after deployment - Rollback triggered on health check failure - Discord notification sent on success/failure - Deployment completes within reasonable time (10-15 minutes)</p> <p>Implementation: - Leverage existing <code>nightly.yml</code> workflow - This already builds <code>:nightly</code> images on master push - Create <code>.github/workflows/dev-deploy.yml</code> workflow that:   - Triggers on master merge (or waits for <code>nightly.yml</code> to complete)   - Deploys to <code>smart-smoker-dev-cloud</code> via <code>cloud-deploy.yml</code> with <code>version=nightly</code> - Deploy to <code>smart-smoker-dev-cloud</code> hostname via Tailscale SSH - Use <code>cloud.docker-compose.yml</code> with <code>VERSION=nightly</code> - Integrate existing health check and rollback scripts</p> <p>Note: The <code>nightly.yml</code> workflow already exists and builds <code>:nightly</code> images on master push. Story 1 should leverage this existing workflow rather than duplicating build logic. We can either: - Option A: Have <code>dev-deploy.yml</code> depend on <code>nightly.yml</code> completion - Option B: Have <code>dev-deploy.yml</code> trigger <code>nightly.yml</code> if not already running - Option C: Merge deployment into <code>nightly.yml</code> workflow</p> <p>Dependencies: - Story 0 complete (health checks, rollback, backups) - Existing <code>nightly.yml</code> workflow (already in place)</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#story-2-virtual-smoker-device-infrastructure-setup","title":"Story 2: Virtual Smoker Device Infrastructure Setup","text":"<p>As a DevOps engineer I want the virtual smoker device infrastructure working and accessible So that I can deploy and test smoker code in a controlled environment</p> <p>Status: \ud83d\udd04 IN PROGRESS - Infrastructure code complete, awaiting VM provisioning</p> <p>Branch: <code>feat/infra-phase3-story-2</code></p> <p>Implementation Complete: - [x] Terraform configuration updated (<code>terraform.tfvars</code> - VM enabled with 4 cores, 1024MB RAM) - [x] Docker role enhanced with version pinning (<code>roles/docker/defaults/main.yml</code>) - [x] Virtual-device role enhanced with swap config (<code>roles/virtual-device/tasks/main.yml</code>) - [x] Device docker-compose file created (<code>virtual-smoker.docker-compose.yml</code>) - [x] Device health check script created (<code>scripts/device-health-check.sh</code>) - [x] Validation script created (<code>scripts/validate-virtual-smoker.sh</code>) - [x] Ansible inventory updated with Tailscale hostname notes</p> <p>Next Steps: 1. Create VM template (ID 9000) in Proxmox with cloud-init 2. Run <code>terraform apply</code> to provision VM 3. Run <code>ansible-playbook playbooks/setup-virtual-smoker.yml --extra-vars \"tailscale_auth_key=YOUR_KEY\"</code> 4. Run <code>./scripts/validate-virtual-smoker.sh virtual-smoker</code> to verify setup 5. Deploy containers with <code>docker compose -f virtual-smoker.docker-compose.yml up -d</code></p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#production-smoker-device-reference","title":"Production Smoker Device Reference","text":"<p>The virtual smoker device should match the production Raspberry Pi as closely as possible. Below are the production device specifications obtained via SSH inspection:</p> Attribute Production Value Model Raspberry Pi 3 Model B Rev 1.2 Architecture armv7l (32-bit ARM) CPU 4x Cortex-A53 @ 1.2GHz Memory 921Mi (~1GB) Swap 99Mi Disk 117GB total OS Raspbian GNU/Linux 11 (bullseye) Kernel 6.1.21-v7+ Docker 24.0.5 Tailscale Hostname <code>smoker</code> (DNSName: <code>smoker.tail74646.ts.net</code>) Display Element 14 7\" Touchscreen Microcontroller Arduino Nano (USB serial at <code>/dev/ttyUSB0</code>) <p>Acceptance Criteria:</p> <p>Core Infrastructure: - Virtual smoker device VM provisioned and accessible via Tailscale - Device can be reached via SSH from GitHub runner - Docker and Docker Compose installed on virtual device - Device-specific docker-compose file configured - Health check script works for virtual device - Device ready to receive deployments</p> <p>Hardware Parity: - CPU: 4 cores (matching Pi 3's quad-core Cortex-A53) - Memory: 1024MB (~1GB, matching production) - Swap: 100MB configured - Architecture: ARM-based (ARM64 acceptable with documented differences)</p> <p>Software Parity: - OS: Raspberry Pi OS / Raspbian 11 (bullseye) or compatible Debian-based - Docker: Version 24.x (pinned to match production) - Directory structure: <code>/opt/smoker-device</code> base path - Node.js runs in containers only (not installed on host)</p> <p>Tailscale Configuration: - Hostname: <code>virtual-smoker</code> (following naming pattern) - Tags: <code>tag:device</code>, <code>tag:virtual</code> - Accept routes and DNS enabled - Accessible from GitHub runner via Tailscale mesh</p> <p>Docker Compose Parity: - Container names match production: <code>device_service</code>, <code>frontend_smoker</code>, <code>electron_shell</code>, <code>watchtower</code> - Port mappings: 8080 (frontend), 3003 (device service) - Host networking for <code>device_service</code> and <code>electron_shell</code> - Watchtower configured for automatic updates - Health checks matching production configuration</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#documented-differences-virtual-vs-production","title":"Documented Differences (Virtual vs Production)","text":"Aspect Production Virtual Notes Architecture armv7l (32-bit) ARM64 (64-bit) 64-bit has better Proxmox support; containers still compatible Display Physical 7\" touchscreen VNC server VNC provides remote GUI access for testing USB Serial Physical Arduino at <code>/dev/ttyUSB0</code> Mock serial device Python simulator for temperature sensor data Kiosk Mode Auto-hide taskbar, no cursor Standard desktop Can be configured if needed for UI testing Network WiFi/Ethernet Virtual bridge Tailscale provides consistent access pattern <p>Implementation:</p> <p>Terraform (Complete): - <code>infra/proxmox/terraform/terraform.tfvars</code> - VM enabled with correct specs   - 4 CPU cores (matching Pi 3 quad-core)   - 1024MB RAM (matching Pi 3 ~1GB)   - Network bridge vmbr0 for Tailscale connectivity   - IP: 10.20.0.40/24</p> <p>Ansible Roles (Complete): - <code>roles/docker/defaults/main.yml</code> - Docker version pinning support - <code>roles/docker/tasks/main.yml</code> - Version pinning logic (5:24.0*) - <code>roles/virtual-device/defaults/main.yml</code> - Default variables - <code>roles/virtual-device/tasks/main.yml</code> - Swap config (100MB), directory structure - <code>inventory/group_vars/devices.yml</code> - Device-specific variables - <code>inventory/hosts.yml</code> - Updated with Tailscale hostname notes</p> <p>Docker Compose (Complete): - <code>virtual-smoker.docker-compose.yml</code> - Device container stack   - NODE_ENV=local for device-service emulator mode   - Container names match production (device_service, frontend_smoker, watchtower)   - Health checks on all services   - Uses amd64 images (not armhf)</p> <p>Scripts (Complete): - <code>scripts/device-health-check.sh</code> - Device service health validation - <code>scripts/validate-virtual-smoker.sh</code> - Infrastructure validation checklist</p> <p>Manual Steps Required: - Create Proxmox VM template (ID 9000) with cloud-init support - Run terraform apply to provision VM - Run Ansible playbook with Tailscale auth key - Mock hardware simulation deferred (device-service has built-in emulator mode)</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#validation-checklist","title":"Validation Checklist","text":"<p>After implementation, verify parity with production using <code>./scripts/validate-virtual-smoker.sh</code>:</p> <ul> <li>[ ] <code>free -h</code> shows ~1GB RAM configured (automated)</li> <li>[ ] <code>nproc</code> shows 4 CPU cores (automated)</li> <li>[ ] <code>cat /etc/os-release</code> shows bullseye or compatible</li> <li>[ ] <code>docker --version</code> shows 24.x (automated)</li> <li>[ ] <code>tailscale status</code> shows hostname <code>virtual-smoker</code> (automated)</li> <li>[ ] Swap configured (~100MB via <code>swapon --show</code>) (automated)</li> <li>[ ] Docker containers match production names (<code>docker ps</code>) (via device-health-check.sh)</li> <li>[ ] Health checks pass for all services (via device-health-check.sh)</li> <li>[ ] VNC access shows GUI environment (optional - vnc_enabled=false by default)</li> <li>[ ] Mock serial device responds (not needed - device-service has built-in emulator mode)</li> </ul> <p>Validation Commands: <pre><code># Run full infrastructure validation\n./scripts/validate-virtual-smoker.sh virtual-smoker\n\n# Run device health check after container deployment\n./scripts/device-health-check.sh virtual-smoker\n</code></pre></p> <p>Dependencies: - Phase 2 infrastructure (Terraform/Ansible) - Tailscale networking configured - Proxmox VM template (ID 9000) with cloud-init</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#story-3-virtual-smoker-device-deployment","title":"Story 3: Virtual Smoker Device Deployment","text":"<p>As a developer I want the smoker code automatically deployed to virtual-smoker-device when merged to master So that I can test device functionality in a controlled environment</p> <p>Acceptance Criteria: - Master merge triggers automatic deployment to <code>virtual-smoker-device</code> VM - Device-service, smoker, and electron-shell deployed via Docker Compose - Uses <code>:nightly</code> images from Docker Hub (built by <code>nightly.yml</code> workflow) - Device health checks run automatically - Deployment completes within reasonable time - Virtual device accessible via Tailscale for testing</p> <p>Implementation: - Extend <code>dev-deploy.yml</code> workflow to include virtual device deployment - Create reusable <code>.github/workflows/smoker-deploy-dev.yml</code> for virtual device - Deploy to <code>virtual-smoker-device</code> hostname via Tailscale SSH - Use device-specific docker-compose file - Add device health check script</p> <p>Dependencies: - Story 1 (dev-cloud deployment working) - Story 2 (virtual smoker infrastructure ready)</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#story-4-production-deployment-workflow","title":"Story 4: Production Deployment Workflow","text":"<p>As a product owner I want manual control over production deployments So that releases are coordinated and verified</p> <p>Acceptance Criteria: - Manual workflow trigger (workflow_dispatch) for production deployment - Deploys to <code>prod-cloud</code> LXC container (Backend, Frontend, MongoDB) - Deploys to actual Raspberry Pi smoker device - Tailscale funnel configured for public web app access - Pre-deployment validation (dev environment must be healthy) - Health checks run after deployment - Automated rollback on failure - Discord notifications for deployment status</p> <p>Implementation: - Create <code>.github/workflows/prod-deploy.yml</code> workflow - Reuse <code>cloud-deploy.yml</code> for prod-cloud (with version tags instead of nightly) - Create <code>.github/workflows/smoker-deploy-prod.yml</code> for Pi deployment - Configure Tailscale funnel on prod-cloud for public access:   - Set up funnel for frontend (port 80) to allow external access   - Configure funnel for backend API (port 8443) if needed   - Verify funnel URL is accessible from outside Tailscale network   - Document funnel configuration and URL   - Add funnel setup step to deployment workflow - Add manual approval step - Tag releases with version numbers - Deploy from dev environment state (promote dev \u2192 prod)</p> <p>Tailscale Funnel Setup: - Run <code>tailscale funnel</code> command on prod-cloud container after deployment - Configure funnel to expose frontend service (port 80) - Optionally expose backend API (port 8443) if public access needed - Test funnel URL from external network - Document funnel URL in deployment workflow output - Verify funnel persists across container restarts</p> <p>Dependencies: - Story 1 (dev-cloud deployment) - Story 3 (virtual smoker deployment) - for testing before prod - Story 0 (production infrastructure ready) - Tailscale configured on prod-cloud (from Phase 2)</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#story-5-production-database-migration","title":"Story 5: Production Database Migration","text":"<p>As a DevOps engineer I want to migrate the production database from Raspberry Pi to Proxmox So that production runs on reliable infrastructure with better performance</p> <p>Note: This story may be completed before Story 4 if needed, as it's infrastructure preparation.</p> <p>Context: This migration moves a single-user production database with minimal traffic. The migration strategy is designed for simplicity and safety rather than zero-downtime, as a brief maintenance window is acceptable for this use case.</p> <p>Acceptance Criteria: - Current Raspberry Pi database backed up (multiple copies) - MongoDB upgraded and secured BEFORE migration - Data migrated to prod-cloud LXC container with validation - Zero data loss during migration - Service cutover completed (30-60 minute maintenance window acceptable) - Old Pi kept as backup for 1-2 weeks before decommissioning - Rollback plan tested and documented</p> <p>Implementation: - Follow detailed migration procedure documented below - Migrate database from Pi to prod-cloud - Update connection strings and configurations - Test migration in dev environment first</p> <p>Dependencies: - Story 0 complete (MongoDB upgrade, backups) - prod-cloud LXC provisioned</p> <p>Technical Details:</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#current-state","title":"Current State","text":"<ul> <li>Database: MongoDB 7.x stable (upgraded from 4.4.14-rc0)</li> <li>Authentication: Enabled with dedicated service account</li> <li>Location: smart-smoker-cloud-prod LXC container (Proxmox)</li> <li>Data Path: <code>/opt/smart-smoker/database:/data/db</code></li> <li>Services: Same configuration with authenticated connections</li> <li>Backup: Integrated with Proxmox automated backup system</li> </ul>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#migration-procedure","title":"Migration Procedure","text":"<p>PREREQUISITE: Complete Story 0 (Critical Infrastructure Fixes) before starting migration. MongoDB MUST be upgraded to 7.x and authentication MUST be enabled on the Raspberry Pi BEFORE attempting migration.</p> <p>Phase 0: MongoDB Upgrade on Raspberry Pi (DO THIS FIRST) 1. Upgrade MongoDB on Raspberry Pi <pre><code># This must be done BEFORE migration to Proxmox\n# Follow MongoDB 4.x -&gt; 7.x upgrade path\n# Enable authentication during this upgrade\n# Test with backend service\n# Verify all data intact after upgrade\n</code></pre></p> <ol> <li>Verify Upgrade Success <pre><code># Confirm MongoDB 7.x running with authentication\n# Backend connecting successfully with credentials\n# All existing data accessible\n# Take final backup of upgraded database\n</code></pre></li> </ol> <p>Phase 1: Preparation (Pre-Migration) 1. Create Migration Plan <pre><code># Document current state\nssh pi@smokecloud \"docker ps --format 'table {{.Names}}\\t{{.Status}}\\t{{.Ports}}'\"\nssh pi@smokecloud \"docker exec mongo mongosh --eval 'db.adminCommand({listDatabases: 1})'\"\n\n# Check database size\nssh pi@smokecloud \"du -sh database/\"\n</code></pre></p> <ol> <li> <p>Backup Current Database <pre><code># Create comprehensive backup\nBACKUP_DATE=$(date +%Y%m%d-%H%M%S)\nssh pi@smokecloud \"docker exec mongo mongodump --out /data/db/backup-${BACKUP_DATE}\"\n\n# Download backup to safe location\nscp -r pi@smokecloud:/path/to/database/backup-${BACKUP_DATE} ./backups/\n\n# Verify backup integrity\nmongorestore --dry-run --drop ./backups/backup-${BACKUP_DATE}\n</code></pre></p> </li> <li> <p>Prepare Target Environment <pre><code># SSH to prod-cloud LXC\nssh root@smart-smoker-cloud-prod\n\n# Create directory structure\nmkdir -p /opt/smart-smoker/database\nchown -R 999:999 /opt/smart-smoker/database  # MongoDB user in container\n\n# Install Docker and Docker Compose (if not already done)\napt-get update &amp;&amp; apt-get install -y docker.io docker-compose\n</code></pre></p> </li> <li> <p>Test Migration on Dev Environment <pre><code># Perform dry-run migration on dev-cloud first\n# This validates the process without affecting production\n</code></pre></p> </li> </ol> <p>Phase 2: Migration Window (Downtime Required)</p> <ol> <li> <p>Announce Maintenance Window <pre><code># Send notification to users\n# Expected downtime: 30-60 minutes\n# Schedule for low-traffic period (e.g., 2 AM)\n</code></pre></p> </li> <li> <p>Stop Production Services on Pi <pre><code>ssh pi@smokecloud \"cd /path/to/compose &amp;&amp; docker-compose down\"\n</code></pre></p> </li> <li> <p>Create Final Backup <pre><code>FINAL_BACKUP=$(date +%Y%m%d-%H%M%S-final)\nssh pi@smokecloud \"sudo tar -czf /tmp/database-${FINAL_BACKUP}.tar.gz database/\"\nscp pi@smokecloud:/tmp/database-${FINAL_BACKUP}.tar.gz ./backups/\n</code></pre></p> </li> <li> <p>Transfer Database to Proxmox <pre><code># Method 1: Direct rsync (if both on Tailscale)\nssh pi@smokecloud \"rsync -avz --progress database/ root@smart-smoker-cloud-prod:/opt/smart-smoker/database/\"\n\n# Method 2: Via mongodump/mongorestore (cleaner, slower)\nssh pi@smokecloud \"docker exec mongo mongodump --archive=/tmp/db-export.archive --gzip\"\nscp pi@smokecloud:/tmp/db-export.archive /tmp/\nscp /tmp/db-export.archive root@smart-smoker-cloud-prod:/tmp/\n</code></pre></p> </li> <li> <p>Deploy MongoDB on Proxmox <pre><code>ssh root@smart-smoker-cloud-prod\ncd /opt/smart-smoker\n\n# Update cloud.docker-compose.yml with correct volume path\n# Ensure mongo service configured identically to Pi\n\n# Start MongoDB only\ndocker-compose up -d mongo\n\n# Wait for MongoDB to be ready\nsleep 10\ndocker-compose logs mongo\n</code></pre></p> </li> <li> <p>Restore Data (if using mongodump method) <pre><code>ssh root@smart-smoker-cloud-prod\ndocker exec -i mongo mongorestore --archive=/tmp/db-export.archive --gzip --drop\n</code></pre></p> </li> <li> <p>Verify Data Integrity <pre><code># Connect to new MongoDB instance\nssh root@smart-smoker-cloud-prod \"docker exec mongo mongosh\"\n\n# Run verification queries\ndb.adminCommand({listDatabases: 1})\ndb.getCollectionNames()\ndb.stats()\n\n# Count documents in each collection\ndb.users.countDocuments()\ndb.cooksessions.countDocuments()\n# ... verify all collections\n</code></pre></p> </li> <li> <p>Deploy Full Application Stack <pre><code>ssh root@smart-smoker-cloud-prod\ncd /opt/smart-smoker\n\n# Copy environment variables from Pi\n# VAPID_PUBLIC_KEY, VAPID_PRIVATE_KEY\n\n# Deploy via GitHub Actions or manual\nVERSION=latest \\\nVAPID_PUBLIC_KEY=\"${VAPID_PUBLIC_KEY}\" \\\nVAPID_PRIVATE_KEY=\"${VAPID_PRIVATE_KEY}\" \\\ndocker-compose -f cloud.docker-compose.yml up -d\n</code></pre></p> </li> </ol> <p>Phase 3: Verification &amp; Cutover</p> <ol> <li> <p>Application Health Checks <pre><code># Wait for services to start\nsleep 30\n\n# Check all containers running\nssh root@smart-smoker-cloud-prod \"docker-compose ps\"\n\n# Test backend endpoint\ncurl -f http://smart-smoker-cloud-prod:8443/health\n\n# Test frontend\ncurl -f http://smart-smoker-cloud-prod:80/\n\n# Test database connectivity\nssh root@smart-smoker-cloud-prod \"docker exec mongo mongosh --eval 'db.adminCommand({ping: 1})'\"\n</code></pre></p> </li> <li> <p>Functional Testing <pre><code># Login with test account\n# Verify existing cook sessions visible\n# Create new cook session\n# Verify real-time updates working\n# Test push notifications (if configured)\n</code></pre></p> </li> <li> <p>Update DNS/Tailscale (if needed) <pre><code># If using Tailscale funnel:\n# Update funnel to point to new prod-cloud instance\nssh root@smart-smoker-cloud-prod \"tailscale funnel --bg 80\"\n\n# Verify external access\ncurl https://smart-smoker-cloud-prod.tail74646.ts.net\n</code></pre></p> </li> <li> <p>Monitor for Issues <pre><code># Watch logs for errors\nssh root@smart-smoker-cloud-prod \"docker-compose logs -f --tail=100\"\n\n# Monitor resource usage\nssh root@smart-smoker-cloud-prod \"docker stats\"\n</code></pre></p> </li> </ol> <p>Phase 4: Cleanup &amp; Decommissioning</p> <ol> <li> <p>Keep Pi as Backup (24-48 hours) <pre><code># Don't delete Pi data immediately\n# Monitor new production for stability\n# Keep Pi available for emergency rollback\n</code></pre></p> </li> <li> <p>Document New Production <pre><code># Update documentation with new:\n# - Connection strings\n# - IP addresses\n# - Volume paths\n# - Backup procedures\n</code></pre></p> </li> <li> <p>Update GitHub Actions Workflows <pre><code># Update cloud-deploy.yml\n# Change runner from \"SmokeCloud\" to appropriate self-hosted runner\n# Or configure runner to deploy to new prod-cloud via Tailscale\n</code></pre></p> </li> <li> <p>Archive Pi Deployment (After 1 Week) <pre><code># Final backup\nssh pi@smokecloud \"sudo tar -czf /tmp/pi-archive-$(date +%Y%m%d).tar.gz /path/to/compose database/\"\nscp pi@smokecloud:/tmp/pi-archive-*.tar.gz ./archives/\n\n# Stop and remove containers\nssh pi@smokecloud \"docker-compose down -v\"\n\n# Optionally repurpose Pi for other use\n</code></pre></p> </li> </ol>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#rollback-plan","title":"Rollback Plan","text":"<p>If migration fails, rollback to Pi:</p> <pre><code># 1. Stop services on Proxmox\nssh root@smart-smoker-cloud-prod \"docker-compose down\"\n\n# 2. Restart services on Pi\nssh pi@smokecloud \"cd /path/to/compose &amp;&amp; docker-compose up -d\"\n\n# 3. Verify Pi services\ncurl -f http://smokecloud:8443/health\ncurl -f http://smokecloud:80/\n\n# 4. Update DNS/Tailscale back to Pi (if changed)\n\n# 5. Investigate issue before retry\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#post-migration-monitoring","title":"Post-Migration Monitoring","text":"<p>Week 1 Checklist: - [ ] Daily database backups configured and tested - [ ] Monitoring alerts configured (disk, memory, container health) - [ ] Performance metrics compared to Pi baseline - [ ] User feedback collected - [ ] No data loss or corruption reported - [ ] Backup Pi still available but not receiving traffic</p> <p>Week 2 Actions: - [ ] Archive Pi deployment - [ ] Update all documentation - [ ] Update disaster recovery procedures - [ ] Close migration ticket - [ ] Plan Pi repurposing (if applicable)</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#risk-assessment","title":"Risk Assessment","text":"Risk Impact Probability Mitigation Data loss during transfer Critical Low Multiple backups, verification steps Extended downtime High Medium Practice on dev, rollback plan ready Service incompatibility Medium Low Same MongoDB version, test first Network connectivity issues Medium Low Tailscale already configured Performance degradation Medium Low Proxmox more powerful than Pi"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#success-criteria-validation","title":"Success Criteria Validation","text":"<ul> <li>[x] Database fully migrated with zero data loss</li> <li>[x] All services running on Proxmox prod-cloud</li> <li>[x] Downtime &lt; 60 minutes</li> <li>[x] No user-reported issues after 48 hours</li> <li>[x] Performance equal or better than Pi</li> <li>[x] Automated backups working</li> <li>[x] Rollback plan tested and documented</li> </ul>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#story-6-watchtower-integration-for-production-pi","title":"Story 6: Watchtower Integration for Production Pi","text":"<p>As a system administrator I want the production Raspberry Pi to automatically update via Watchtower So that the device stays up-to-date even when offline</p> <p>Acceptance Criteria: - Watchtower container running on production Pi - Watchtower configured to monitor <code>benjr70/smart-smoker-*:latest</code> images - Automatic updates when Pi comes online - Standardized container naming works with Watchtower - Device health monitoring after updates - Update rollback capability if device fails</p> <p>Implementation: - Configure Watchtower on production Pi - Ensure container names match Watchtower expectations - Set up health checks for Watchtower to validate updates - Document Watchtower configuration - Test update process on dev/virtual device first</p> <p>Dependencies: - Story 4 (production deployment workflow) - Production Pi accessible and configured</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#story-7-end-to-end-testing-framework-deferred","title":"Story 7: End-to-End Testing Framework (DEFERRED)","text":"<p>As a QA engineer I want full end-to-end tests to run automatically after dev deployment So that integration issues are caught before production</p> <p>Status: DEFERRED - Will be implemented after core deployment workflows are stable</p> <p>Acceptance Criteria: - E2E tests run automatically after both dev-cloud and virtual-smoker deployments - Tests validate communication between dev-cloud backend and virtual device - Tests include: API endpoints, WebSocket communication, device service integration - Test results reported in workflow and Discord notifications - Failed tests don't block deployment but are reported</p> <p>Implementation: - Create <code>scripts/run-e2e-tests.sh</code> script - Tests connect to dev-cloud backend and virtual-smoker device - Validate full user workflows (create cook session, monitor temps, etc.) - Add test job to <code>dev-deploy.yml</code> workflow - Integrate test results into Discord notifications</p> <p>Dependencies: - Story 1 (dev-cloud deployment) - Story 3 (virtual smoker deployment) - Core deployment workflows stable</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#updated-workflow-architecture","title":"Updated Workflow Architecture","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#development-pipeline-master-merge","title":"Development Pipeline (Master Merge)","text":"<pre><code>Master Branch Merge\n    \u2193\nnightly.yml (existing - builds :nightly images)\n    \u2193\ndev-deploy.yml (triggered)\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 deploy-dev-cloud                \u2502\n\u2502 (cloud-deploy.yml with nightly)  \u2502\n\u2502 - Deploy to smart-smoker-dev-cloud\u2502\n\u2502 - Backend, Frontend, MongoDB    \u2502\n\u2502 - Health checks                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 deploy-virtual-smoker           \u2502\n\u2502 (smoker-deploy-dev.yml)         \u2502\n\u2502 - Deploy to virtual-smoker-device\u2502\n\u2502 - Device-service, Smoker, Shell \u2502\n\u2502 - Health checks                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 notify-discord                  \u2502\n\u2502 - Deployment status             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#production-pipeline-manual-trigger","title":"Production Pipeline (Manual Trigger)","text":"<pre><code>Manual Production Release\n    \u2193\nprod-deploy.yml (workflow_dispatch)\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 validate-dev-environment        \u2502\n\u2502 - Check dev-cloud health        \u2502\n\u2502 - Verify dev deployment stable  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 build-production-images         \u2502\n\u2502 - Build from dev state          \u2502\n\u2502 - Tag with version (vX.Y.Z)     \u2502\n\u2502 - Push to Docker Hub            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 deploy-prod-cloud               \u2502\n\u2502 (cloud-deploy.yml with version) \u2502\n\u2502 - Deploy to prod-cloud LXC      \u2502\n\u2502 - Health checks                 \u2502\n\u2502 - Tailscale funnel setup        \u2502\n\u2502 - Rollback on failure           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 deploy-production-pi            \u2502\n\u2502 (smoker-deploy-prod.yml)        \u2502\n\u2502 - Deploy to actual Raspberry Pi \u2502\n\u2502 - Health checks                 \u2502\n\u2502 - Watchtower will auto-update   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 notify-discord                  \u2502\n\u2502 - Production deployment status   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#github-actions-workflows","title":"GitHub Actions Workflows","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#development-workflow","title":"Development Workflow","text":"<pre><code># .github/workflows/development.yml\nname: Development CI/CD\n\non:\n  push:\n    branches: [ master ]\n  pull_request:\n    branches: [ master ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '18'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run linting\n        run: npm run lint\n\n      - name: Run tests\n        run: npm run test:ci\n\n      - name: Build applications\n        run: npm run build\n\n  build-images:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/master'\n\n    strategy:\n      matrix:\n        service: [backend, frontend, device-service, smoker]\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      - name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      - name: Build and push development image\n        uses: docker/build-push-action@v5\n        with:\n          context: ./apps/${{ matrix.service }}\n          push: true\n          tags: |\n            benjr70/smart-smoker-${{ matrix.service }}:dev-latest\n            benjr70/smart-smoker-${{ matrix.service }}:dev-${{ github.sha }}\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n\n  deploy-dev:\n    needs: [test, build-images]\n    runs-on: self-hosted\n    if: github.ref == 'refs/heads/master'\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Deploy to development environment\n        run: |\n          # Connect to dev environment via Tailscale\n          export DEV_HOST=\"smoker-dev-cloud\"\n\n          # Update docker-compose with new images\n          sed -i 's/:latest/:dev-latest/g' cloud.docker-compose.yml\n\n          # Deploy to development environment\n          scp cloud.docker-compose.yml root@${DEV_HOST}:/opt/smart-smoker/\n          ssh root@${DEV_HOST} \"cd /opt/smart-smoker &amp;&amp; docker-compose pull &amp;&amp; docker-compose up -d\"\n\n          # Wait for services to be healthy\n          sleep 30\n\n          # Run health checks\n          ./scripts/health-check.sh ${DEV_HOST}\n\n      - name: Run integration tests\n        run: |\n          export VIRTUAL_SMOKER_HOST=\"virtual-smoker-device\"\n          ./scripts/run-integration-tests.sh ${VIRTUAL_SMOKER_HOST}\n\n      - name: Notify deployment status\n        uses: 8398a7/action-slack@v3\n        with:\n          status: ${{ job.status }}\n          channel: '#deployments'\n          webhook_url: ${{ secrets.SLACK_WEBHOOK }}\n        if: always()\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#production-workflow","title":"Production Workflow","text":"<pre><code># .github/workflows/production.yml\nname: Production Deployment\n\non:\n  workflow_dispatch:\n    inputs:\n      version:\n        description: 'Version to deploy'\n        required: true\n        type: string\n      environment:\n        description: 'Target environment'\n        required: true\n        default: 'production'\n        type: choice\n        options:\n        - production\n        - staging\n\njobs:\n  approval:\n    runs-on: ubuntu-latest\n    environment: \n      name: ${{ github.event.inputs.environment }}\n      url: https://smokecloud.tail74646.ts.net\n    steps:\n      - name: Manual approval checkpoint\n        run: echo \"Deployment approved for ${{ github.event.inputs.environment }}\"\n\n  build-production:\n    needs: approval\n    runs-on: self-hosted\n\n    strategy:\n      matrix:\n        service: [backend, frontend, device-service, smoker]\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.inputs.version }}\n\n      - name: Build production images\n        run: |\n          cd apps/${{ matrix.service }}\n          docker build -t benjr70/smart-smoker-${{ matrix.service }}:${{ github.event.inputs.version }} .\n          docker build -t benjr70/smart-smoker-${{ matrix.service }}:latest .\n\n      - name: Push production images\n        run: |\n          docker push benjr70/smart-smoker-${{ matrix.service }}:${{ github.event.inputs.version }}\n          docker push benjr70/smart-smoker-${{ matrix.service }}:latest\n\n  deploy-cloud:\n    needs: build-production\n    runs-on: self-hosted\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.inputs.version }}\n\n      - name: Backup current deployment\n        run: |\n          export PROD_HOST=\"smokecloud\"\n          ssh root@${PROD_HOST} \"cd /opt/smart-smoker &amp;&amp; cp cloud.docker-compose.yml cloud.docker-compose.yml.backup\"\n          ssh root@${PROD_HOST} \"cd /opt/smart-smoker &amp;&amp; docker-compose ps &gt; deployment.backup\"\n\n      - name: Deploy to production cloud\n        run: |\n          export PROD_HOST=\"smokecloud\"\n\n          # Update compose file with new version\n          sed -i \"s/:latest/:${{ github.event.inputs.version }}/g\" cloud.docker-compose.yml\n\n          # Deploy new version\n          scp cloud.docker-compose.yml root@${PROD_HOST}:/opt/smart-smoker/\n          ssh root@${PROD_HOST} \"cd /opt/smart-smoker &amp;&amp; docker-compose pull\"\n          ssh root@${PROD_HOST} \"cd /opt/smart-smoker &amp;&amp; docker-compose up -d\"\n\n          # Wait for deployment\n          sleep 60\n\n      - name: Verify deployment health\n        run: |\n          export PROD_HOST=\"smokecloud\"\n\n          # Run comprehensive health checks\n          ./scripts/health-check.sh ${PROD_HOST}\n          ./scripts/performance-test.sh ${PROD_HOST}\n\n          # Check Tailscale funnel status\n          ssh root@${PROD_HOST} \"tailscale funnel status\"\n\n      - name: Update Raspberry Pi devices\n        run: |\n          # Trigger Watchtower updates on all Pi devices\n          ./scripts/update-raspberry-pi-devices.sh ${{ github.event.inputs.version }}\n\n      - name: Tag successful deployment\n        run: |\n          git tag -a \"prod-${{ github.event.inputs.version }}-$(date +%Y%m%d-%H%M%S)\" -m \"Production deployment ${{ github.event.inputs.version }}\"\n          git push origin --tags\n\n  rollback:\n    needs: deploy-cloud\n    runs-on: self-hosted\n    if: failure()\n\n    steps:\n      - name: Rollback deployment\n        run: |\n          export PROD_HOST=\"smokecloud\"\n\n          echo \"Deployment failed, initiating rollback...\"\n\n          # Restore previous deployment\n          ssh root@${PROD_HOST} \"cd /opt/smart-smoker &amp;&amp; cp cloud.docker-compose.yml.backup cloud.docker-compose.yml\"\n          ssh root@${PROD_HOST} \"cd /opt/smart-smoker &amp;&amp; docker-compose up -d\"\n\n          # Verify rollback\n          sleep 30\n          ./scripts/health-check.sh ${PROD_HOST}\n\n      - name: Notify rollback\n        uses: 8398a7/action-slack@v3\n        with:\n          status: custom\n          custom_payload: |\n            {\n              text: \"\ud83d\udea8 Production deployment failed and was rolled back\",\n              channel: \"#alerts\",\n              username: \"Deployment Bot\",\n              icon_emoji: \":warning:\"\n            }\n          webhook_url: ${{ secrets.SLACK_WEBHOOK }}\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#raspberry-pi-update-workflow","title":"Raspberry Pi Update Workflow","text":"<pre><code># .github/workflows/raspberry-pi-update.yml\nname: Raspberry Pi Device Updates\n\non:\n  workflow_dispatch:\n    inputs:\n      target_devices:\n        description: 'Target devices (all, production, development)'\n        required: true\n        default: 'production'\n        type: choice\n        options:\n        - all\n        - production\n        - development\n      update_strategy:\n        description: 'Update strategy'\n        required: true\n        default: 'rolling'\n        type: choice\n        options:\n        - rolling\n        - immediate\n        - scheduled\n\njobs:\n  update-devices:\n    runs-on: self-hosted\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Discover Raspberry Pi devices\n        run: |\n          # Use Tailscale to discover Pi devices\n          tailscale status --json | jq -r '.Peer[] | select(.HostName | contains(\"smoker-pi\")) | .HostName' &gt; pi-devices.txt\n\n          # Filter based on target\n          case \"${{ github.event.inputs.target_devices }}\" in\n            \"production\")\n              grep \"smoker-pi-prod\" pi-devices.txt &gt; target-devices.txt || true\n              ;;\n            \"development\") \n              grep \"smoker-pi-dev\" pi-devices.txt &gt; target-devices.txt || true\n              ;;\n            \"all\")\n              cp pi-devices.txt target-devices.txt\n              ;;\n          esac\n\n          echo \"Target devices:\"\n          cat target-devices.txt\n\n      - name: Update devices\n        run: |\n          update_strategy=\"${{ github.event.inputs.update_strategy }}\"\n\n          while IFS= read -r device; do\n            echo \"Updating device: $device\"\n\n            case \"$update_strategy\" in\n              \"rolling\")\n                # Update one device at a time with health checks\n                ./scripts/update-single-device.sh \"$device\"\n                ./scripts/health-check-device.sh \"$device\"\n                sleep 120  # Wait between devices\n                ;;\n              \"immediate\")\n                # Update all devices simultaneously\n                ./scripts/update-single-device.sh \"$device\" &amp;\n                ;;\n              \"scheduled\")\n                # Schedule update for maintenance window\n                ssh pi@$device \"echo 'watchtower --run-once --cleanup' | at 02:00\"\n                ;;\n            esac\n\n          done &lt; target-devices.txt\n\n          # Wait for immediate updates to complete\n          if [ \"$update_strategy\" = \"immediate\" ]; then\n            wait\n          fi\n\n      - name: Verify device updates\n        run: |\n          failed_devices=\"\"\n\n          while IFS= read -r device; do\n            echo \"Verifying device: $device\"\n\n            if ! ./scripts/health-check-device.sh \"$device\"; then\n              failed_devices=\"$failed_devices $device\"\n              echo \"\u274c Device $device failed health check\"\n            else\n              echo \"\u2705 Device $device updated successfully\"\n            fi\n\n          done &lt; target-devices.txt\n\n          if [ -n \"$failed_devices\" ]; then\n            echo \"Failed devices: $failed_devices\"\n            exit 1\n          fi\n\n      - name: Update monitoring dashboard\n        run: |\n          # Update device status in monitoring system\n          ./scripts/update-device-dashboard.sh\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#deployment-scripts","title":"Deployment Scripts","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#health-check-script","title":"Health Check Script","text":"<pre><code>#!/bin/bash\n# scripts/health-check.sh\n\nHOST=$1\nif [ -z \"$HOST\" ]; then\n    echo \"Usage: $0 &lt;hostname&gt;\"\n    exit 1\nfi\n\necho \"Running health checks for $HOST...\"\n\n# Check if services are running\ncheck_service() {\n    local service=$1\n    local port=$2\n    local path=${3:-\"/\"}\n\n    echo \"Checking $service on port $port...\"\n\n    if curl -f -s --max-time 10 \"http://${HOST}:${port}${path}\" &gt; /dev/null; then\n        echo \"\u2705 $service is healthy\"\n        return 0\n    else\n        echo \"\u274c $service is unhealthy\"\n        return 1\n    fi\n}\n\n# Service health checks\nHEALTH_CHECKS=(\n    \"backend:3001:/health\"\n    \"frontend:80:/\"\n    \"device-service:3002:/health\"\n)\n\nfailed_checks=0\n\nfor check in \"${HEALTH_CHECKS[@]}\"; do\n    IFS=':' read -r service port path &lt;&lt;&lt; \"$check\"\n    if ! check_service \"$service\" \"$port\" \"$path\"; then\n        failed_checks=$((failed_checks + 1))\n    fi\n    sleep 2\ndone\n\n# Check Docker containers\necho \"Checking Docker container status...\"\ncontainer_status=$(ssh root@${HOST} \"docker-compose ps --services --filter 'status=running'\" 2&gt;/dev/null | wc -l)\nexpected_containers=4  # backend, frontend, device-service, database\n\nif [ \"$container_status\" -ge \"$expected_containers\" ]; then\n    echo \"\u2705 All containers are running\"\nelse\n    echo \"\u274c Some containers are not running (expected: $expected_containers, running: $container_status)\"\n    failed_checks=$((failed_checks + 1))\nfi\n\n# Check system resources\necho \"Checking system resources...\"\nmemory_usage=$(ssh root@${HOST} \"free | grep Mem | awk '{printf \\\"%.1f\\\", \\$3/\\$2 * 100.0}'\")\ndisk_usage=$(ssh root@${HOST} \"df / | tail -1 | awk '{print \\$5}' | sed 's/%//'\")\n\nif (( $(echo \"$memory_usage &lt; 90\" | bc -l) )); then\n    echo \"\u2705 Memory usage: ${memory_usage}%\"\nelse\n    echo \"\u26a0\ufe0f  High memory usage: ${memory_usage}%\"\nfi\n\nif [ \"$disk_usage\" -lt 90 ]; then\n    echo \"\u2705 Disk usage: ${disk_usage}%\"\nelse\n    echo \"\u26a0\ufe0f  High disk usage: ${disk_usage}%\"\nfi\n\n# Final result\nif [ $failed_checks -eq 0 ]; then\n    echo \"\ud83c\udf89 All health checks passed for $HOST\"\n    exit 0\nelse\n    echo \"\ud83d\udca5 $failed_checks health check(s) failed for $HOST\"\n    exit 1\nfi\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#raspberry-pi-device-update-script","title":"Raspberry Pi Device Update Script","text":"<pre><code>#!/bin/bash\n# scripts/update-single-device.sh\n\nDEVICE=$1\nif [ -z \"$DEVICE\" ]; then\n    echo \"Usage: $0 &lt;device-hostname&gt;\"\n    exit 1\nfi\n\necho \"Updating Raspberry Pi device: $DEVICE\"\n\n# Pre-update health check\necho \"Running pre-update health check...\"\nif ! ./scripts/health-check-device.sh \"$DEVICE\"; then\n    echo \"\u26a0\ufe0f  Device $DEVICE is not healthy before update, proceeding anyway...\"\nfi\n\n# Backup current state\necho \"Creating device backup...\"\nssh pi@$DEVICE \"docker-compose ps &gt; /tmp/pre-update-status.txt\"\nssh pi@$DEVICE \"sudo systemctl is-active --quiet watchtower &amp;&amp; echo 'watchtower active' || echo 'watchtower inactive'\" &gt; /tmp/watchtower-status.txt\n\n# Update container images\necho \"Triggering container updates...\"\nssh pi@$DEVICE \"docker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once --cleanup\"\n\n# Wait for updates to complete\necho \"Waiting for updates to complete...\"\nsleep 60\n\n# Verify services restarted properly\necho \"Verifying service restart...\"\nmax_attempts=10\nattempt=0\n\nwhile [ $attempt -lt $max_attempts ]; do\n    if ssh pi@$DEVICE \"docker-compose ps | grep -q 'Up'\"; then\n        echo \"\u2705 Services are running\"\n        break\n    else\n        echo \"\u23f3 Waiting for services to start (attempt $((attempt + 1))/$max_attempts)...\"\n        sleep 30\n        attempt=$((attempt + 1))\n    fi\ndone\n\nif [ $attempt -eq $max_attempts ]; then\n    echo \"\u274c Services failed to start after update\"\n\n    # Attempt recovery\n    echo \"Attempting service recovery...\"\n    ssh pi@$DEVICE \"cd /opt/smart-smoker &amp;&amp; docker-compose down &amp;&amp; docker-compose up -d\"\n    sleep 30\n\n    if ! ssh pi@$DEVICE \"docker-compose ps | grep -q 'Up'\"; then\n        echo \"\ud83d\udca5 Recovery failed for device $DEVICE\"\n        exit 1\n    fi\nfi\n\n# Post-update verification\necho \"Running post-update health check...\"\nif ./scripts/health-check-device.sh \"$DEVICE\"; then\n    echo \"\ud83c\udf89 Device $DEVICE updated successfully\"\n\n    # Log successful update\n    echo \"$(date): Successfully updated $DEVICE\" &gt;&gt; /tmp/device-updates.log\nelse\n    echo \"\ud83d\udca5 Device $DEVICE failed post-update health check\"\n    exit 1\nfi\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#integration-test-script","title":"Integration Test Script","text":"<pre><code>#!/bin/bash\n# scripts/run-integration-tests.sh\n\nVIRTUAL_DEVICE=$1\nif [ -z \"$VIRTUAL_DEVICE\" ]; then\n    echo \"Usage: $0 &lt;virtual-device-hostname&gt;\"\n    exit 1\nfi\n\necho \"Running integration tests on virtual device: $VIRTUAL_DEVICE\"\n\n# Start test suite\necho \"Starting integration test suite...\"\n\n# Test 1: Device Service Connectivity\necho \"Test 1: Device Service Connectivity\"\nif curl -f -s --max-time 10 \"http://${VIRTUAL_DEVICE}:3002/health\" &gt; /dev/null; then\n    echo \"\u2705 Device service is reachable\"\nelse\n    echo \"\u274c Device service connectivity test failed\"\n    exit 1\nfi\n\n# Test 2: Mock Hardware Integration\necho \"Test 2: Mock Hardware Integration\"\ntemp_response=$(curl -s \"http://${VIRTUAL_DEVICE}:5000/temperature/28-000000000001\")\nif echo \"$temp_response\" | jq -e '.temperature' &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u2705 Mock temperature sensor responding\"\nelse\n    echo \"\u274c Mock hardware integration test failed\"\n    exit 1\nfi\n\n# Test 3: WebSocket Communication\necho \"Test 3: WebSocket Communication\"\n# Use a simple Node.js script to test WebSocket\ncat &gt; /tmp/websocket-test.js &lt;&lt; 'EOF'\nconst WebSocket = require('ws');\n\nconst ws = new WebSocket('ws://VIRTUAL_DEVICE:8765');\n\nws.on('open', function open() {\n    console.log('WebSocket connection established');\n    ws.send(JSON.stringify({ type: 'test', message: 'integration test' }));\n});\n\nws.on('message', function message(data) {\n    const response = JSON.parse(data);\n    if (response.type === 'command_ack') {\n        console.log('\u2705 WebSocket communication test passed');\n        process.exit(0);\n    }\n});\n\nws.on('error', function error(err) {\n    console.log('\u274c WebSocket test failed:', err.message);\n    process.exit(1);\n});\n\nsetTimeout(() =&gt; {\n    console.log('\u274c WebSocket test timeout');\n    process.exit(1);\n}, 10000);\nEOF\n\n# Replace placeholder and run test\nsed -i \"s/VIRTUAL_DEVICE/${VIRTUAL_DEVICE}/g\" /tmp/websocket-test.js\nif ssh pi@$VIRTUAL_DEVICE \"cd /tmp &amp;&amp; node /tmp/websocket-test.js\"; then\n    echo \"\u2705 WebSocket communication test passed\"\nelse\n    echo \"\u274c WebSocket communication test failed\"\n    exit 1\nfi\n\n# Test 4: End-to-End Smoke Test\necho \"Test 4: End-to-End Smoke Test\"\n# Set target temperature and verify response\ncurl -X POST \"http://${VIRTUAL_DEVICE}:5000/temperature/28-000000000001/target/250\"\nsleep 5\nnew_temp=$(curl -s \"http://${VIRTUAL_DEVICE}:5000/temperature/28-000000000001\" | jq -r '.temperature')\n\nif (( $(echo \"$new_temp &gt; 230\" | bc -l) )); then\n    echo \"\u2705 End-to-end smoke test passed (temp: $new_temp)\"\nelse\n    echo \"\u274c End-to-end smoke test failed (temp: $new_temp)\"\n    exit 1\nfi\n\necho \"\ud83c\udf89 All integration tests passed on $VIRTUAL_DEVICE\"\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#monitoring-alerting","title":"Monitoring &amp; Alerting","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#deployment-monitoring-dashboard","title":"Deployment Monitoring Dashboard","text":"<pre><code># monitoring/deployment-dashboard.yml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: deployment-dashboard\ndata:\n  dashboard.json: |\n    {\n      \"dashboard\": {\n        \"title\": \"Smart Smoker Deployments\",\n        \"panels\": [\n          {\n            \"title\": \"Deployment Status\",\n            \"type\": \"stat\",\n            \"targets\": [\n              {\n                \"expr\": \"github_actions_workflow_runs_total{repository=\\\"Smart-Smoker-V2\\\",workflow=\\\"Production Deployment\\\",conclusion=\\\"success\\\"}\",\n                \"legendFormat\": \"Successful Deployments\"\n              }\n            ]\n          },\n          {\n            \"title\": \"Environment Health\",\n            \"type\": \"table\",\n            \"targets\": [\n              {\n                \"expr\": \"up{job=\\\"smart-smoker-health\\\"}\",\n                \"legendFormat\": \"{{instance}}\"\n              }\n            ]\n          },\n          {\n            \"title\": \"Raspberry Pi Devices\",\n            \"type\": \"table\", \n            \"targets\": [\n              {\n                \"expr\": \"tailscale_device_status{device_type=\\\"raspberry-pi\\\"}\",\n                \"legendFormat\": \"{{hostname}}\"\n              }\n            ]\n          }\n        ]\n      }\n    }\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#slack-notification-templates","title":"Slack Notification Templates","text":"<pre><code># monitoring/slack-notifications.yml\ndeployment_success: |\n  {\n    \"channel\": \"#deployments\",\n    \"username\": \"Deployment Bot\",\n    \"icon_emoji\": \":rocket:\",\n    \"attachments\": [\n      {\n        \"color\": \"good\",\n        \"title\": \"\ud83d\ude80 Deployment Successful\",\n        \"fields\": [\n          {\n            \"title\": \"Environment\",\n            \"value\": \"{{ .environment }}\",\n            \"short\": true\n          },\n          {\n            \"title\": \"Version\", \n            \"value\": \"{{ .version }}\",\n            \"short\": true\n          },\n          {\n            \"title\": \"Duration\",\n            \"value\": \"{{ .duration }}\",\n            \"short\": true\n          },\n          {\n            \"title\": \"Services\",\n            \"value\": \"{{ .services }}\",\n            \"short\": true\n          }\n        ],\n        \"actions\": [\n          {\n            \"type\": \"button\",\n            \"text\": \"View Logs\",\n            \"url\": \"{{ .logs_url }}\"\n          },\n          {\n            \"type\": \"button\", \n            \"text\": \"Environment\",\n            \"url\": \"{{ .environment_url }}\"\n          }\n        ]\n      }\n    ]\n  }\n\ndeployment_failure: |\n  {\n    \"channel\": \"#alerts\",\n    \"username\": \"Deployment Bot\", \n    \"icon_emoji\": \":warning:\",\n    \"attachments\": [\n      {\n        \"color\": \"danger\",\n        \"title\": \"\ud83d\udca5 Deployment Failed\",\n        \"fields\": [\n          {\n            \"title\": \"Environment\",\n            \"value\": \"{{ .environment }}\",\n            \"short\": true\n          },\n          {\n            \"title\": \"Version\",\n            \"value\": \"{{ .version }}\",\n            \"short\": true\n          },\n          {\n            \"title\": \"Failed Step\",\n            \"value\": \"{{ .failed_step }}\",\n            \"short\": true\n          },\n          {\n            \"title\": \"Error\",\n            \"value\": \"{{ .error_message }}\",\n            \"short\": false\n          }\n        ],\n        \"actions\": [\n          {\n            \"type\": \"button\",\n            \"text\": \"View Logs\",\n            \"url\": \"{{ .logs_url }}\"\n          },\n          {\n            \"type\": \"button\",\n            \"text\": \"Rollback\",\n            \"url\": \"{{ .rollback_url }}\"\n          }\n        ]\n      }\n    ]\n  }\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#security-compliance","title":"Security &amp; Compliance","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#deployment-security-checklist","title":"Deployment Security Checklist","text":"<ul> <li>[ ] Image Scanning: All container images scanned for vulnerabilities</li> <li>[ ] Secret Management: Secrets stored in GitHub Secrets, not in code</li> <li>[ ] Access Control: Deployment approvals required for production</li> <li>[ ] Audit Logging: All deployment activities logged and monitored</li> <li>[ ] Network Security: Tailscale provides encrypted communication</li> <li>[ ] Backup Verification: Automated backups before each deployment</li> <li>[ ] Rollback Testing: Rollback procedures tested regularly</li> </ul>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#compliance-requirements","title":"Compliance Requirements","text":"<pre><code># security/compliance-checks.yml\nsecurity_scans:\n  - name: \"Container Image Vulnerability Scan\"\n    tool: \"Trivy\"\n    threshold: \"HIGH\"\n    action: \"fail\"\n\n  - name: \"Secret Detection\"\n    tool: \"GitLeaks\"\n    scope: \"all_files\"\n    action: \"fail\"\n\n  - name: \"License Compliance\"\n    tool: \"FOSSA\"\n    scope: \"dependencies\"\n    action: \"warn\"\n\naudit_requirements:\n  - deployment_logs: \"retained_90_days\"\n  - access_logs: \"retained_365_days\"\n  - security_events: \"retained_2_years\"\n  - compliance_reports: \"monthly\"\n</code></pre>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#testing-strategy","title":"Testing Strategy","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#workflow-testing","title":"Workflow Testing","text":"<ul> <li>Unit Tests: Individual script validation</li> <li>Integration Tests: Full pipeline execution</li> <li>Security Tests: Vulnerability and compliance scans</li> <li>Performance Tests: Deployment speed and reliability</li> <li>Rollback Tests: Failure scenario validation</li> </ul>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#validation-criteria","title":"Validation Criteria","text":"<ul> <li>Development deployment completes in &lt; 10 minutes</li> <li>Production deployment completes in &lt; 30 minutes</li> <li>Zero-downtime deployments achieved</li> <li>Rollback completes in &lt; 5 minutes</li> <li>99.9% deployment success rate</li> </ul>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#risk-assessment_1","title":"Risk Assessment","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#critical-risks-must-address-before-production","title":"Critical Risks (Must Address Before Production)","text":"Risk Impact Probability Current Status Mitigation MongoDB security vulnerability Critical High \u2705 MITIGATED Story 0 - \u2705 Upgraded to 7.x and enabled authentication Data loss (no backups) Critical Medium \u2705 MITIGATED Story 0 - \u2705 Automated backups implemented and tested Failed deployment (no rollback) High Medium \u2705 MITIGATED Story 0 - \u2705 Automated health checks and rollback implemented Database migration failure High Medium PLANNED Comprehensive migration plan with rollback (Story 3)"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#operational-risks-ongoing-management","title":"Operational Risks (Ongoing Management)","text":"Risk Impact Mitigation Self-hosted runner failure High Multiple runner instances, fallback to GitHub hosted Tailscale connectivity issues Medium VPN backup, local network fallback Docker registry unavailability High Multiple registry mirrors, local caching Raspberry Pi network issues Medium Batch updates, device health monitoring Proxmox server hardware failure Critical Regular backups, UPS, Pi as emergency fallback"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#risk-adjusted-implementation-order","title":"Risk-Adjusted Implementation Order","text":"<p>Based on risk assessment and CI/CD vision, Phase 3 implementation order is:</p> <p>\u2705 Week 1-2: Critical Security &amp; Reliability (Story 0) - COMPLETE 1. \u2705 MongoDB upgrade to 7.x with authentication (dev environment first) 2. \u2705 Automated backup implementation and validation 3. \u2705 Deployment health checks and rollback automation 4. \u2705 Test all fixes in dev environment</p> <p>Week 3-4: Development Pipeline (Stories 1, 2, 3) 5. Story 1: Automated development deployment to dev-cloud (uses existing nightly.yml) 6. Story 2: Virtual smoker device infrastructure setup (must be done before Story 3) 7. Story 3: Virtual smoker device deployment</p> <p>Week 5: Production Infrastructure (Story 5) 8. Story 5: Production database migration (can happen in parallel with dev pipeline)</p> <p>Week 6+: Production Automation (Stories 4, 6) 9. Story 4: Production deployment workflow (with Tailscale funnel) 10. Story 6: Watchtower integration for production Pi</p> <p>Future: Testing Enhancement (Story 7) 11. Story 7: E2E testing framework (DEFERRED - add after core workflows stable)</p> <p>Rationale:  - Stories 1-3 build the dev pipeline incrementally - Story 2 is critical infrastructure that must be done before Story 3 - Story 5 can happen in parallel as it's infrastructure prep - Story 4 depends on dev pipeline being stable - Story 6 is the final piece for production Pi automation - Story 7 is deferred to focus on getting core workflows working first</p> <p>\u2705 Story 0 Status Update:  - Implementation: \u2705 Complete - Dev-Cloud Testing: \u2705 Complete - All infrastructure fixes validated - Production Deployment: \u23f3 Pending (manual deployment acceptable for Story 0) - Story 0 Status: \u2705 COMPLETE - Ready for production deployment</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#success-metrics","title":"Success Metrics","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#critical-success-criteria-story-0-achieved","title":"Critical Success Criteria (Story 0 - \u2705 ACHIEVED)","text":"<ul> <li>MongoDB Security: \u2705 Authentication enabled, version 7.x stable, zero vulnerabilities</li> <li>Backup Reliability: \u2705 100% backup success rate, restore tested and validated</li> <li>Deployment Safety: \u2705 Automated rollback working, &lt; 5 minute rollback time</li> <li>Zero Data Loss: \u2705 All migrations and upgrades complete without data loss</li> <li>Dev-Cloud Testing: \u2705 All infrastructure fixes validated in dev environment</li> </ul>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#quantitative-metrics-post-story-0","title":"Quantitative Metrics (Post-Story 0)","text":"<ul> <li>&lt; 10 minutes for development deployments</li> <li>&lt; 30 minutes for production deployments (single-user context, not optimizing for zero-downtime)</li> <li>99% deployment success rate (more realistic for single-developer project)</li> <li>&lt; 5 minutes mean time to rollback</li> <li>100% infrastructure provisioned via automation</li> </ul>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#qualitative-metrics","title":"Qualitative Metrics","text":"<ul> <li>Security vulnerabilities addressed before production use</li> <li>Confidence in backup and recovery procedures</li> <li>Reduced manual deployment steps (not necessarily zero - manual approval gates are valuable)</li> <li>Improved development velocity</li> <li>Clear documentation and runbooks for all procedures</li> </ul>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#deliverables","title":"Deliverables","text":""},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#phase-3-outputs-revised-priority-order","title":"Phase 3 Outputs (Revised Priority Order)","text":"<p>Critical Deliverables (Week 1-3) - \u2705 COMPLETE: - [x] MongoDB upgrade to 7.x with authentication - [x] Automated backup system with validation - [x] Deployment health checks and automated rollback - [x] Documented and tested restore procedures - [x] Security hardening for production readiness - [x] Dev-cloud testing completed and validated</p> <p>High Priority Deliverables (Week 4-6): - [ ] Production database migration from Pi to Proxmox - [ ] Migration validation and monitoring - [ ] Updated deployment workflows with security measures - [ ] Comprehensive infrastructure documentation</p> <p>Standard Deliverables (Week 7+): - [ ] Automated development deployment workflows - [ ] Production deployment automation with approval gates - [ ] Raspberry Pi device management automation - [ ] Integration testing framework - [ ] Virtual device testing automation - [ ] Deployment monitoring dashboard - [ ] Security scanning and compliance validation</p> <p>Deferred/Optional Deliverables: - [ ] Advanced deployment strategies (blue-green, canary) - not needed for single-user - [ ] Multi-region deployment - not needed for local Proxmox - [ ] High availability configurations - overkill for current scale</p>"},{"location":"Infrastructure/implementation/phase-3-deployment-automation/#handoff-to-phase-4","title":"Handoff to Phase 4","text":"<ul> <li>All deployment workflows functional and tested</li> <li>Monitoring and alerting systems operational</li> <li>Security and compliance measures implemented</li> <li>Team trained on deployment procedures</li> <li>Documentation complete and accessible</li> </ul> <p>Phase Owner: DevOps Team Status: Ready for Implementation Dependencies: Phase 2 completion Risk Level: Medium</p>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/","title":"Smart Smoker V2 - Proxmox Infrastructure as Code Plan","text":""},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#executive-summary","title":"Executive Summary","text":"<p>This document outlines the comprehensive plan to implement Infrastructure as Code (IaC) for the Smart Smoker V2 project using Terraform on a local Proxmox server. The initiative will enable automated deployment to development environments and provide manual deployment capabilities for production while maintaining the existing Raspberry Pi smoker device deployment strategy and current Tailscale networking.</p> <p>Architectural Review Update (2025-10-14): Following a comprehensive architectural review, this plan has been updated to include Architecture Decision Records (ADRs), risk assessments, and adjusted implementation priorities. Critical security and reliability issues have been identified and prioritized for immediate remediation in Phase 3.</p>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#project-goals","title":"Project Goals","text":""},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#primary-objectives","title":"Primary Objectives","text":"<ul> <li>Infrastructure as Code: Implement Terraform to manage VM/LXC provisioning on Proxmox</li> <li>Automated Dev Deployment: Auto-deploy to development environment on master branch merges</li> <li>Manual Production Control: Provide controlled manual deployment to production environments</li> <li>Tailscale Integration: Automate Tailscale funnel configuration for public API exposure</li> <li>Enhanced Testing: Create virtual smoker device for complete application testing</li> <li>Container Standardization: Improve Docker image naming for Watchtower compatibility</li> <li>Private Server Support: Enable GitHub Actions deployment to local infrastructure</li> </ul>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#success-metrics","title":"Success Metrics","text":"<ul> <li>\u2705 100% of infrastructure defined in Terraform code</li> <li>\u2705 Development environment auto-deploys within 5 minutes of master merge</li> <li>\u2705 Production cloud environment accessible via Tailscale funnel (https://smokecloud.tail74646.ts.net)</li> <li>\u2705 Virtual smoker device provides full GUI testing capability via VNC</li> <li>\u2705 Zero manual infrastructure provisioning for development</li> <li>\u2705 Raspberry Pi continues auto-updating via Watchtower with improved container naming</li> </ul>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#current-state-analysis","title":"Current State Analysis","text":""},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#strengths","title":"Strengths","text":"<ul> <li>\u2705 Robust CI/CD pipeline with comprehensive testing</li> <li>\u2705 Containerized applications with Docker Compose</li> <li>\u2705 Dual deployment strategy (Cloud + Raspberry Pi)</li> <li>\u2705 Tailscale networking with SSL and public funnel access</li> <li>\u2705 Automatic updates via Watchtower on Pi</li> <li>\u2705 Well-structured monorepo with 4 applications</li> </ul>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#current-tailscale-configuration","title":"Current Tailscale Configuration","text":"<ul> <li>Frontend: https://smokecloud.tail74646.ts.net \u2192 http://127.0.0.1:80</li> <li>Backend: https://smokecloud.tail74646.ts.net:8443 \u2192 http://127.0.0.1:3001</li> <li>Portainer: smokerCloudIp:10000 (internal access)</li> </ul>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#gaps","title":"Gaps","text":"<ul> <li>\u274c Manual infrastructure provisioning</li> <li>\u274c Manual Tailscale configuration during deployments</li> <li>\u274c No development environment automation</li> <li>\u274c Container naming incompatible with Watchtower best practices</li> <li>\u274c Limited ability to test smoker hardware interactions</li> <li>\u274c GitHub Actions cannot reach private Proxmox server</li> </ul>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#critical-issues-architectural-review-findings","title":"Critical Issues (Architectural Review Findings)","text":"<ul> <li>\u274c MongoDB Security: Running 4.4.14-rc0 (release candidate) without authentication</li> <li>\u274c Backup System: No automated backups or restore validation</li> <li>\u274c Deployment Safety: No automated health checks or rollback mechanisms</li> <li>\u274c State Management: Local Terraform state with no locking (acceptable for single-user)</li> <li>\u274c Single Point of Failure: All infrastructure on one Proxmox server (acceptable trade-off)</li> </ul>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#target-architecture","title":"Target Architecture","text":""},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#infrastructure-layout","title":"Infrastructure Layout","text":"<pre><code>Proxmox Server\n\u251c\u2500\u2500 github-runner (LXC Container)\n\u2502   \u251c\u2500\u2500 Self-hosted GitHub Actions runner\n\u2502   \u251c\u2500\u2500 Terraform with Proxmox provider\n\u2502   \u251c\u2500\u2500 Docker CLI for deployment\n\u2502   \u251c\u2500\u2500 Tailscale client for network access\n\u2502   \u2514\u2500\u2500 Node.js/npm for builds\n\u2502\n\u251c\u2500\u2500 smart-smoker-dev-cloud (LXC Container)\n\u2502   \u251c\u2500\u2500 Auto-deployed on master merge\n\u2502   \u251c\u2500\u2500 Backend + Frontend + MongoDB\n\u2502   \u251c\u2500\u2500 Environment variables injection\n\u2502   \u251c\u2500\u2500 Health monitoring\n\u2502   \u2514\u2500\u2500 Internal Tailscale access (dev.smokecloud.tail74646.ts.net)\n\u2502\n\u251c\u2500\u2500 smart-smoker-cloud-prod (LXC Container)\n\u2502   \u251c\u2500\u2500 Manual deployment trigger\n\u2502   \u251c\u2500\u2500 Backend + Frontend + MongoDB\n\u2502   \u251c\u2500\u2500 Tailscale client with funnel configuration\n\u2502   \u251c\u2500\u2500 Production SSL certificates via Tailscale\n\u2502   \u251c\u2500\u2500 Public access: https://smokecloud.tail74646.ts.net\n\u2502   \u251c\u2500\u2500 Backend API: https://smokecloud.tail74646.ts.net:8443\n\u2502   \u251c\u2500\u2500 Portainer: Internal access on port 10000\n\u2502   \u2514\u2500\u2500 Automated deployment workflow with Tailscale restart\n\u2502\n\u2514\u2500\u2500 smart-smoker-dev-smoker (VM - ARM64)\n    \u251c\u2500\u2500 Raspberry Pi OS with desktop\n    \u251c\u2500\u2500 VNC server for GUI access\n    \u251c\u2500\u2500 Mock hardware devices (/dev/ttyUSB0, audio, etc.)\n    \u251c\u2500\u2500 Device Service + Smoker UI + Electron Shell\n    \u251c\u2500\u2500 Internal Tailscale network access\n    \u2514\u2500\u2500 Complete smoker simulation environment\n</code></pre>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#implementation-phases","title":"Implementation Phases","text":""},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#phase-1-container-standardization","title":"Phase 1: Container Standardization","text":"<p>Duration: 1-2 weeks Focus: Update Docker image naming and publishing workflows Status: Completed</p>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#phase-2-proxmox-infrastructure-setup","title":"Phase 2: Proxmox Infrastructure Setup","text":"<p>Duration: 2-3 weeks Focus: Terraform infrastructure and GitHub runner setup Status: \u2705 Complete (3/4 stories - Story 4 deferred to Phase 4) Key Additions: Architecture Decision Records (ADRs), risk assessment, evolution path Note: Virtual smoker device (Story 4) moved to Phase 4 as it's testing infrastructure</p>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#phase-3-deployment-automation","title":"Phase 3: Deployment Automation","text":"<p>Duration: 3-4 weeks (EXTENDED) Focus: PRIORITY ADJUSTED - Security fixes, backup automation, then deployment workflows Status: In Progress - Critical fixes prioritized Critical Changes: - Story 0 (NEW): MongoDB security upgrade, automated backups, deployment safety - MUST COMPLETE FIRST - Story 3 (UPDATED): Production database migration - requires Story 0 completion - Risk-adjusted implementation order focusing on security and reliability before advanced automation</p>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#phase-4-testing-documentation","title":"Phase 4: Testing &amp; Documentation","text":"<p>Duration: 1-2 weeks Focus: Validation, monitoring, documentation, and virtual device testing Status: Planned Additions: Virtual smoker device setup (moved from Phase 2, Story 4)</p>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#architectural-principles-trade-offs","title":"Architectural Principles &amp; Trade-offs","text":"<p>This infrastructure is designed with specific principles reflecting its context as a single-developer, personal project:</p>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#design-principles","title":"Design Principles","text":"<ol> <li>Simplicity over Complexity: Choose simpler solutions even if they're less \"enterprise-grade\"</li> <li>Cost Efficiency: Prioritize zero-cost local infrastructure over cloud services</li> <li>Learning Focus: Balance production readiness with learning opportunities</li> <li>Pragmatic Security: Address critical risks, accept reasonable trade-offs</li> <li>Evolutionary Design: Start simple, add complexity only when needed</li> </ol>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#conscious-trade-offs","title":"Conscious Trade-offs","text":"<ul> <li>High Availability: Single server acceptable for personal project (vs multi-node cluster)</li> <li>Zero-Downtime: Brief maintenance windows acceptable (vs complex blue-green deployments)</li> <li>State Management: Local Terraform state acceptable for single user (vs remote backend with locking)</li> <li>Backup Strategy: Daily backups sufficient (vs real-time replication)</li> <li>Monitoring: Basic monitoring adequate (vs enterprise observability stack)</li> </ul>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#non-negotiable-requirements","title":"Non-Negotiable Requirements","text":"<ul> <li>Data Safety: Must have automated backups and tested restore procedures</li> <li>Security: Must secure database with authentication, keep systems patched</li> <li>Recoverability: Must be able to rebuild infrastructure from code and backups</li> <li>Documentation: Must document all procedures for future reference</li> </ul> <p>For detailed architectural decisions, see Phase 2 ADR Section.</p>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#related-documentation","title":"Related Documentation","text":"<ul> <li>Phase 1: Container Standardization</li> <li>Phase 2: Proxmox Infrastructure</li> <li>Phase 3: Deployment Automation</li> <li>Phase 4: Testing &amp; Documentation</li> <li>Terraform Architecture</li> <li>Virtual Smoker Setup</li> <li>Deployment Workflows</li> <li>Tailscale Network Configuration</li> </ul>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#current-status-next-steps-2025-11-25","title":"Current Status &amp; Next Steps (2025-11-25)","text":""},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#infrastructure-status","title":"Infrastructure Status","text":"<ul> <li>Phase 1: \u2705 Complete - Container standardization implemented</li> <li>Phase 2: \u2705 Complete - Infrastructure provisioned with Terraform and Ansible, Tailscale mesh operational (3/4 stories complete, virtual smoker deferred to Phase 4)</li> <li>Phase 3: \ud83d\ude80 Ready to Start - Security fixes prioritized before automation</li> <li>Phase 4: \u23f8\ufe0f Planned - Testing, documentation, and virtual smoker device</li> </ul>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#immediate-next-steps-priority-order","title":"Immediate Next Steps (Priority Order)","text":"<p>Critical (Complete First - Weeks 1-2): 1. Upgrade MongoDB from 4.4.14-rc0 to 7.x stable in dev environment 2. Enable MongoDB authentication with service accounts 3. Implement automated backup system for LXC containers and MongoDB 4. Add deployment health checks with automated rollback 5. Test all fixes thoroughly in dev environment</p> <p>High Priority (Weeks 3-5): 6. Apply MongoDB upgrade to Raspberry Pi production 7. Migrate production database from Pi to Proxmox 8. Validate migration and monitor for stability 9. Update deployment workflows with new security measures</p> <p>Standard Priority (Weeks 6+): 10. Implement automated development deployment 11. Add production deployment automation with approval gates 12. Set up Raspberry Pi device management 13. Complete virtual device testing automation</p>"},{"location":"Infrastructure/implementation/proxmox-infrastructure-plan/#key-architectural-insights","title":"Key Architectural Insights","text":"<p>From the comprehensive architectural review:</p> <p>Strengths: - Excellent cost efficiency (near-zero monthly infrastructure costs) - Good separation of concerns (Terraform + Ansible) - Appropriate simplicity for single-developer context - Solid foundation for future growth</p> <p>Critical Issues Identified: - MongoDB security vulnerability (no auth, old version) - IMMEDIATE FIX REQUIRED - Missing automated backup system - HIGH PRIORITY - No deployment safety mechanisms - HIGH PRIORITY</p> <p>Pragmatic Acceptance: - Single point of failure (single Proxmox server) - ACCEPTABLE for current scale - Local Terraform state - ACCEPTABLE for single operator - Basic monitoring - ACCEPTABLE, enhance later if needed</p> <p>Architecture Philosophy: This infrastructure optimizes for simplicity, cost, and learning over enterprise-grade high availability and scale. All architectural decisions are documented in ADRs within Phase 2 documentation.</p> <p>Document Version: 2.0 Last Updated: October 14, 2025 Status: Implementation Phase (Phase 3 - Critical Fixes) Owner: Development Team Next Review: After Phase 3 Story 0 completion</p>"},{"location":"Packages/","title":"Packages Documentation","text":"<p>The Smart Smoker V2 monorepo includes shared packages that provide reusable components and utilities across multiple applications.</p>"},{"location":"Packages/#current-packages","title":"Current Packages","text":""},{"location":"Packages/#temperaturechart","title":"TemperatureChart","text":"<ul> <li>Location: <code>packages/TemperatureChart/</code></li> <li>Purpose: Shared temperature visualization component using D3.js</li> <li>Used By: Frontend, Smoker App</li> <li>Technologies: React, TypeScript, D3.js, Material-UI</li> </ul>"},{"location":"Packages/#package-development","title":"Package Development","text":""},{"location":"Packages/#getting-started","title":"Getting Started","text":"<p>All packages in this monorepo follow a standardized structure and testing approach. For creating new packages, see the Testing Template which provides:</p> <ul> <li>Jest configuration with TypeScript support</li> <li>D3.js ES module handling</li> <li>React Testing Library setup</li> <li>Coverage reporting</li> <li>Consistent dependency management</li> </ul>"},{"location":"Packages/#architecture-principles","title":"Architecture Principles","text":"<ol> <li>Reusability: Packages should be generic enough to be used across multiple apps</li> <li>TypeScript First: All packages use TypeScript with strict typing</li> <li>Testing: Minimum 70% test coverage required</li> <li>Documentation: Each package should have comprehensive README</li> <li>Dependencies: Use <code>--legacy-peer-deps</code> for compatibility</li> </ol>"},{"location":"Packages/#package-structure","title":"Package Structure","text":"<pre><code>packages/\n\u251c\u2500\u2500 PackageName/\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 index.ts           # Main export\n\u2502   \u2502   \u251c\u2500\u2500 components/        # React components\n\u2502   \u2502   \u251c\u2500\u2500 types/            # TypeScript interfaces\n\u2502   \u2502   \u251c\u2500\u2500 utils/            # Utility functions\n\u2502   \u2502   \u251c\u2500\u2500 __tests__/        # Test files\n\u2502   \u2502   \u251c\u2500\u2500 __mocks__/        # Mock files (D3, etc.)\n\u2502   \u2502   \u2514\u2500\u2500 setupTests.ts     # Test configuration\n\u2502   \u251c\u2500\u2500 package.json          # Package configuration\n\u2502   \u251c\u2500\u2500 tsconfig.json         # TypeScript config\n\u2502   \u2514\u2500\u2500 README.md             # Package documentation\n</code></pre>"},{"location":"Packages/#testing-strategy","title":"Testing Strategy","text":"<p>Each package follows the same testing approach: - Unit Tests: Component logic and data transformations - Interface Tests: TypeScript type validation - Integration Tests: Cross-component interactions - Mock Strategy: Complex dependencies (D3.js, APIs) are mocked</p>"},{"location":"Packages/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create Package: Follow the testing template structure</li> <li>Install Dependencies: Use <code>npm install --legacy-peer-deps</code></li> <li>Develop: Write TypeScript code with proper interfaces</li> <li>Test: Maintain test coverage above 70%</li> <li>Document: Update README and add to this index</li> <li>Integrate: Import and use in target applications</li> </ol>"},{"location":"Packages/#integration-with-apps","title":"Integration with Apps","text":"<p>Packages are designed to integrate seamlessly with the main applications:</p> <ul> <li>Backend: Can import utility functions and types</li> <li>Device Service: Shares communication protocols and data structures</li> <li>Frontend: Imports React components and visualization tools</li> <li>Smoker App: Uses same components as frontend for consistency</li> </ul>"},{"location":"Packages/#best-practices","title":"Best Practices","text":""},{"location":"Packages/#code-quality","title":"Code Quality","text":"<ul> <li>Follow TypeScript strict mode</li> <li>Use proper error handling</li> <li>Implement comprehensive logging</li> <li>Write self-documenting code</li> </ul>"},{"location":"Packages/#performance","title":"Performance","text":"<ul> <li>Lazy load heavy components</li> <li>Optimize D3.js rendering</li> <li>Use React.memo for expensive renders</li> <li>Implement proper cleanup in useEffect</li> </ul>"},{"location":"Packages/#maintenance","title":"Maintenance","text":"<ul> <li>Keep dependencies up to date</li> <li>Monitor bundle size</li> <li>Regular security audits</li> <li>Backward compatibility considerations</li> </ul>"},{"location":"Packages/#future-packages","title":"Future Packages","text":"<p>Planned packages for future development: - DataProcessor: Time series data analysis utilities - NotificationManager: Cross-platform notification handling - ConfigManager: Shared configuration and settings management - ProtocolHandler: Serial communication protocols - StateManager: Centralized state management utilities</p> <p>For more information about testing new packages, see Testing Template.</p>"},{"location":"Packages/testing-template/","title":"Package Testing Template","text":"<p>This template provides a standardized approach for adding Jest testing to packages in the Smart Smoker V2 monorepo.</p>"},{"location":"Packages/testing-template/#quick-setup-for-new-packages","title":"Quick Setup for New Packages","text":""},{"location":"Packages/testing-template/#1-packagejson-configuration","title":"1. Package.json Configuration","text":"<p>Add these sections to your package's <code>package.json</code>:</p> <pre><code>{\n  \"scripts\": {\n    \"test\": \"jest\",\n    \"test:watch\": \"jest --watch\", \n    \"test:coverage\": \"jest --coverage\"\n  },\n  \"devDependencies\": {\n    \"@testing-library/jest-dom\": \"^5.16.5\",\n    \"@testing-library/react\": \"^13.4.0\",\n    \"@testing-library/user-event\": \"^14.4.3\",\n    \"@types/jest\": \"28.1.8\",\n    \"@types/node\": \"^18.15.0\",\n    \"identity-obj-proxy\": \"^3.0.0\",\n    \"jest\": \"28.0.3\",\n    \"jest-environment-jsdom\": \"28.0.2\",\n    \"ts-jest\": \"28.0.8\",\n    \"typescript\": \"^4.9.5\"\n  },\n  \"jest\": {\n    \"preset\": \"ts-jest\",\n    \"testEnvironment\": \"jsdom\",\n    \"setupFilesAfterEnv\": [\"&lt;rootDir&gt;/src/setupTests.ts\"],\n    \"moduleNameMapper\": {\n      \"\\\\.(css|less|scss)$\": \"identity-obj-proxy\"\n    },\n    \"transformIgnorePatterns\": [\n      \"node_modules/(?!(d3|d3-.*|internmap|delaunator|robust-predicates)/)\"\n    ],\n    \"testMatch\": [\n      \"&lt;rootDir&gt;/src/**/__tests__/**/*.{ts,tsx}\",\n      \"&lt;rootDir&gt;/src/**/*.{test,spec}.{ts,tsx}\"\n    ],\n    \"collectCoverageFrom\": [\n      \"src/**/*.{ts,tsx}\",\n      \"!src/**/*.d.ts\",\n      \"!src/setupTests.ts\",\n      \"!src/__mocks__/**\"\n    ]\n  }\n}\n</code></pre>"},{"location":"Packages/testing-template/#2-setup-files","title":"2. Setup Files","text":"<p>Create <code>src/setupTests.ts</code>: <pre><code>// Jest DOM matchers\nimport '@testing-library/jest-dom';\n\n// Add any global test setup here\n</code></pre></p>"},{"location":"Packages/testing-template/#3-test-file-template","title":"3. Test File Template","text":"<p>Create <code>src/[YourComponent].test.tsx</code>: <pre><code>import React from 'react';\nimport { render, screen } from '@testing-library/react';\nimport YourComponent from './YourComponent';\n\ndescribe('YourComponent Package', () =&gt; {\n  describe('Basic Functionality', () =&gt; {\n    test('component renders without crashing', () =&gt; {\n      render(&lt;YourComponent /&gt;);\n      expect(screen.getByRole('main')).toBeInTheDocument();\n    });\n\n    test('component accepts required props', () =&gt; {\n      const props = {\n        // Add your component props here\n      };\n      render(&lt;YourComponent {...props} /&gt;);\n      // Add assertions based on your component\n    });\n  });\n\n  describe('TypeScript Interfaces', () =&gt; {\n    test('interfaces work correctly', () =&gt; {\n      // Test your TypeScript interfaces\n      const testData: YourDataType = {\n        // Add test data structure\n      };\n      expect(testData).toBeDefined();\n    });\n  });\n});\n</code></pre></p>"},{"location":"Packages/testing-template/#4-d3js-components-if-needed","title":"4. D3.js Components (If Needed)","text":"<p>If your package uses D3.js, create <code>src/__mocks__/d3.ts</code>: <pre><code>// Mock D3 module for testing\nconst mockChainable = {\n  attr: jest.fn().mockReturnThis(),\n  style: jest.fn().mockReturnThis(),\n  text: jest.fn().mockReturnThis(),\n  classed: jest.fn().mockReturnThis(),\n  call: jest.fn().mockReturnThis(),\n  transition: jest.fn().mockReturnThis(),\n  duration: jest.fn().mockReturnThis(),\n  on: jest.fn().mockReturnThis(),\n};\n\n// Add more D3 mocks as needed for your specific use case\nexport const select = jest.fn(() =&gt; ({\n  ...mockChainable,\n  // Add specific methods your component uses\n}));\n\nexport default { select /* other exports */ };\n</code></pre></p> <p>And add to your Jest config: <pre><code>\"moduleNameMapper\": {\n  \"\\\\.(css|less|scss)$\": \"identity-obj-proxy\",\n  \"^d3$\": \"&lt;rootDir&gt;/src/__mocks__/d3.ts\"\n}\n</code></pre></p>"},{"location":"Packages/testing-template/#5-installation","title":"5. Installation","text":"<pre><code>cd packages/YourPackage\nnpm install --legacy-peer-deps\nnpm test\n</code></pre>"},{"location":"Packages/testing-template/#testing-commands","title":"Testing Commands","text":"<p>All packages should support these commands:</p> <ul> <li><code>npm test</code> - Run tests once</li> <li><code>npm run test:watch</code> - Run tests in watch mode</li> <li><code>npm run test:coverage</code> - Run tests with coverage report</li> </ul>"},{"location":"Packages/testing-template/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple: Begin with interface/type tests and basic rendering</li> <li>Mock Complex Dependencies: Use mocks for D3, external APIs, etc.</li> <li>Test Data Structures: Ensure TypeScript interfaces work correctly</li> <li>Focus on Logic: Test business logic rather than visual rendering</li> <li>Use Consistent Naming: Follow the pattern <code>[ComponentName].test.tsx</code></li> </ol>"},{"location":"Packages/testing-template/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Minimum: 70% code coverage</li> <li>Target: 80%+ code coverage</li> <li>Focus: Critical business logic and data transformations</li> </ul>"},{"location":"Packages/testing-template/#integration-with-monorepo","title":"Integration with Monorepo","text":"<p>This testing setup integrates with the overall Smart Smoker V2 testing strategy:</p> <ul> <li>Consistent Jest Version: 28.0.3 across all packages and apps</li> <li>Shared Dependencies: Reuses testing utilities from workspace</li> <li>D3.js Support: Handles D3.js modules consistently</li> <li>TypeScript: Full TypeScript support with proper type checking</li> </ul>"},{"location":"Smoker%20Frontend/","title":"Getting Started","text":"<p>after installing (on the welcome page)</p> <p>just run  <code>npm run start</code>  and  visit <code>http://localhost:8080</code> </p> <p>will need to run the backend in order for app to function and save data  so go to that page to set that up</p> <p>make sure you double check the values in .env.local is pointing to your local backend (it should be by default)</p> <p>dimensions for the ui is build for 800 X 400</p>"},{"location":"Smoker%20Frontend/#electron-shell","title":"Electron shell","text":"<p>To build electron shell use the <code>npm run forge:thin</code> cmd to build for you arch  use the <code>npm run forgeLinux64:thin</code> to build for pi 3 that is used on smoker (results may vary base on what you are building on)</p> <p>To run electron docker container on Pi use this cmd  <pre><code>sudo xhost local:root &amp;&amp; sudo docker run --net=host \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=unix$DISPLAY \\\n  -v \"$(pwd)\"/src:/app/src --rm -it --device /dev/snd \\\n  benjr70/smart-smoker-electron-shell:latest\n</code></pre></p>"}]}