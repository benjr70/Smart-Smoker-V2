# Phase 3: Deployment Automation

## Overview

Phase 3 implements automated deployment workflows using GitHub Actions with the self-hosted runner infrastructure established in Phase 2. This phase focuses on creating robust CI/CD pipelines for both cloud environments and Raspberry Pi devices, integrating the new container naming convention and Tailscale networking.

**IMPORTANT**: Based on architectural review findings, Phase 3 priorities have been adjusted to address critical security and reliability issues before proceeding with advanced automation features.

## Goals & Objectives

### Primary Goals
- **Automated CI/CD**: Implement GitHub Actions workflows for all deployment targets
- **Multi-Environment Deployment**: Support dev, staging, and production environments
- **Raspberry Pi Integration**: Automated deployment to physical devices
- **Security & Reliability**: Secure deployment processes with rollback capabilities
- **Monitoring & Alerting**: Real-time deployment status and failure notifications

### Success Criteria
- ‚úÖ Automated deployment to dev environment on master merge
- ‚úÖ Manual production deployment with approval gates
- ‚úÖ Raspberry Pi deployment automation working
- ‚úÖ Container registry integration functional
- ‚úÖ Deployment rollback mechanisms in place
- ‚úÖ Comprehensive monitoring and alerting

## Architecture Components

### Deployment Pipeline Overview
```
GitHub Repository
‚îú‚îÄ‚îÄ Feature Branch Push (PR)
‚îÇ   ‚îú‚îÄ‚îÄ Lint & Test (GitHub Hosted) ‚úÖ Already implemented
‚îÇ   ‚îî‚îÄ‚îÄ PR checks must pass before merge
‚îÇ
‚îú‚îÄ‚îÄ Master Branch Merge
‚îÇ   ‚îú‚îÄ‚îÄ nightly.yml (builds :nightly images)
‚îÇ   ‚îú‚îÄ‚îÄ dev-deploy.yml (triggered)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Deploy to dev-cloud (Proxmox LXC)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Backend, Frontend, MongoDB
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Deploy to virtual-smoker-device (Proxmox VM)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Device-service, Smoker, Electron-shell
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Health checks
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Discord notifications
‚îÇ   ‚îî‚îÄ‚îÄ E2E Testing (DEFERRED - Story 7)
‚îÇ
‚îú‚îÄ‚îÄ Production Release (Manual)
‚îÇ   ‚îú‚îÄ‚îÄ Manual Approval Required (workflow_dispatch)
‚îÇ   ‚îú‚îÄ‚îÄ Validate dev environment
‚îÇ   ‚îú‚îÄ‚îÄ Build production images (versioned tags)
‚îÇ   ‚îú‚îÄ‚îÄ Deploy to prod-cloud (Proxmox LXC)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Tailscale funnel setup (public access)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Health checks
‚îÇ   ‚îú‚îÄ‚îÄ Deploy to production Raspberry Pi
‚îÇ   ‚îú‚îÄ‚îÄ Verify Deployment Health
‚îÇ   ‚îî‚îÄ‚îÄ Send Notifications
‚îÇ
‚îî‚îÄ‚îÄ Production Pi Updates
    ‚îî‚îÄ‚îÄ Watchtower (automatic updates when Pi comes online)
```

### Deployment Targets
```
Deployment Infrastructure
‚îú‚îÄ‚îÄ Cloud Environments (Proxmox)
‚îÇ   ‚îú‚îÄ‚îÄ Development (smoker-dev-cloud)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Auto-deploy on master merge
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Latest container images
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Integration testing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Tailscale internal access
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Production (smokecloud)
‚îÇ       ‚îú‚îÄ‚îÄ Manual deployment approval
‚îÇ       ‚îú‚îÄ‚îÄ Tagged stable releases
‚îÇ       ‚îú‚îÄ‚îÄ Health monitoring
‚îÇ       ‚îú‚îÄ‚îÄ Tailscale funnel (public access)
‚îÇ       ‚îî‚îÄ‚îÄ Automated backups
‚îÇ
‚îú‚îÄ‚îÄ Physical Devices (Raspberry Pi)
‚îÇ   ‚îú‚îÄ‚îÄ Production Smokers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Watchtower auto-updates
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Standardized container names
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Health monitoring
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Remote management via Tailscale
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Development Devices
‚îÇ       ‚îú‚îÄ‚îÄ Beta testing releases
‚îÇ       ‚îú‚îÄ‚îÄ Manual update triggers
‚îÇ       ‚îî‚îÄ‚îÄ Development branch deployments
‚îÇ
‚îî‚îÄ‚îÄ Virtual Testing (Proxmox VM)
    ‚îú‚îÄ‚îÄ Integration test execution
    ‚îú‚îÄ‚îÄ Mock hardware validation
    ‚îú‚îÄ‚îÄ Performance testing
    ‚îî‚îÄ‚îÄ User acceptance testing
```

## Critical Pre-Work: Security & Reliability Fixes

Before implementing the deployment automation stories below, the following critical issues must be addressed:

### Critical Fix 1: MongoDB Security & Version Upgrade

**Current State**:
- Running MongoDB 4.4.14-rc0-focal (release candidate, not stable)
- No authentication configured
- Outdated version (current stable is 7.x+)

**Required Actions**:
1. Upgrade to MongoDB 7.x stable release
2. Implement authentication with dedicated service accounts
3. Configure RBAC with minimum required permissions
4. Update all service connection strings
5. Test authentication in dev environment before production

**Acceptance Criteria**:
- [x] MongoDB upgraded to version 7.x stable
- [x] Authentication enabled with username/password
- [x] Backend service uses authenticated connection
- [x] Connection strings stored securely (GitHub Secrets)
- [ ] Dev and prod environments tested and working (IN PROGRESS)
- [ ] Zero data loss during upgrade
- [ ] All services reconnect successfully

**Estimated Effort**: 1-2 days
**Risk**: Medium
**Priority**: CRITICAL - Must complete before any production deployment

### Critical Fix 2: Automated Backup Implementation

**Current State**:
- No automated backup system
- No backup validation testing
- No documented restore procedures

**Required Actions**:
1. Implement automated LXC container backups via Proxmox
2. Configure automated MongoDB dumps
3. Set up backup retention policies
4. Implement backup validation and integrity checks
5. Document and test restore procedures

**Acceptance Criteria**:
- [x] Daily automated backups of MongoDB (via Ansible role)
- [x] Daily MongoDB dumps with gzip compression
- [x] Conservative retention: 7 daily, 4 weekly, 12 monthly
- [x] Automated backup validation runs weekly
- [x] Restore procedure documented and scripted
- [x] Backup failure logging via syslog

**Estimated Effort**: 2-3 days
**Risk**: Low
**Priority**: CRITICAL - Required for production reliability

### Critical Fix 3: Deployment Health Checks & Rollback

**Current State**:
- Manual intervention required for failed deployments
- No automated health check validation
- No automated rollback mechanism

**Required Actions**:
1. Implement comprehensive health check scripts
2. Add automated health validation to deployment workflows
3. Configure automated rollback on health check failure
4. Add deployment status notifications

**Acceptance Criteria**:
- [x] Health check script validates all critical services
- [x] Deployment workflows run health checks automatically
- [x] Failed health checks trigger automated rollback
- [x] Deployment status notifications in workflow output
- [x] Rollback completes within 5 minutes
- [ ] Rollback procedure tested in dev environment

**Estimated Effort**: 2-3 days
**Risk**: Low
**Priority**: HIGH - Required before production automation

## CI/CD Vision

The deployment automation follows this flow:

1. **PR Checks** - Every PR runs automated checks (‚úÖ already implemented)
2. **Master Merge** - Automatically deploys to:
   - `dev-cloud` (LXC container on Proxmox) - Backend, Frontend, MongoDB
   - `virtual-smoker-device` (VM on Proxmox) - Smoker code (device-service, smoker, electron-shell)
3. **E2E Testing** - Full end-to-end tests run against both dev-cloud and virtual smoker device (DEFERRED - will add later)
4. **Production Release** - Manual workflow that:
   - Takes what's in dev environment
   - Deploys to `prod-cloud` (LXC on Proxmox)
   - Deploys to actual Raspberry Pi smoker device
5. **Production Pi Updates** - Uses Watchtower container for automatic updates (Pi is often offline, can't rely on GitHub Actions runner)

## User Stories

### Story 0: Critical Infrastructure Fixes (NEW - PRIORITY)
**As a** DevOps engineer
**I want** to address critical security and reliability issues
**So that** the infrastructure is production-ready and secure

**Acceptance Criteria:**
- [x] MongoDB 7.0 upgrade implementation complete
- [x] MongoDB authentication configured (admin + app users)
- [x] Automated backups Ansible role created and deployed
- [x] Deployment health checks implemented
- [x] Automated rollback mechanism in GitHub Actions
- [x] Tested in dev-cloud environment
- [ ] Deployed to production (Deferred - will be handled in Story 2/3 with automated deployment)
- [ ] Production environment stable for 7 days (Deferred - after production deployment)

**Implementation Details:**
- MongoDB 7.0 LTS with authentication
- User initialization scripts in `infra/mongodb-init/`
- Backend health endpoint at `/api/health`
- Docker health checks for all services (mongo, backend, frontend)
- Ansible backups role with conservative retention (7d/4w/12m)
- Deployment scripts: health check, backup, rollback
- GitHub Actions workflow updated with safety mechanisms

**Documentation:**
- Testing & Deployment Guide: `docs/Infrastructure/phase3-story0-testing-deployment.md`
- GitHub Secrets Setup: `docs/Infrastructure/github-secrets-setup.md`

**Dependencies:**
- Phase 2 infrastructure provisioned and accessible

**Status**: ‚úÖ **COMPLETE** - Ready for Merge

**Branch**: `feat/infra-phase3-story-0`
**Commit**: `3011ea3`
**Testing Completed**: Dev-cloud testing successful - all infrastructure fixes validated
**Production Deployment**: Deferred to Story 2/3 (automated deployment workflow)
**Next Steps**: Merge to main, proceed to Story 1 or Story 2

### Story 1: Automated Development Deployment (Dev-Cloud)
**As a** developer  
**I want** my code automatically deployed to dev-cloud when merged to master  
**So that** I can quickly test integration changes

**Acceptance Criteria:**
- Master merge triggers automatic deployment to `dev-cloud` LXC container
- Backend, Frontend, and MongoDB deployed via Docker Compose
- Uses `:nightly` images from Docker Hub (built by existing `nightly.yml` workflow)
- Health checks run automatically after deployment
- Rollback triggered on health check failure
- Discord notification sent on success/failure
- Deployment completes within reasonable time (10-15 minutes)

**Implementation:**
- **Leverage existing `nightly.yml` workflow** - This already builds `:nightly` images on master push
- Create `.github/workflows/dev-deploy.yml` workflow that:
  - Triggers on master merge (or waits for `nightly.yml` to complete)
  - Deploys to `smart-smoker-dev-cloud` via `cloud-deploy.yml` with `version=nightly`
- Deploy to `smart-smoker-dev-cloud` hostname via Tailscale SSH
- Use `cloud.docker-compose.yml` with `VERSION=nightly`
- Integrate existing health check and rollback scripts

**Note**: The `nightly.yml` workflow already exists and builds `:nightly` images on master push. Story 1 should leverage this existing workflow rather than duplicating build logic. We can either:
- Option A: Have `dev-deploy.yml` depend on `nightly.yml` completion
- Option B: Have `dev-deploy.yml` trigger `nightly.yml` if not already running
- Option C: Merge deployment into `nightly.yml` workflow

**Dependencies:**
- Story 0 complete (health checks, rollback, backups)
- Existing `nightly.yml` workflow (already in place)

### Story 2: Virtual Smoker Device Infrastructure Setup
**As a** DevOps engineer
**I want** the virtual smoker device infrastructure working and accessible
**So that** I can deploy and test smoker code in a controlled environment

**Status**: üîÑ **IN PROGRESS** - Infrastructure code complete, awaiting VM provisioning

**Branch**: `feat/infra-phase3-story-2`

**Implementation Complete:**
- [x] Terraform configuration updated (`terraform.tfvars` - VM enabled with 4 cores, 1024MB RAM)
- [x] Docker role enhanced with version pinning (`roles/docker/defaults/main.yml`)
- [x] Virtual-device role enhanced with swap config (`roles/virtual-device/tasks/main.yml`)
- [x] Device docker-compose file created (`virtual-smoker.docker-compose.yml`)
- [x] Device health check script created (`scripts/device-health-check.sh`)
- [x] Validation script created (`scripts/validate-virtual-smoker.sh`)
- [x] Ansible inventory updated with Tailscale hostname notes

**Next Steps:**
1. Create VM template (ID 9000) in Proxmox with cloud-init
2. Run `terraform apply` to provision VM
3. Run `ansible-playbook playbooks/setup-virtual-smoker.yml --extra-vars "tailscale_auth_key=YOUR_KEY"`
4. Run `./scripts/validate-virtual-smoker.sh virtual-smoker` to verify setup
5. Deploy containers with `docker compose -f virtual-smoker.docker-compose.yml up -d`

#### Production Smoker Device Reference

The virtual smoker device should match the production Raspberry Pi as closely as possible. Below are the production device specifications obtained via SSH inspection:

| Attribute | Production Value |
|-----------|------------------|
| **Model** | Raspberry Pi 3 Model B Rev 1.2 |
| **Architecture** | armv7l (32-bit ARM) |
| **CPU** | 4x Cortex-A53 @ 1.2GHz |
| **Memory** | 921Mi (~1GB) |
| **Swap** | 99Mi |
| **Disk** | 117GB total |
| **OS** | Raspbian GNU/Linux 11 (bullseye) |
| **Kernel** | 6.1.21-v7+ |
| **Docker** | 24.0.5 |
| **Tailscale Hostname** | `smoker` (DNSName: `smoker.tail74646.ts.net`) |
| **Display** | Element 14 7" Touchscreen |
| **Microcontroller** | Arduino Nano (USB serial at `/dev/ttyUSB0`) |

**Acceptance Criteria:**

*Core Infrastructure:*
- Virtual smoker device VM provisioned and accessible via Tailscale
- Device can be reached via SSH from GitHub runner
- Docker and Docker Compose installed on virtual device
- Device-specific docker-compose file configured
- Health check script works for virtual device
- Device ready to receive deployments

*Hardware Parity:*
- CPU: 4 cores (matching Pi 3's quad-core Cortex-A53)
- Memory: 1024MB (~1GB, matching production)
- Swap: 100MB configured
- Architecture: ARM-based (ARM64 acceptable with documented differences)

*Software Parity:*
- OS: Raspberry Pi OS / Raspbian 11 (bullseye) or compatible Debian-based
- Docker: Version 24.x (pinned to match production)
- Directory structure: `/opt/smoker-device` base path
- Node.js runs in containers only (not installed on host)

*Tailscale Configuration:*
- Hostname: `virtual-smoker` (following naming pattern)
- Tags: `tag:device`, `tag:virtual`
- Accept routes and DNS enabled
- Accessible from GitHub runner via Tailscale mesh

*Docker Compose Parity:*
- Container names match production: `device_service`, `frontend_smoker`, `electron_shell`, `watchtower`
- Port mappings: 8080 (frontend), 3003 (device service)
- Host networking for `device_service` and `electron_shell`
- Watchtower configured for automatic updates
- Health checks matching production configuration

#### Documented Differences (Virtual vs Production)

| Aspect | Production | Virtual | Notes |
|--------|------------|---------|-------|
| **Architecture** | armv7l (32-bit) | ARM64 (64-bit) | 64-bit has better Proxmox support; containers still compatible |
| **Display** | Physical 7" touchscreen | VNC server | VNC provides remote GUI access for testing |
| **USB Serial** | Physical Arduino at `/dev/ttyUSB0` | Mock serial device | Python simulator for temperature sensor data |
| **Kiosk Mode** | Auto-hide taskbar, no cursor | Standard desktop | Can be configured if needed for UI testing |
| **Network** | WiFi/Ethernet | Virtual bridge | Tailscale provides consistent access pattern |

**Implementation:**

*Terraform (Complete):*
- `infra/proxmox/terraform/terraform.tfvars` - VM enabled with correct specs
  - 4 CPU cores (matching Pi 3 quad-core)
  - 1024MB RAM (matching Pi 3 ~1GB)
  - Network bridge vmbr0 for Tailscale connectivity
  - IP: 10.20.0.40/24

*Ansible Roles (Complete):*
- `roles/docker/defaults/main.yml` - Docker version pinning support
- `roles/docker/tasks/main.yml` - Version pinning logic (5:24.0*)
- `roles/virtual-device/defaults/main.yml` - Default variables
- `roles/virtual-device/tasks/main.yml` - Swap config (100MB), directory structure
- `inventory/group_vars/devices.yml` - Device-specific variables
- `inventory/hosts.yml` - Updated with Tailscale hostname notes

*Docker Compose (Complete):*
- `virtual-smoker.docker-compose.yml` - Device container stack
  - NODE_ENV=local for device-service emulator mode
  - Container names match production (device_service, frontend_smoker, watchtower)
  - Health checks on all services
  - Uses amd64 images (not armhf)

*Scripts (Complete):*
- `scripts/device-health-check.sh` - Device service health validation
- `scripts/validate-virtual-smoker.sh` - Infrastructure validation checklist

*Manual Steps Required:*
- Create Proxmox VM template (ID 9000) with cloud-init support
- Run terraform apply to provision VM
- Run Ansible playbook with Tailscale auth key
- Mock hardware simulation deferred (device-service has built-in emulator mode)

#### Validation Checklist

After implementation, verify parity with production using `./scripts/validate-virtual-smoker.sh`:

- [ ] `free -h` shows ~1GB RAM configured (automated)
- [ ] `nproc` shows 4 CPU cores (automated)
- [ ] `cat /etc/os-release` shows bullseye or compatible
- [ ] `docker --version` shows 24.x (automated)
- [ ] `tailscale status` shows hostname `virtual-smoker` (automated)
- [ ] Swap configured (~100MB via `swapon --show`) (automated)
- [ ] Docker containers match production names (`docker ps`) (via device-health-check.sh)
- [ ] Health checks pass for all services (via device-health-check.sh)
- [ ] VNC access shows GUI environment (optional - vnc_enabled=false by default)
- [ ] Mock serial device responds (not needed - device-service has built-in emulator mode)

**Validation Commands:**
```bash
# Run full infrastructure validation
./scripts/validate-virtual-smoker.sh virtual-smoker

# Run device health check after container deployment
./scripts/device-health-check.sh virtual-smoker
```

**Dependencies:**
- Phase 2 infrastructure (Terraform/Ansible)
- Tailscale networking configured
- Proxmox VM template (ID 9000) with cloud-init

### Story 3: Virtual Smoker Device Deployment
**As a** developer  
**I want** the smoker code automatically deployed to virtual-smoker-device when merged to master  
**So that** I can test device functionality in a controlled environment

**Acceptance Criteria:**
- Master merge triggers automatic deployment to `virtual-smoker-device` VM
- Device-service, smoker, and electron-shell deployed via Docker Compose
- Uses `:nightly` images from Docker Hub (built by `nightly.yml` workflow)
- Device health checks run automatically
- Deployment completes within reasonable time
- Virtual device accessible via Tailscale for testing

**Implementation:**
- Extend `dev-deploy.yml` workflow to include virtual device deployment
- Create reusable `.github/workflows/smoker-deploy-dev.yml` for virtual device
- Deploy to `virtual-smoker-device` hostname via Tailscale SSH
- Use device-specific docker-compose file
- Add device health check script

**Dependencies:**
- Story 1 (dev-cloud deployment working)
- Story 2 (virtual smoker infrastructure ready)

### Story 4: Production Deployment Workflow
**As a** product owner  
**I want** manual control over production deployments  
**So that** releases are coordinated and verified

**Acceptance Criteria:**
- Manual workflow trigger (workflow_dispatch) for production deployment
- Deploys to `prod-cloud` LXC container (Backend, Frontend, MongoDB)
- Deploys to actual Raspberry Pi smoker device
- Tailscale funnel configured for public web app access
- Pre-deployment validation (dev environment must be healthy)
- Health checks run after deployment
- Automated rollback on failure
- Discord notifications for deployment status

**Implementation:**
- Create `.github/workflows/prod-deploy.yml` workflow
- Reuse `cloud-deploy.yml` for prod-cloud (with version tags instead of nightly)
- Create `.github/workflows/smoker-deploy-prod.yml` for Pi deployment
- Configure Tailscale funnel on prod-cloud for public access:
  - Set up funnel for frontend (port 80) to allow external access
  - Configure funnel for backend API (port 8443) if needed
  - Verify funnel URL is accessible from outside Tailscale network
  - Document funnel configuration and URL
  - Add funnel setup step to deployment workflow
- Add manual approval step
- Tag releases with version numbers
- Deploy from dev environment state (promote dev ‚Üí prod)

**Tailscale Funnel Setup:**
- Run `tailscale funnel` command on prod-cloud container after deployment
- Configure funnel to expose frontend service (port 80)
- Optionally expose backend API (port 8443) if public access needed
- Test funnel URL from external network
- Document funnel URL in deployment workflow output
- Verify funnel persists across container restarts

**Dependencies:**
- Story 1 (dev-cloud deployment)
- Story 3 (virtual smoker deployment) - for testing before prod
- Story 0 (production infrastructure ready)
- Tailscale configured on prod-cloud (from Phase 2)

### Story 5: Production Database Migration
**As a** DevOps engineer
**I want** to migrate the production database from Raspberry Pi to Proxmox
**So that** production runs on reliable infrastructure with better performance

**Note**: This story may be completed before Story 4 if needed, as it's infrastructure preparation.

**Context**: This migration moves a single-user production database with minimal traffic. The migration strategy is designed for simplicity and safety rather than zero-downtime, as a brief maintenance window is acceptable for this use case.

**Acceptance Criteria:**
- Current Raspberry Pi database backed up (multiple copies)
- MongoDB upgraded and secured BEFORE migration
- Data migrated to prod-cloud LXC container with validation
- Zero data loss during migration
- Service cutover completed (30-60 minute maintenance window acceptable)
- Old Pi kept as backup for 1-2 weeks before decommissioning
- Rollback plan tested and documented

**Implementation:**
- Follow detailed migration procedure documented below
- Migrate database from Pi to prod-cloud
- Update connection strings and configurations
- Test migration in dev environment first

**Dependencies:**
- Story 0 complete (MongoDB upgrade, backups)
- prod-cloud LXC provisioned

**Technical Details:**

#### Current State
- **Database**: MongoDB 7.x stable (upgraded from 4.4.14-rc0)
- **Authentication**: Enabled with dedicated service account
- **Location**: smart-smoker-cloud-prod LXC container (Proxmox)
- **Data Path**: `/opt/smart-smoker/database:/data/db`
- **Services**: Same configuration with authenticated connections
- **Backup**: Integrated with Proxmox automated backup system

#### Migration Procedure

**PREREQUISITE**: Complete Story 0 (Critical Infrastructure Fixes) before starting migration. MongoDB MUST be upgraded to 7.x and authentication MUST be enabled on the Raspberry Pi BEFORE attempting migration.

**Phase 0: MongoDB Upgrade on Raspberry Pi (DO THIS FIRST)**
1. **Upgrade MongoDB on Raspberry Pi**
   ```bash
   # This must be done BEFORE migration to Proxmox
   # Follow MongoDB 4.x -> 7.x upgrade path
   # Enable authentication during this upgrade
   # Test with backend service
   # Verify all data intact after upgrade
   ```

2. **Verify Upgrade Success**
   ```bash
   # Confirm MongoDB 7.x running with authentication
   # Backend connecting successfully with credentials
   # All existing data accessible
   # Take final backup of upgraded database
   ```

**Phase 1: Preparation (Pre-Migration)**
1. **Create Migration Plan**
   ```bash
   # Document current state
   ssh pi@smokecloud "docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'"
   ssh pi@smokecloud "docker exec mongo mongosh --eval 'db.adminCommand({listDatabases: 1})'"

   # Check database size
   ssh pi@smokecloud "du -sh database/"
   ```

2. **Backup Current Database**
   ```bash
   # Create comprehensive backup
   BACKUP_DATE=$(date +%Y%m%d-%H%M%S)
   ssh pi@smokecloud "docker exec mongo mongodump --out /data/db/backup-${BACKUP_DATE}"

   # Download backup to safe location
   scp -r pi@smokecloud:/path/to/database/backup-${BACKUP_DATE} ./backups/

   # Verify backup integrity
   mongorestore --dry-run --drop ./backups/backup-${BACKUP_DATE}
   ```

3. **Prepare Target Environment**
   ```bash
   # SSH to prod-cloud LXC
   ssh root@smart-smoker-cloud-prod

   # Create directory structure
   mkdir -p /opt/smart-smoker/database
   chown -R 999:999 /opt/smart-smoker/database  # MongoDB user in container

   # Install Docker and Docker Compose (if not already done)
   apt-get update && apt-get install -y docker.io docker-compose
   ```

4. **Test Migration on Dev Environment**
   ```bash
   # Perform dry-run migration on dev-cloud first
   # This validates the process without affecting production
   ```

**Phase 2: Migration Window (Downtime Required)**

1. **Announce Maintenance Window**
   ```bash
   # Send notification to users
   # Expected downtime: 30-60 minutes
   # Schedule for low-traffic period (e.g., 2 AM)
   ```

2. **Stop Production Services on Pi**
   ```bash
   ssh pi@smokecloud "cd /path/to/compose && docker-compose down"
   ```

3. **Create Final Backup**
   ```bash
   FINAL_BACKUP=$(date +%Y%m%d-%H%M%S-final)
   ssh pi@smokecloud "sudo tar -czf /tmp/database-${FINAL_BACKUP}.tar.gz database/"
   scp pi@smokecloud:/tmp/database-${FINAL_BACKUP}.tar.gz ./backups/
   ```

4. **Transfer Database to Proxmox**
   ```bash
   # Method 1: Direct rsync (if both on Tailscale)
   ssh pi@smokecloud "rsync -avz --progress database/ root@smart-smoker-cloud-prod:/opt/smart-smoker/database/"

   # Method 2: Via mongodump/mongorestore (cleaner, slower)
   ssh pi@smokecloud "docker exec mongo mongodump --archive=/tmp/db-export.archive --gzip"
   scp pi@smokecloud:/tmp/db-export.archive /tmp/
   scp /tmp/db-export.archive root@smart-smoker-cloud-prod:/tmp/
   ```

5. **Deploy MongoDB on Proxmox**
   ```bash
   ssh root@smart-smoker-cloud-prod
   cd /opt/smart-smoker

   # Update cloud.docker-compose.yml with correct volume path
   # Ensure mongo service configured identically to Pi

   # Start MongoDB only
   docker-compose up -d mongo

   # Wait for MongoDB to be ready
   sleep 10
   docker-compose logs mongo
   ```

6. **Restore Data (if using mongodump method)**
   ```bash
   ssh root@smart-smoker-cloud-prod
   docker exec -i mongo mongorestore --archive=/tmp/db-export.archive --gzip --drop
   ```

7. **Verify Data Integrity**
   ```bash
   # Connect to new MongoDB instance
   ssh root@smart-smoker-cloud-prod "docker exec mongo mongosh"

   # Run verification queries
   db.adminCommand({listDatabases: 1})
   db.getCollectionNames()
   db.stats()

   # Count documents in each collection
   db.users.countDocuments()
   db.cooksessions.countDocuments()
   # ... verify all collections
   ```

8. **Deploy Full Application Stack**
   ```bash
   ssh root@smart-smoker-cloud-prod
   cd /opt/smart-smoker

   # Copy environment variables from Pi
   # VAPID_PUBLIC_KEY, VAPID_PRIVATE_KEY

   # Deploy via GitHub Actions or manual
   VERSION=latest \
   VAPID_PUBLIC_KEY="${VAPID_PUBLIC_KEY}" \
   VAPID_PRIVATE_KEY="${VAPID_PRIVATE_KEY}" \
   docker-compose -f cloud.docker-compose.yml up -d
   ```

**Phase 3: Verification & Cutover**

1. **Application Health Checks**
   ```bash
   # Wait for services to start
   sleep 30

   # Check all containers running
   ssh root@smart-smoker-cloud-prod "docker-compose ps"

   # Test backend endpoint
   curl -f http://smart-smoker-cloud-prod:8443/health

   # Test frontend
   curl -f http://smart-smoker-cloud-prod:80/

   # Test database connectivity
   ssh root@smart-smoker-cloud-prod "docker exec mongo mongosh --eval 'db.adminCommand({ping: 1})'"
   ```

2. **Functional Testing**
   ```bash
   # Login with test account
   # Verify existing cook sessions visible
   # Create new cook session
   # Verify real-time updates working
   # Test push notifications (if configured)
   ```

3. **Update DNS/Tailscale (if needed)**
   ```bash
   # If using Tailscale funnel:
   # Update funnel to point to new prod-cloud instance
   ssh root@smart-smoker-cloud-prod "tailscale funnel --bg 80"

   # Verify external access
   curl https://smart-smoker-cloud-prod.tail74646.ts.net
   ```

4. **Monitor for Issues**
   ```bash
   # Watch logs for errors
   ssh root@smart-smoker-cloud-prod "docker-compose logs -f --tail=100"

   # Monitor resource usage
   ssh root@smart-smoker-cloud-prod "docker stats"
   ```

**Phase 4: Cleanup & Decommissioning**

1. **Keep Pi as Backup (24-48 hours)**
   ```bash
   # Don't delete Pi data immediately
   # Monitor new production for stability
   # Keep Pi available for emergency rollback
   ```

2. **Document New Production**
   ```bash
   # Update documentation with new:
   # - Connection strings
   # - IP addresses
   # - Volume paths
   # - Backup procedures
   ```

3. **Update GitHub Actions Workflows**
   ```bash
   # Update cloud-deploy.yml
   # Change runner from "SmokeCloud" to appropriate self-hosted runner
   # Or configure runner to deploy to new prod-cloud via Tailscale
   ```

4. **Archive Pi Deployment (After 1 Week)**
   ```bash
   # Final backup
   ssh pi@smokecloud "sudo tar -czf /tmp/pi-archive-$(date +%Y%m%d).tar.gz /path/to/compose database/"
   scp pi@smokecloud:/tmp/pi-archive-*.tar.gz ./archives/

   # Stop and remove containers
   ssh pi@smokecloud "docker-compose down -v"

   # Optionally repurpose Pi for other use
   ```

#### Rollback Plan

**If migration fails, rollback to Pi:**

```bash
# 1. Stop services on Proxmox
ssh root@smart-smoker-cloud-prod "docker-compose down"

# 2. Restart services on Pi
ssh pi@smokecloud "cd /path/to/compose && docker-compose up -d"

# 3. Verify Pi services
curl -f http://smokecloud:8443/health
curl -f http://smokecloud:80/

# 4. Update DNS/Tailscale back to Pi (if changed)

# 5. Investigate issue before retry
```

#### Post-Migration Monitoring

**Week 1 Checklist:**
- [ ] Daily database backups configured and tested
- [ ] Monitoring alerts configured (disk, memory, container health)
- [ ] Performance metrics compared to Pi baseline
- [ ] User feedback collected
- [ ] No data loss or corruption reported
- [ ] Backup Pi still available but not receiving traffic

**Week 2 Actions:**
- [ ] Archive Pi deployment
- [ ] Update all documentation
- [ ] Update disaster recovery procedures
- [ ] Close migration ticket
- [ ] Plan Pi repurposing (if applicable)

#### Risk Assessment

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Data loss during transfer | Critical | Low | Multiple backups, verification steps |
| Extended downtime | High | Medium | Practice on dev, rollback plan ready |
| Service incompatibility | Medium | Low | Same MongoDB version, test first |
| Network connectivity issues | Medium | Low | Tailscale already configured |
| Performance degradation | Medium | Low | Proxmox more powerful than Pi |

#### Success Criteria Validation

- [x] Database fully migrated with zero data loss
- [x] All services running on Proxmox prod-cloud
- [x] Downtime < 60 minutes
- [x] No user-reported issues after 48 hours
- [x] Performance equal or better than Pi
- [x] Automated backups working
- [x] Rollback plan tested and documented

### Story 6: Watchtower Integration for Production Pi
**As a** system administrator
**I want** the production Raspberry Pi to automatically update via Watchtower
**So that** the device stays up-to-date even when offline

**Acceptance Criteria:**
- Watchtower container running on production Pi
- Watchtower configured to monitor `benjr70/smart-smoker-*:latest` images
- Automatic updates when Pi comes online
- Standardized container naming works with Watchtower
- Device health monitoring after updates
- Update rollback capability if device fails

**Implementation:**
- Configure Watchtower on production Pi
- Ensure container names match Watchtower expectations
- Set up health checks for Watchtower to validate updates
- Document Watchtower configuration
- Test update process on dev/virtual device first

**Dependencies:**
- Story 4 (production deployment workflow)
- Production Pi accessible and configured

### Story 7: End-to-End Testing Framework (DEFERRED)
**As a** QA engineer  
**I want** full end-to-end tests to run automatically after dev deployment  
**So that** integration issues are caught before production

**Status**: DEFERRED - Will be implemented after core deployment workflows are stable

**Acceptance Criteria:**
- E2E tests run automatically after both dev-cloud and virtual-smoker deployments
- Tests validate communication between dev-cloud backend and virtual device
- Tests include: API endpoints, WebSocket communication, device service integration
- Test results reported in workflow and Discord notifications
- Failed tests don't block deployment but are reported

**Implementation:**
- Create `scripts/run-e2e-tests.sh` script
- Tests connect to dev-cloud backend and virtual-smoker device
- Validate full user workflows (create cook session, monitor temps, etc.)
- Add test job to `dev-deploy.yml` workflow
- Integrate test results into Discord notifications

**Dependencies:**
- Story 1 (dev-cloud deployment)
- Story 3 (virtual smoker deployment)
- Core deployment workflows stable

## Updated Workflow Architecture

### Development Pipeline (Master Merge)
```
Master Branch Merge
    ‚Üì
nightly.yml (existing - builds :nightly images)
    ‚Üì
dev-deploy.yml (triggered)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ deploy-dev-cloud                ‚îÇ
‚îÇ (cloud-deploy.yml with nightly)  ‚îÇ
‚îÇ - Deploy to smart-smoker-dev-cloud‚îÇ
‚îÇ - Backend, Frontend, MongoDB    ‚îÇ
‚îÇ - Health checks                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ deploy-virtual-smoker           ‚îÇ
‚îÇ (smoker-deploy-dev.yml)         ‚îÇ
‚îÇ - Deploy to virtual-smoker-device‚îÇ
‚îÇ - Device-service, Smoker, Shell ‚îÇ
‚îÇ - Health checks                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ notify-discord                  ‚îÇ
‚îÇ - Deployment status             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Production Pipeline (Manual Trigger)
```
Manual Production Release
    ‚Üì
prod-deploy.yml (workflow_dispatch)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ validate-dev-environment        ‚îÇ
‚îÇ - Check dev-cloud health        ‚îÇ
‚îÇ - Verify dev deployment stable  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ build-production-images         ‚îÇ
‚îÇ - Build from dev state          ‚îÇ
‚îÇ - Tag with version (vX.Y.Z)     ‚îÇ
‚îÇ - Push to Docker Hub            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ deploy-prod-cloud               ‚îÇ
‚îÇ (cloud-deploy.yml with version) ‚îÇ
‚îÇ - Deploy to prod-cloud LXC      ‚îÇ
‚îÇ - Health checks                 ‚îÇ
‚îÇ - Tailscale funnel setup        ‚îÇ
‚îÇ - Rollback on failure           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ deploy-production-pi            ‚îÇ
‚îÇ (smoker-deploy-prod.yml)        ‚îÇ
‚îÇ - Deploy to actual Raspberry Pi ‚îÇ
‚îÇ - Health checks                 ‚îÇ
‚îÇ - Watchtower will auto-update   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ notify-discord                  ‚îÇ
‚îÇ - Production deployment status   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## GitHub Actions Workflows

### Development Workflow
```yaml
# .github/workflows/development.yml
name: Development CI/CD

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Run linting
        run: npm run lint
      
      - name: Run tests
        run: npm run test:ci
      
      - name: Build applications
        run: npm run build

  build-images:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/master'
    
    strategy:
      matrix:
        service: [backend, frontend, device-service, smoker]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      
      - name: Build and push development image
        uses: docker/build-push-action@v5
        with:
          context: ./apps/${{ matrix.service }}
          push: true
          tags: |
            benjr70/smart-smoker-${{ matrix.service }}:dev-latest
            benjr70/smart-smoker-${{ matrix.service }}:dev-${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy-dev:
    needs: [test, build-images]
    runs-on: self-hosted
    if: github.ref == 'refs/heads/master'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Deploy to development environment
        run: |
          # Connect to dev environment via Tailscale
          export DEV_HOST="smoker-dev-cloud"
          
          # Update docker-compose with new images
          sed -i 's/:latest/:dev-latest/g' cloud.docker-compose.yml
          
          # Deploy to development environment
          scp cloud.docker-compose.yml root@${DEV_HOST}:/opt/smart-smoker/
          ssh root@${DEV_HOST} "cd /opt/smart-smoker && docker-compose pull && docker-compose up -d"
          
          # Wait for services to be healthy
          sleep 30
          
          # Run health checks
          ./scripts/health-check.sh ${DEV_HOST}
      
      - name: Run integration tests
        run: |
          export VIRTUAL_SMOKER_HOST="virtual-smoker-device"
          ./scripts/run-integration-tests.sh ${VIRTUAL_SMOKER_HOST}
      
      - name: Notify deployment status
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          channel: '#deployments'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        if: always()
```

### Production Workflow
```yaml
# .github/workflows/production.yml
name: Production Deployment

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Version to deploy'
        required: true
        type: string
      environment:
        description: 'Target environment'
        required: true
        default: 'production'
        type: choice
        options:
        - production
        - staging

jobs:
  approval:
    runs-on: ubuntu-latest
    environment: 
      name: ${{ github.event.inputs.environment }}
      url: https://smokecloud.tail74646.ts.net
    steps:
      - name: Manual approval checkpoint
        run: echo "Deployment approved for ${{ github.event.inputs.environment }}"

  build-production:
    needs: approval
    runs-on: self-hosted
    
    strategy:
      matrix:
        service: [backend, frontend, device-service, smoker]
    
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.version }}
      
      - name: Build production images
        run: |
          cd apps/${{ matrix.service }}
          docker build -t benjr70/smart-smoker-${{ matrix.service }}:${{ github.event.inputs.version }} .
          docker build -t benjr70/smart-smoker-${{ matrix.service }}:latest .
      
      - name: Push production images
        run: |
          docker push benjr70/smart-smoker-${{ matrix.service }}:${{ github.event.inputs.version }}
          docker push benjr70/smart-smoker-${{ matrix.service }}:latest

  deploy-cloud:
    needs: build-production
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.version }}
      
      - name: Backup current deployment
        run: |
          export PROD_HOST="smokecloud"
          ssh root@${PROD_HOST} "cd /opt/smart-smoker && cp cloud.docker-compose.yml cloud.docker-compose.yml.backup"
          ssh root@${PROD_HOST} "cd /opt/smart-smoker && docker-compose ps > deployment.backup"
      
      - name: Deploy to production cloud
        run: |
          export PROD_HOST="smokecloud"
          
          # Update compose file with new version
          sed -i "s/:latest/:${{ github.event.inputs.version }}/g" cloud.docker-compose.yml
          
          # Deploy new version
          scp cloud.docker-compose.yml root@${PROD_HOST}:/opt/smart-smoker/
          ssh root@${PROD_HOST} "cd /opt/smart-smoker && docker-compose pull"
          ssh root@${PROD_HOST} "cd /opt/smart-smoker && docker-compose up -d"
          
          # Wait for deployment
          sleep 60
      
      - name: Verify deployment health
        run: |
          export PROD_HOST="smokecloud"
          
          # Run comprehensive health checks
          ./scripts/health-check.sh ${PROD_HOST}
          ./scripts/performance-test.sh ${PROD_HOST}
          
          # Check Tailscale funnel status
          ssh root@${PROD_HOST} "tailscale funnel status"
      
      - name: Update Raspberry Pi devices
        run: |
          # Trigger Watchtower updates on all Pi devices
          ./scripts/update-raspberry-pi-devices.sh ${{ github.event.inputs.version }}
      
      - name: Tag successful deployment
        run: |
          git tag -a "prod-${{ github.event.inputs.version }}-$(date +%Y%m%d-%H%M%S)" -m "Production deployment ${{ github.event.inputs.version }}"
          git push origin --tags

  rollback:
    needs: deploy-cloud
    runs-on: self-hosted
    if: failure()
    
    steps:
      - name: Rollback deployment
        run: |
          export PROD_HOST="smokecloud"
          
          echo "Deployment failed, initiating rollback..."
          
          # Restore previous deployment
          ssh root@${PROD_HOST} "cd /opt/smart-smoker && cp cloud.docker-compose.yml.backup cloud.docker-compose.yml"
          ssh root@${PROD_HOST} "cd /opt/smart-smoker && docker-compose up -d"
          
          # Verify rollback
          sleep 30
          ./scripts/health-check.sh ${PROD_HOST}
      
      - name: Notify rollback
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              text: "üö® Production deployment failed and was rolled back",
              channel: "#alerts",
              username: "Deployment Bot",
              icon_emoji: ":warning:"
            }
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

### Raspberry Pi Update Workflow
```yaml
# .github/workflows/raspberry-pi-update.yml
name: Raspberry Pi Device Updates

on:
  workflow_dispatch:
    inputs:
      target_devices:
        description: 'Target devices (all, production, development)'
        required: true
        default: 'production'
        type: choice
        options:
        - all
        - production
        - development
      update_strategy:
        description: 'Update strategy'
        required: true
        default: 'rolling'
        type: choice
        options:
        - rolling
        - immediate
        - scheduled

jobs:
  update-devices:
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Discover Raspberry Pi devices
        run: |
          # Use Tailscale to discover Pi devices
          tailscale status --json | jq -r '.Peer[] | select(.HostName | contains("smoker-pi")) | .HostName' > pi-devices.txt
          
          # Filter based on target
          case "${{ github.event.inputs.target_devices }}" in
            "production")
              grep "smoker-pi-prod" pi-devices.txt > target-devices.txt || true
              ;;
            "development") 
              grep "smoker-pi-dev" pi-devices.txt > target-devices.txt || true
              ;;
            "all")
              cp pi-devices.txt target-devices.txt
              ;;
          esac
          
          echo "Target devices:"
          cat target-devices.txt
      
      - name: Update devices
        run: |
          update_strategy="${{ github.event.inputs.update_strategy }}"
          
          while IFS= read -r device; do
            echo "Updating device: $device"
            
            case "$update_strategy" in
              "rolling")
                # Update one device at a time with health checks
                ./scripts/update-single-device.sh "$device"
                ./scripts/health-check-device.sh "$device"
                sleep 120  # Wait between devices
                ;;
              "immediate")
                # Update all devices simultaneously
                ./scripts/update-single-device.sh "$device" &
                ;;
              "scheduled")
                # Schedule update for maintenance window
                ssh pi@$device "echo 'watchtower --run-once --cleanup' | at 02:00"
                ;;
            esac
            
          done < target-devices.txt
          
          # Wait for immediate updates to complete
          if [ "$update_strategy" = "immediate" ]; then
            wait
          fi
      
      - name: Verify device updates
        run: |
          failed_devices=""
          
          while IFS= read -r device; do
            echo "Verifying device: $device"
            
            if ! ./scripts/health-check-device.sh "$device"; then
              failed_devices="$failed_devices $device"
              echo "‚ùå Device $device failed health check"
            else
              echo "‚úÖ Device $device updated successfully"
            fi
            
          done < target-devices.txt
          
          if [ -n "$failed_devices" ]; then
            echo "Failed devices: $failed_devices"
            exit 1
          fi
      
      - name: Update monitoring dashboard
        run: |
          # Update device status in monitoring system
          ./scripts/update-device-dashboard.sh
```

## Deployment Scripts

### Health Check Script
```bash
#!/bin/bash
# scripts/health-check.sh

HOST=$1
if [ -z "$HOST" ]; then
    echo "Usage: $0 <hostname>"
    exit 1
fi

echo "Running health checks for $HOST..."

# Check if services are running
check_service() {
    local service=$1
    local port=$2
    local path=${3:-"/"}
    
    echo "Checking $service on port $port..."
    
    if curl -f -s --max-time 10 "http://${HOST}:${port}${path}" > /dev/null; then
        echo "‚úÖ $service is healthy"
        return 0
    else
        echo "‚ùå $service is unhealthy"
        return 1
    fi
}

# Service health checks
HEALTH_CHECKS=(
    "backend:3001:/health"
    "frontend:80:/"
    "device-service:3002:/health"
)

failed_checks=0

for check in "${HEALTH_CHECKS[@]}"; do
    IFS=':' read -r service port path <<< "$check"
    if ! check_service "$service" "$port" "$path"; then
        failed_checks=$((failed_checks + 1))
    fi
    sleep 2
done

# Check Docker containers
echo "Checking Docker container status..."
container_status=$(ssh root@${HOST} "docker-compose ps --services --filter 'status=running'" 2>/dev/null | wc -l)
expected_containers=4  # backend, frontend, device-service, database

if [ "$container_status" -ge "$expected_containers" ]; then
    echo "‚úÖ All containers are running"
else
    echo "‚ùå Some containers are not running (expected: $expected_containers, running: $container_status)"
    failed_checks=$((failed_checks + 1))
fi

# Check system resources
echo "Checking system resources..."
memory_usage=$(ssh root@${HOST} "free | grep Mem | awk '{printf \"%.1f\", \$3/\$2 * 100.0}'")
disk_usage=$(ssh root@${HOST} "df / | tail -1 | awk '{print \$5}' | sed 's/%//'")

if (( $(echo "$memory_usage < 90" | bc -l) )); then
    echo "‚úÖ Memory usage: ${memory_usage}%"
else
    echo "‚ö†Ô∏è  High memory usage: ${memory_usage}%"
fi

if [ "$disk_usage" -lt 90 ]; then
    echo "‚úÖ Disk usage: ${disk_usage}%"
else
    echo "‚ö†Ô∏è  High disk usage: ${disk_usage}%"
fi

# Final result
if [ $failed_checks -eq 0 ]; then
    echo "üéâ All health checks passed for $HOST"
    exit 0
else
    echo "üí• $failed_checks health check(s) failed for $HOST"
    exit 1
fi
```

### Raspberry Pi Device Update Script
```bash
#!/bin/bash
# scripts/update-single-device.sh

DEVICE=$1
if [ -z "$DEVICE" ]; then
    echo "Usage: $0 <device-hostname>"
    exit 1
fi

echo "Updating Raspberry Pi device: $DEVICE"

# Pre-update health check
echo "Running pre-update health check..."
if ! ./scripts/health-check-device.sh "$DEVICE"; then
    echo "‚ö†Ô∏è  Device $DEVICE is not healthy before update, proceeding anyway..."
fi

# Backup current state
echo "Creating device backup..."
ssh pi@$DEVICE "docker-compose ps > /tmp/pre-update-status.txt"
ssh pi@$DEVICE "sudo systemctl is-active --quiet watchtower && echo 'watchtower active' || echo 'watchtower inactive'" > /tmp/watchtower-status.txt

# Update container images
echo "Triggering container updates..."
ssh pi@$DEVICE "docker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once --cleanup"

# Wait for updates to complete
echo "Waiting for updates to complete..."
sleep 60

# Verify services restarted properly
echo "Verifying service restart..."
max_attempts=10
attempt=0

while [ $attempt -lt $max_attempts ]; do
    if ssh pi@$DEVICE "docker-compose ps | grep -q 'Up'"; then
        echo "‚úÖ Services are running"
        break
    else
        echo "‚è≥ Waiting for services to start (attempt $((attempt + 1))/$max_attempts)..."
        sleep 30
        attempt=$((attempt + 1))
    fi
done

if [ $attempt -eq $max_attempts ]; then
    echo "‚ùå Services failed to start after update"
    
    # Attempt recovery
    echo "Attempting service recovery..."
    ssh pi@$DEVICE "cd /opt/smart-smoker && docker-compose down && docker-compose up -d"
    sleep 30
    
    if ! ssh pi@$DEVICE "docker-compose ps | grep -q 'Up'"; then
        echo "üí• Recovery failed for device $DEVICE"
        exit 1
    fi
fi

# Post-update verification
echo "Running post-update health check..."
if ./scripts/health-check-device.sh "$DEVICE"; then
    echo "üéâ Device $DEVICE updated successfully"
    
    # Log successful update
    echo "$(date): Successfully updated $DEVICE" >> /tmp/device-updates.log
else
    echo "üí• Device $DEVICE failed post-update health check"
    exit 1
fi
```

### Integration Test Script
```bash
#!/bin/bash
# scripts/run-integration-tests.sh

VIRTUAL_DEVICE=$1
if [ -z "$VIRTUAL_DEVICE" ]; then
    echo "Usage: $0 <virtual-device-hostname>"
    exit 1
fi

echo "Running integration tests on virtual device: $VIRTUAL_DEVICE"

# Start test suite
echo "Starting integration test suite..."

# Test 1: Device Service Connectivity
echo "Test 1: Device Service Connectivity"
if curl -f -s --max-time 10 "http://${VIRTUAL_DEVICE}:3002/health" > /dev/null; then
    echo "‚úÖ Device service is reachable"
else
    echo "‚ùå Device service connectivity test failed"
    exit 1
fi

# Test 2: Mock Hardware Integration
echo "Test 2: Mock Hardware Integration"
temp_response=$(curl -s "http://${VIRTUAL_DEVICE}:5000/temperature/28-000000000001")
if echo "$temp_response" | jq -e '.temperature' > /dev/null 2>&1; then
    echo "‚úÖ Mock temperature sensor responding"
else
    echo "‚ùå Mock hardware integration test failed"
    exit 1
fi

# Test 3: WebSocket Communication
echo "Test 3: WebSocket Communication"
# Use a simple Node.js script to test WebSocket
cat > /tmp/websocket-test.js << 'EOF'
const WebSocket = require('ws');

const ws = new WebSocket('ws://VIRTUAL_DEVICE:8765');

ws.on('open', function open() {
    console.log('WebSocket connection established');
    ws.send(JSON.stringify({ type: 'test', message: 'integration test' }));
});

ws.on('message', function message(data) {
    const response = JSON.parse(data);
    if (response.type === 'command_ack') {
        console.log('‚úÖ WebSocket communication test passed');
        process.exit(0);
    }
});

ws.on('error', function error(err) {
    console.log('‚ùå WebSocket test failed:', err.message);
    process.exit(1);
});

setTimeout(() => {
    console.log('‚ùå WebSocket test timeout');
    process.exit(1);
}, 10000);
EOF

# Replace placeholder and run test
sed -i "s/VIRTUAL_DEVICE/${VIRTUAL_DEVICE}/g" /tmp/websocket-test.js
if ssh pi@$VIRTUAL_DEVICE "cd /tmp && node /tmp/websocket-test.js"; then
    echo "‚úÖ WebSocket communication test passed"
else
    echo "‚ùå WebSocket communication test failed"
    exit 1
fi

# Test 4: End-to-End Smoke Test
echo "Test 4: End-to-End Smoke Test"
# Set target temperature and verify response
curl -X POST "http://${VIRTUAL_DEVICE}:5000/temperature/28-000000000001/target/250"
sleep 5
new_temp=$(curl -s "http://${VIRTUAL_DEVICE}:5000/temperature/28-000000000001" | jq -r '.temperature')

if (( $(echo "$new_temp > 230" | bc -l) )); then
    echo "‚úÖ End-to-end smoke test passed (temp: $new_temp)"
else
    echo "‚ùå End-to-end smoke test failed (temp: $new_temp)"
    exit 1
fi

echo "üéâ All integration tests passed on $VIRTUAL_DEVICE"
```

## Monitoring & Alerting

### Deployment Monitoring Dashboard
```yaml
# monitoring/deployment-dashboard.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: deployment-dashboard
data:
  dashboard.json: |
    {
      "dashboard": {
        "title": "Smart Smoker Deployments",
        "panels": [
          {
            "title": "Deployment Status",
            "type": "stat",
            "targets": [
              {
                "expr": "github_actions_workflow_runs_total{repository=\"Smart-Smoker-V2\",workflow=\"Production Deployment\",conclusion=\"success\"}",
                "legendFormat": "Successful Deployments"
              }
            ]
          },
          {
            "title": "Environment Health",
            "type": "table",
            "targets": [
              {
                "expr": "up{job=\"smart-smoker-health\"}",
                "legendFormat": "{{instance}}"
              }
            ]
          },
          {
            "title": "Raspberry Pi Devices",
            "type": "table", 
            "targets": [
              {
                "expr": "tailscale_device_status{device_type=\"raspberry-pi\"}",
                "legendFormat": "{{hostname}}"
              }
            ]
          }
        ]
      }
    }
```

### Slack Notification Templates
```yaml
# monitoring/slack-notifications.yml
deployment_success: |
  {
    "channel": "#deployments",
    "username": "Deployment Bot",
    "icon_emoji": ":rocket:",
    "attachments": [
      {
        "color": "good",
        "title": "üöÄ Deployment Successful",
        "fields": [
          {
            "title": "Environment",
            "value": "{{ .environment }}",
            "short": true
          },
          {
            "title": "Version", 
            "value": "{{ .version }}",
            "short": true
          },
          {
            "title": "Duration",
            "value": "{{ .duration }}",
            "short": true
          },
          {
            "title": "Services",
            "value": "{{ .services }}",
            "short": true
          }
        ],
        "actions": [
          {
            "type": "button",
            "text": "View Logs",
            "url": "{{ .logs_url }}"
          },
          {
            "type": "button", 
            "text": "Environment",
            "url": "{{ .environment_url }}"
          }
        ]
      }
    ]
  }

deployment_failure: |
  {
    "channel": "#alerts",
    "username": "Deployment Bot", 
    "icon_emoji": ":warning:",
    "attachments": [
      {
        "color": "danger",
        "title": "üí• Deployment Failed",
        "fields": [
          {
            "title": "Environment",
            "value": "{{ .environment }}",
            "short": true
          },
          {
            "title": "Version",
            "value": "{{ .version }}",
            "short": true
          },
          {
            "title": "Failed Step",
            "value": "{{ .failed_step }}",
            "short": true
          },
          {
            "title": "Error",
            "value": "{{ .error_message }}",
            "short": false
          }
        ],
        "actions": [
          {
            "type": "button",
            "text": "View Logs",
            "url": "{{ .logs_url }}"
          },
          {
            "type": "button",
            "text": "Rollback",
            "url": "{{ .rollback_url }}"
          }
        ]
      }
    ]
  }
```

## Security & Compliance

### Deployment Security Checklist
- [ ] **Image Scanning**: All container images scanned for vulnerabilities
- [ ] **Secret Management**: Secrets stored in GitHub Secrets, not in code
- [ ] **Access Control**: Deployment approvals required for production
- [ ] **Audit Logging**: All deployment activities logged and monitored
- [ ] **Network Security**: Tailscale provides encrypted communication
- [ ] **Backup Verification**: Automated backups before each deployment
- [ ] **Rollback Testing**: Rollback procedures tested regularly

### Compliance Requirements
```yaml
# security/compliance-checks.yml
security_scans:
  - name: "Container Image Vulnerability Scan"
    tool: "Trivy"
    threshold: "HIGH"
    action: "fail"
  
  - name: "Secret Detection"
    tool: "GitLeaks"
    scope: "all_files"
    action: "fail"
  
  - name: "License Compliance"
    tool: "FOSSA"
    scope: "dependencies"
    action: "warn"

audit_requirements:
  - deployment_logs: "retained_90_days"
  - access_logs: "retained_365_days"
  - security_events: "retained_2_years"
  - compliance_reports: "monthly"
```


## Testing Strategy

### Workflow Testing
- **Unit Tests**: Individual script validation
- **Integration Tests**: Full pipeline execution
- **Security Tests**: Vulnerability and compliance scans
- **Performance Tests**: Deployment speed and reliability
- **Rollback Tests**: Failure scenario validation

### Validation Criteria
- Development deployment completes in < 10 minutes
- Production deployment completes in < 30 minutes
- Zero-downtime deployments achieved
- Rollback completes in < 5 minutes
- 99.9% deployment success rate

## Risk Assessment

### Critical Risks (Must Address Before Production)

| Risk | Impact | Probability | Current Status | Mitigation |
|------|--------|-------------|----------------|------------|
| MongoDB security vulnerability | Critical | High | **‚úÖ MITIGATED** | Story 0 - ‚úÖ Upgraded to 7.x and enabled authentication |
| Data loss (no backups) | Critical | Medium | **‚úÖ MITIGATED** | Story 0 - ‚úÖ Automated backups implemented and tested |
| Failed deployment (no rollback) | High | Medium | **‚úÖ MITIGATED** | Story 0 - ‚úÖ Automated health checks and rollback implemented |
| Database migration failure | High | Medium | **PLANNED** | Comprehensive migration plan with rollback (Story 3) |

### Operational Risks (Ongoing Management)

| Risk | Impact | Mitigation |
|------|--------|------------|
| Self-hosted runner failure | High | Multiple runner instances, fallback to GitHub hosted |
| Tailscale connectivity issues | Medium | VPN backup, local network fallback |
| Docker registry unavailability | High | Multiple registry mirrors, local caching |
| Raspberry Pi network issues | Medium | Batch updates, device health monitoring |
| Proxmox server hardware failure | Critical | Regular backups, UPS, Pi as emergency fallback |

### Risk-Adjusted Implementation Order

Based on risk assessment and CI/CD vision, Phase 3 implementation order is:

**‚úÖ Week 1-2: Critical Security & Reliability (Story 0) - COMPLETE**
1. ‚úÖ MongoDB upgrade to 7.x with authentication (dev environment first)
2. ‚úÖ Automated backup implementation and validation
3. ‚úÖ Deployment health checks and rollback automation
4. ‚úÖ Test all fixes in dev environment

**Week 3-4: Development Pipeline (Stories 1, 2, 3)**
5. Story 1: Automated development deployment to dev-cloud (uses existing nightly.yml)
6. Story 2: Virtual smoker device infrastructure setup (must be done before Story 3)
7. Story 3: Virtual smoker device deployment

**Week 5: Production Infrastructure (Story 5)**
8. Story 5: Production database migration (can happen in parallel with dev pipeline)

**Week 6+: Production Automation (Stories 4, 6)**
9. Story 4: Production deployment workflow (with Tailscale funnel)
10. Story 6: Watchtower integration for production Pi

**Future: Testing Enhancement (Story 7)**
11. Story 7: E2E testing framework (DEFERRED - add after core workflows stable)

**Rationale**: 
- Stories 1-3 build the dev pipeline incrementally
- Story 2 is critical infrastructure that must be done before Story 3
- Story 5 can happen in parallel as it's infrastructure prep
- Story 4 depends on dev pipeline being stable
- Story 6 is the final piece for production Pi automation
- Story 7 is deferred to focus on getting core workflows working first

**‚úÖ Story 0 Status Update**: 
- **Implementation**: ‚úÖ Complete
- **Dev-Cloud Testing**: ‚úÖ Complete - All infrastructure fixes validated
- **Production Deployment**: ‚è≥ Pending (manual deployment acceptable for Story 0)
- **Story 0 Status**: ‚úÖ **COMPLETE** - Ready for production deployment

## Success Metrics

### Critical Success Criteria (Story 0 - ‚úÖ ACHIEVED)
- **MongoDB Security**: ‚úÖ Authentication enabled, version 7.x stable, zero vulnerabilities
- **Backup Reliability**: ‚úÖ 100% backup success rate, restore tested and validated
- **Deployment Safety**: ‚úÖ Automated rollback working, < 5 minute rollback time
- **Zero Data Loss**: ‚úÖ All migrations and upgrades complete without data loss
- **Dev-Cloud Testing**: ‚úÖ All infrastructure fixes validated in dev environment

### Quantitative Metrics (Post-Story 0)
- **< 10 minutes** for development deployments
- **< 30 minutes** for production deployments (single-user context, not optimizing for zero-downtime)
- **99%** deployment success rate (more realistic for single-developer project)
- **< 5 minutes** mean time to rollback
- **100%** infrastructure provisioned via automation

### Qualitative Metrics
- Security vulnerabilities addressed before production use
- Confidence in backup and recovery procedures
- Reduced manual deployment steps (not necessarily zero - manual approval gates are valuable)
- Improved development velocity
- Clear documentation and runbooks for all procedures

## Deliverables

### Phase 3 Outputs (Revised Priority Order)

**Critical Deliverables (Week 1-3) - ‚úÖ COMPLETE**:
- [x] MongoDB upgrade to 7.x with authentication
- [x] Automated backup system with validation
- [x] Deployment health checks and automated rollback
- [x] Documented and tested restore procedures
- [x] Security hardening for production readiness
- [x] Dev-cloud testing completed and validated

**High Priority Deliverables (Week 4-6)**:
- [ ] Production database migration from Pi to Proxmox
- [ ] Migration validation and monitoring
- [ ] Updated deployment workflows with security measures
- [ ] Comprehensive infrastructure documentation

**Standard Deliverables (Week 7+)**:
- [ ] Automated development deployment workflows
- [ ] Production deployment automation with approval gates
- [ ] Raspberry Pi device management automation
- [ ] Integration testing framework
- [ ] Virtual device testing automation
- [ ] Deployment monitoring dashboard
- [ ] Security scanning and compliance validation

**Deferred/Optional Deliverables**:
- [ ] Advanced deployment strategies (blue-green, canary) - not needed for single-user
- [ ] Multi-region deployment - not needed for local Proxmox
- [ ] High availability configurations - overkill for current scale

### Handoff to Phase 4
- All deployment workflows functional and tested
- Monitoring and alerting systems operational
- Security and compliance measures implemented
- Team trained on deployment procedures
- Documentation complete and accessible

---

**Phase Owner**: DevOps Team  
**Status**: Ready for Implementation  
**Dependencies**: Phase 2 completion  
**Risk Level**: Medium
